#!/usr/bin/env python3
"""
Web Chat Application with OpenAI API Integration
æ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œæ–‡æœ¬å¯¹è¯çš„ç½‘é¡µèŠå¤©ç•Œé¢
"""

import os
import json
import uuid
import numpy as np
from datetime import datetime, timedelta
from flask import Flask, render_template, request, jsonify, session, send_file, abort
from werkzeug.utils import secure_filename
from agents import HydroSim, PartitionSim, SensorPlacement, LeakDetectionAgent, LLMTaskAnalyzer
from agents.agent_executor import AgentExecutor
import openai
from pathlib import Path
import mimetypes
import re
import threading
import time

app = Flask(__name__)
app.secret_key = 'your-secret-key-change-this'  # è¯·æ›´æ”¹ä¸ºå®‰å…¨çš„å¯†é’¥

# é…ç½®
UPLOAD_FOLDER = 'uploads'
DOWNLOADS_FOLDER = 'downloads'  # ä¸‹è½½æ–‡ä»¶å¤¹
CONVERSATIONS_FOLDER = 'conversations'  # å¯¹è¯å­˜å‚¨ç›®å½•
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'doc', 'docx', 'md', 'py', 'js', 'html', 'css', 'json', 'xml', 'csv', 'inp'}
MAX_FILE_SIZE = 16 * 1024 * 1024  # 16MB

# æ–‡ä»¶ç®¡ç†é…ç½®
MAX_FILES_COUNT = 100  # æœ€å¤§æ–‡ä»¶æ•°é‡
MAX_FOLDER_SIZE = 500 * 1024 * 1024  # æœ€å¤§æ–‡ä»¶å¤¹å¤§å° 500MB
FILE_RETENTION_DAYS = 7  # æ–‡ä»¶ä¿ç•™å¤©æ•°

# åˆ›å»ºç›®å½•
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(DOWNLOADS_FOLDER, exist_ok=True)
os.makedirs(CONVERSATIONS_FOLDER, exist_ok=True)

# å»¶è¿Ÿåˆå§‹åŒ–æ™ºèƒ½ä½“ï¼Œé¿å…é‡å¤çš„IntentClassifieråˆå§‹åŒ–
hydro_sim_agent = None
partition_sim_agent = None
sensor_placement_agent = None
leak_detection_agent = None
llm_task_analyzer = None
agent_executor = None

def extract_training_parameters(user_message: str, default_scenarios: int, default_epochs: int) -> tuple:
    """æ™ºèƒ½æå–è®­ç»ƒå‚æ•°"""
    import re

    num_scenarios = default_scenarios
    epochs = default_epochs

    # æå–è¿­ä»£æ¬¡æ•°/è®­ç»ƒè½®æ•°çš„æ¨¡å¼
    epoch_patterns = [
        r'è¿­ä»£æ¬¡æ•°ä¸º?(\d+)æ¬¡?',
        r'è¿­ä»£(\d+)æ¬¡',
        r'è®­ç»ƒ(\d+)è½®',
        r'(\d+)è½®è®­ç»ƒ',
        r'epochs?\s*[=:ä¸º]\s*(\d+)',
        r'(\d+)\s*ä¸ª?epochs?',
        r'è®­ç»ƒè½®æ•°\s*[=:ä¸º]\s*(\d+)',
        r'(\d+)æ¬¡è¿­ä»£',
        r'epoch\s*[=:]\s*(\d+)',
        r'è½®æ•°\s*[=:ä¸º]\s*(\d+)'
    ]

    # æå–æ ·æœ¬æ•°/æ•°æ®ç»„æ•°çš„æ¨¡å¼
    scenario_patterns = [
        r'ç”Ÿæˆæ•°æ®ä¸º?(\d+)ç»„',
        r'(\d+)ç»„æ•°æ®',
        r'(\d+)ä¸ªæ ·æœ¬',
        r'(\d+)ä¸ªåœºæ™¯',
        r'æ•°æ®é‡\s*[=:ä¸º]\s*(\d+)',
        r'æ ·æœ¬æ•°\s*[=:ä¸º]\s*(\d+)',
        r'åœºæ™¯æ•°\s*[=:ä¸º]\s*(\d+)',
        r'æ•°æ®\s*(\d+)ç»„',
        r'æ ·æœ¬\s*(\d+)ä¸ª',
        r'åœºæ™¯\s*(\d+)ä¸ª',
        r'(\d+)\s*ä¸ªæ•°æ®',
        r'(\d+)\s*ç»„æ ·æœ¬',
        r'(\d+)ç»„',  # ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥åŒ¹é…"1000ç»„"
        r'æ€»æ ·æœ¬æ•°\s*[=:ä¸º]\s*(\d+)',
        r'æ ·æœ¬æ€»æ•°\s*[=:ä¸º]\s*(\d+)',
        r'æ•°æ®æ€»æ•°\s*[=:ä¸º]\s*(\d+)',
        r'ç”Ÿæˆ\s*(\d+)\s*ç»„',
        r'è®­ç»ƒæ•°æ®\s*(\d+)\s*ç»„',
        r'(\d+)\s*ä¸ªè®­ç»ƒæ ·æœ¬'
    ]

    # å°è¯•åŒ¹é…è¿­ä»£æ¬¡æ•°
    for pattern in epoch_patterns:
        match = re.search(pattern, user_message, re.IGNORECASE)
        if match:
            epochs = min(int(match.group(1)), 500)
            print(f"è¯†åˆ«åˆ°è¿­ä»£æ¬¡æ•°: {epochs} (åŒ¹é…æ¨¡å¼: {pattern})")
            break

    # å°è¯•åŒ¹é…æ ·æœ¬æ•°
    for pattern in scenario_patterns:
        match = re.search(pattern, user_message, re.IGNORECASE)
        if match:
            num_scenarios = min(int(match.group(1)), 2000)  # æé«˜æœ€å¤§é™åˆ¶åˆ°2000
            print(f"è¯†åˆ«åˆ°æ ·æœ¬æ•°: {num_scenarios} (åŒ¹é…æ¨¡å¼: {pattern})")
            break

    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæ¨¡å¼ï¼Œä½¿ç”¨åŸæ¥çš„ç®€å•æ•°å­—æå–ä½œä¸ºå¤‡ç”¨
    if num_scenarios == default_scenarios and epochs == default_epochs:
        numbers = re.findall(r'\d+', user_message)
        if numbers:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ•°å­—ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­
            if len(numbers) == 1:
                num = int(numbers[0])
                if any(keyword in user_message.lower() for keyword in ['è¿­ä»£', 'è½®', 'epoch']):
                    epochs = min(num, 500)
                    print(f"æ ¹æ®ä¸Šä¸‹æ–‡è¯†åˆ«ä¸ºè¿­ä»£æ¬¡æ•°: {epochs}")
                elif any(keyword in user_message.lower() for keyword in ['æ•°æ®', 'æ ·æœ¬', 'åœºæ™¯', 'ç»„']):
                    num_scenarios = min(num, 2000)  # æé«˜æœ€å¤§é™åˆ¶åˆ°2000
                    print(f"æ ¹æ®ä¸Šä¸‹æ–‡è¯†åˆ«ä¸ºæ ·æœ¬æ•°: {num_scenarios}")
                else:
                    # é»˜è®¤ç¬¬ä¸€ä¸ªæ•°å­—ä½œä¸ºæ ·æœ¬æ•°
                    num_scenarios = min(num, 2000)  # æé«˜æœ€å¤§é™åˆ¶åˆ°2000
                    print(f"é»˜è®¤è¯†åˆ«ä¸ºæ ·æœ¬æ•°: {num_scenarios}")
            elif len(numbers) >= 2:
                # å¤šä¸ªæ•°å­—æ—¶ï¼ŒæŒ‰åŸæ¥çš„é€»è¾‘ï¼šç¬¬ä¸€ä¸ªä½œä¸ºæ ·æœ¬æ•°ï¼Œç¬¬äºŒä¸ªä½œä¸ºè¿­ä»£æ¬¡æ•°
                num_scenarios = min(int(numbers[0]), 2000)  # æé«˜æœ€å¤§é™åˆ¶åˆ°2000
                epochs = min(int(numbers[1]), 500)
                print(f"å¤šæ•°å­—æ¨¡å¼: æ ·æœ¬æ•°={num_scenarios}, è¿­ä»£æ¬¡æ•°={epochs}")

    return num_scenarios, epochs

def init_agents():
    """å»¶è¿Ÿåˆå§‹åŒ–æ™ºèƒ½ä½“"""
    global hydro_sim_agent, partition_sim_agent, sensor_placement_agent, leak_detection_agent
    global llm_task_analyzer, agent_executor

    if hydro_sim_agent is None:
        print("åˆå§‹åŒ–æ™ºèƒ½ä½“...")
        hydro_sim_agent = HydroSim()
        partition_sim_agent = PartitionSim()
        sensor_placement_agent = SensorPlacement()
        leak_detection_agent = LeakDetectionAgent()

        # åˆå§‹åŒ–LLMä»»åŠ¡åˆ†æå™¨å’Œæ™ºèƒ½ä½“æ‰§è¡Œå™¨
        llm_task_analyzer = LLMTaskAnalyzer()
        agent_executor = AgentExecutor(
            hydro_sim_agent,
            partition_sim_agent,
            sensor_placement_agent,
            leak_detection_agent
        )
        print("æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")

# OpenAI é…ç½®
openai.api_base = "https://api.chatanywhere.tech"
openai.api_key = "sk-eHk6ICs2KGZ2M2xJ0AZK9DJu3DVqgO91EnatH7FsUokii7HH"

# æ™ºèƒ½ä½“æ ‡å‡†è¯­å¥æ˜ å°„
AGENT_STANDARD_PHRASES = {
    "ç®¡ç½‘åˆ†æ": "åˆ†æç®¡ç½‘ç»“æ„å’ŒåŸºæœ¬ä¿¡æ¯",
    "ç®¡ç½‘åˆ†åŒº": "æŠŠç®¡ç½‘åˆ’åˆ†ä¸ºæŒ‡å®šæ•°é‡çš„åŒºåŸŸ",
    "ç¦»ç¾¤ç‚¹æ£€æµ‹": "æ£€æµ‹å’Œå‰”é™¤ç®¡ç½‘åˆ†åŒºä¸­çš„ç¦»ç¾¤ç‚¹",
    "ä¼ æ„Ÿå™¨å¸ƒç½®": "åœ¨ç®¡ç½‘ä¸­ä¼˜åŒ–å¸ƒç½®å‹åŠ›ç›‘æµ‹ä¼ æ„Ÿå™¨",
    "éŸ§æ€§åˆ†æ": "åˆ†æä¼ æ„Ÿå™¨å¸ƒç½®çš„éŸ§æ€§å’Œæ•…éšœæ£€æµ‹èƒ½åŠ›",
    "æ¼æŸæ¨¡å‹è®­ç»ƒ": "è®­ç»ƒåŸºäºæœºå™¨å­¦ä¹ çš„æ¼æŸæ£€æµ‹æ¨¡å‹",
    "æ¼æŸæ£€æµ‹": "ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æµ‹ç®¡ç½‘æ¼æŸ",
    "æ°´åŠ›ä»¿çœŸ": "è¿›è¡Œç®¡ç½‘æ°´åŠ›è®¡ç®—å’Œä»¿çœŸåˆ†æ",
    "æ‹“æ‰‘åˆ†æ": "åˆ†æç®¡ç½‘çš„æ‹“æ‰‘ç»“æ„å’Œè¿é€šæ€§"
}

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def read_file_content(filepath):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        # å°è¯•ä»¥æ–‡æœ¬æ¨¡å¼è¯»å–
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        try:
            # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
            with open(filepath, 'r', encoding='gbk') as f:
                return f.read()
        except:
            return "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰"
    except Exception as e:
        return f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"

def generate_conversation_title(first_message):
    """æ ¹æ®ç¬¬ä¸€æ¡æ¶ˆæ¯ç”Ÿæˆå¯¹è¯æ ‡é¢˜"""
    if not first_message:
        return "æ–°å¯¹è¯"

    # æ¸…ç†æ¶ˆæ¯å†…å®¹
    clean_message = re.sub(r'\s+', ' ', first_message.strip())

    # å¦‚æœæ¶ˆæ¯å¤ªé•¿ï¼Œæˆªå–å‰30ä¸ªå­—ç¬¦
    if len(clean_message) > 30:
        return clean_message[:30] + "..."

    return clean_message if clean_message else "æ–°å¯¹è¯"

def ensure_conversations_folder():
    """ç¡®ä¿å¯¹è¯å­˜å‚¨ç›®å½•å­˜åœ¨"""
    os.makedirs(CONVERSATIONS_FOLDER, exist_ok=True)

class NumpyEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†numpyæ•°æ®ç±»å‹"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        return super().default(obj)

def safe_jsonify(data, status_code=200):
    """å®‰å…¨çš„jsonifyå‡½æ•°ï¼Œå¤„ç†numpyæ•°æ®ç±»å‹"""
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨åºåˆ—åŒ–æ•°æ®
        json_str = json.dumps(data, cls=NumpyEncoder, ensure_ascii=False)
        # åˆ›å»ºå“åº”
        response = app.response_class(
            json_str,
            mimetype='application/json'
        )
        response.status_code = status_code
        return response
    except Exception as e:
        # å¦‚æœè‡ªå®šä¹‰åºåˆ—åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†jsonify
        print(f"JSONåºåˆ—åŒ–è­¦å‘Š: {e}")
        return jsonify({'error': 'JSONåºåˆ—åŒ–å¤±è´¥'}), 500

def save_conversation_to_file(conversation_id, conversation_data):
    """ä¿å­˜å•ä¸ªå¯¹è¯åˆ°æ–‡ä»¶"""
    ensure_conversations_folder()
    filepath = os.path.join(CONVERSATIONS_FOLDER, f'conversation_{conversation_id}.json')
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(conversation_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
    except Exception as e:
        print(f"ä¿å­˜å¯¹è¯æ–‡ä»¶å¤±è´¥: {e}")

def load_conversation_from_file(conversation_id):
    """ä»æ–‡ä»¶åŠ è½½å•ä¸ªå¯¹è¯"""
    filepath = os.path.join(CONVERSATIONS_FOLDER, f'conversation_{conversation_id}.json')
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½å¯¹è¯æ–‡ä»¶å¤±è´¥: {e}")
    return None

def save_conversations_index(conversations_dict):
    """ä¿å­˜å¯¹è¯ç´¢å¼•"""
    ensure_conversations_folder()
    index_data = {
        'conversations': {
            conv_id: {
                'id': conv_data['id'],
                'title': conv_data['title'],
                'created_at': conv_data['created_at'],
                'updated_at': conv_data['updated_at'],
                'message_count': len(conv_data['messages'])
            }
            for conv_id, conv_data in conversations_dict.items()
        },
        'last_updated': datetime.now().isoformat()
    }

    filepath = os.path.join(CONVERSATIONS_FOLDER, 'conversations_index.json')
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
    except Exception as e:
        print(f"ä¿å­˜å¯¹è¯ç´¢å¼•å¤±è´¥: {e}")

def load_all_conversations():
    """ä»æ–‡ä»¶åŠ è½½æ‰€æœ‰å¯¹è¯"""
    ensure_conversations_folder()
    index_filepath = os.path.join(CONVERSATIONS_FOLDER, 'conversations_index.json')

    if not os.path.exists(index_filepath):
        return {}

    try:
        with open(index_filepath, 'r', encoding='utf-8') as f:
            index_data = json.load(f)

        conversations = {}
        for conv_id in index_data['conversations']:
            conv_data = load_conversation_from_file(conv_id)
            if conv_data:
                conversations[conv_id] = conv_data

        return conversations
    except Exception as e:
        print(f"åŠ è½½å¯¹è¯å†å²å¤±è´¥: {e}")
        return {}

def delete_conversation_file(conversation_id):
    """åˆ é™¤å¯¹è¯æ–‡ä»¶"""
    filepath = os.path.join(CONVERSATIONS_FOLDER, f'conversation_{conversation_id}.json')
    if os.path.exists(filepath):
        try:
            os.remove(filepath)
        except Exception as e:
            print(f"åˆ é™¤å¯¹è¯æ–‡ä»¶å¤±è´¥: {e}")

def save_pinned_conversations(pinned_list):
    """ä¿å­˜ç½®é¡¶å¯¹è¯åˆ—è¡¨"""
    ensure_conversations_folder()
    filepath = os.path.join(CONVERSATIONS_FOLDER, 'pinned_conversations.json')
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                'pinned_conversations': pinned_list,
                'last_updated': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
    except Exception as e:
        print(f"ä¿å­˜ç½®é¡¶å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")

def load_pinned_conversations():
    """åŠ è½½ç½®é¡¶å¯¹è¯åˆ—è¡¨"""
    ensure_conversations_folder()
    filepath = os.path.join(CONVERSATIONS_FOLDER, 'pinned_conversations.json')

    if not os.path.exists(filepath):
        return []

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('pinned_conversations', [])
    except Exception as e:
        print(f"åŠ è½½ç½®é¡¶å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
        return []

def get_inp_file_from_conversation_history(conversation):
    """ä»å¯¹è¯å†å²ä¸­è·å–æœ€è¿‘çš„.inpæ–‡ä»¶è·¯å¾„"""
    for msg in reversed(conversation['messages']):
        if msg.get('file_type') == 'inp' and msg.get('file_path'):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
            if os.path.exists(msg['file_path']):
                return msg['file_path']
    return None

def has_inp_file_in_conversation_history(conversation):
    """æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦åŒ…å«.inpæ–‡ä»¶"""
    return get_inp_file_from_conversation_history(conversation) is not None

def get_partition_csv_from_conversation_history(conversation):
    """ä»å¯¹è¯å†å²ä¸­è·å–æœ€è¿‘çš„åˆ†åŒºCSVæ–‡ä»¶è·¯å¾„"""
    for msg in reversed(conversation['messages']):
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†åŒºç›¸å…³çš„æ¶ˆæ¯ä¸”æœ‰CSVæ–‡ä»¶ç”Ÿæˆ
        if (msg.get('intent') == 'partition_analysis' and
            msg.get('csv_info') and
            msg['csv_info'].get('success')):
            csv_path = msg['csv_info']['filepath']
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
            if os.path.exists(csv_path):
                return csv_path
    return None

def has_partition_csv_in_conversation_history(conversation):
    """æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦åŒ…å«åˆ†åŒºCSVæ–‡ä»¶"""
    return get_partition_csv_from_conversation_history(conversation) is not None

def cleanup_old_files():
    """æ¸…ç†è¿‡æœŸæ–‡ä»¶"""
    try:
        if not os.path.exists(DOWNLOADS_FOLDER):
            return

        current_time = datetime.now()
        cutoff_time = current_time - timedelta(days=FILE_RETENTION_DAYS)

        files_info = []
        total_size = 0

        # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
        for filename in os.listdir(DOWNLOADS_FOLDER):
            file_path = os.path.join(DOWNLOADS_FOLDER, filename)
            if os.path.isfile(file_path):
                file_stat = os.stat(file_path)
                file_time = datetime.fromtimestamp(file_stat.st_mtime)
                file_size = file_stat.st_size

                files_info.append({
                    'path': file_path,
                    'filename': filename,
                    'mtime': file_time,
                    'size': file_size
                })
                total_size += file_size

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼‰
        files_info.sort(key=lambda x: x['mtime'])

        deleted_count = 0

        # åˆ é™¤è¿‡æœŸæ–‡ä»¶
        for file_info in files_info:
            if file_info['mtime'] < cutoff_time:
                try:
                    os.remove(file_info['path'])
                    deleted_count += 1
                    total_size -= file_info['size']
                    print(f"åˆ é™¤è¿‡æœŸæ–‡ä»¶: {file_info['filename']}")
                except Exception as e:
                    print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_info['filename']}: {e}")

        # å¦‚æœæ–‡ä»¶æ•°é‡ä»ç„¶è¿‡å¤šï¼Œåˆ é™¤æœ€æ—§çš„æ–‡ä»¶
        remaining_files = [f for f in files_info if os.path.exists(f['path'])]
        while len(remaining_files) > MAX_FILES_COUNT:
            oldest_file = remaining_files.pop(0)
            try:
                os.remove(oldest_file['path'])
                deleted_count += 1
                total_size -= oldest_file['size']
                print(f"åˆ é™¤å¤šä½™æ–‡ä»¶: {oldest_file['filename']}")
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {oldest_file['filename']}: {e}")

        # å¦‚æœæ–‡ä»¶å¤¹å¤§å°ä»ç„¶è¿‡å¤§ï¼Œåˆ é™¤æœ€æ—§çš„æ–‡ä»¶
        remaining_files = [f for f in files_info if os.path.exists(f['path'])]
        while total_size > MAX_FOLDER_SIZE and remaining_files:
            oldest_file = remaining_files.pop(0)
            try:
                os.remove(oldest_file['path'])
                deleted_count += 1
                total_size -= oldest_file['size']
                print(f"åˆ é™¤å¤§æ–‡ä»¶: {oldest_file['filename']}")
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {oldest_file['filename']}: {e}")

        if deleted_count > 0:
            print(f"æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")

    except Exception as e:
        print(f"æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")

def start_file_cleanup_scheduler():
    """å¯åŠ¨æ–‡ä»¶æ¸…ç†è°ƒåº¦å™¨"""
    def cleanup_worker():
        while True:
            try:
                cleanup_old_files()
                # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
                time.sleep(3600)
            except Exception as e:
                print(f"æ–‡ä»¶æ¸…ç†è°ƒåº¦å™¨é”™è¯¯: {e}")
                time.sleep(3600)

    # å¯åŠ¨åå°çº¿ç¨‹
    cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
    cleanup_thread.start()
    print("æ–‡ä»¶æ¸…ç†è°ƒåº¦å™¨å·²å¯åŠ¨")

def init_session():
    """åˆå§‹åŒ–ä¼šè¯æ•°æ®"""
    # åªåœ¨sessionä¸­å­˜å‚¨å¿…è¦çš„ä¿¡æ¯ï¼Œé¿å…sessionè¿‡å¤§
    if 'current_conversation_id' not in session:
        session['current_conversation_id'] = None
    # ä¸å†åœ¨sessionä¸­å­˜å‚¨chat_historyï¼Œæ”¹ä¸ºæŒ‰éœ€ä»æ–‡ä»¶åŠ è½½
    if 'pinned_conversations' not in session:
        # ä»æ–‡ä»¶åŠ è½½ç½®é¡¶å¯¹è¯åˆ—è¡¨
        session['pinned_conversations'] = load_pinned_conversations()
    # ä¸å†åœ¨sessionä¸­å­˜å‚¨æ‰€æœ‰å¯¹è¯ï¼Œæ”¹ä¸ºæŒ‰éœ€ä»æ–‡ä»¶åŠ è½½

@app.route('/')
def index():
    """ä¸»é¡µ"""
    init_session()
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    """å¤„ç†æ–‡ä»¶ä¸Šä¼ """
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

    if file and allowed_file(file.filename):
        init_session()

        # è·å–æˆ–åˆ›å»ºå¯¹è¯ID
        conversation_id = session.get('current_conversation_id')
        if not conversation_id:
            conversation_id = str(uuid.uuid4())
            session['current_conversation_id'] = conversation_id
            session.modified = True

        filename = secure_filename(file.filename)
        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
        name, ext = os.path.splitext(filename)

        # æ·»åŠ æ—¶é—´æˆ³å’Œå¯¹è¯IDé¿å…æ–‡ä»¶åå†²çªï¼Œä¿æŒä¸æ™ºèƒ½ä½“ç”Ÿæˆæ–‡ä»¶çš„å‘½åä¸€è‡´
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        conversation_prefix = conversation_id[:8]  # ä½¿ç”¨å¯¹è¯IDçš„å‰8ä½
        filename = f"uploaded_{conversation_prefix}_{timestamp}_{name}{ext}"
        filepath = os.path.join(UPLOAD_FOLDER, filename)

        file.save(filepath)

        # è¯»å–æ–‡ä»¶å†…å®¹
        content = read_file_content(filepath)

        return jsonify({
            'success': True,
            'filename': filename,
            'content': content[:2000] + '...' if len(content) > 2000 else content,  # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
            'full_content': content,
            'conversation_id': conversation_id
        })

    return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'}), 400

@app.route('/chat', methods=['POST'])
def chat():
    """å¤„ç†èŠå¤©è¯·æ±‚"""
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        file_content = data.get('file_content', '')
        conversation_id = data.get('conversation_id', None)

        if not user_message and not file_content:
            return jsonify({'error': 'è¯·è¾“å…¥æ¶ˆæ¯æˆ–ä¸Šä¼ æ–‡ä»¶'}), 400

        init_session()

        # åˆå§‹åŒ–æ™ºèƒ½ä½“ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        init_agents()

        # å¦‚æœæ²¡æœ‰æŒ‡å®šå¯¹è¯IDï¼Œå°è¯•ä½¿ç”¨å½“å‰æ´»è·ƒçš„å¯¹è¯
        if not conversation_id:
            conversation_id = session.get('current_conversation_id')

        # è·å–æˆ–åˆ›å»ºå¯¹è¯
        if not conversation_id:
            conversation_id = str(uuid.uuid4())
            current_conversation = {
                'id': conversation_id,
                'title': generate_conversation_title(user_message),
                'messages': [],
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }
        else:
            # ä»æ–‡ä»¶åŠ è½½ç°æœ‰å¯¹è¯
            current_conversation = load_conversation_from_file(conversation_id)
            if not current_conversation:
                # å¯¹è¯ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°å¯¹è¯
                conversation_id = str(uuid.uuid4())
                current_conversation = {
                    'id': conversation_id,
                    'title': generate_conversation_title(user_message),
                    'messages': [],
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }

        # è®¾ç½®å½“å‰å¯¹è¯
        session['current_conversation_id'] = conversation_id

        # æ¸…é™¤æ–°å¯¹è¯æ ‡å¿—ï¼ˆç”¨æˆ·å·²ç»å¼€å§‹å‘é€æ¶ˆæ¯ï¼‰
        if 'is_new_conversation' in session:
            del session['is_new_conversation']
            session.modified = True

        # æ£€æŸ¥æ˜¯å¦æ˜¯.inpæ–‡ä»¶ï¼ˆé€šè¿‡æ–‡ä»¶å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼‰
        is_inp_file = False
        inp_file_path = None
        is_csv_file = False
        csv_file_path = None

        if file_content:
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«EPANETæ ¼å¼ç‰¹å¾
            if ('[JUNCTIONS]' in file_content or '[PIPES]' in file_content or
                '[RESERVOIRS]' in file_content or '[TANKS]' in file_content):
                is_inp_file = True

                # ä¿å­˜ä¸ºä¸´æ—¶.inpæ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                inp_filename = f"temp_network_{conversation_id[:8]}_{timestamp}.inp"
                inp_file_path = os.path.join(UPLOAD_FOLDER, inp_filename)

                with open(inp_file_path, 'w', encoding='utf-8') as f:
                    f.write(file_content)

            # æ£€æŸ¥æ˜¯å¦æ˜¯CSVæ–‡ä»¶ï¼ˆé€šè¿‡å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼‰
            elif (',' in file_content and '\n' in file_content):
                # ç®€å•æ£€æŸ¥æ˜¯å¦åƒCSVæ ¼å¼
                lines = file_content.strip().split('\n')
                if len(lines) > 1:  # è‡³å°‘æœ‰æ ‡é¢˜è¡Œå’Œæ•°æ®è¡Œ
                    is_csv_file = True

                    # ä¿å­˜ä¸ºä¸´æ—¶CSVæ–‡ä»¶
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_filename = f"sensor_data_{conversation_id[:8]}_{timestamp}.csv"
                    csv_file_path = os.path.join(UPLOAD_FOLDER, csv_filename)

                    with open(csv_file_path, 'w', encoding='utf-8') as f:
                        f.write(file_content)

        # ä¼˜å…ˆæ£€æŸ¥CSVæ¨ç†åœºæ™¯ - å¦‚æœä¸Šä¼ äº†CSVæ–‡ä»¶ä¸”ç”¨æˆ·è¾“å…¥åŒ…å«æ¨ç†å…³é”®è¯
        skip_llm_analysis = False
        if is_csv_file and csv_file_path:
            message_lower = user_message.lower()
            inference_keywords = ['æ¨ç†', 'é¢„æµ‹', 'åˆ†æ', 'æ£€æµ‹', 'è¯†åˆ«']

            if any(keyword in message_lower for keyword in inference_keywords):
                print(f"ğŸ¯ æ£€æµ‹åˆ°CSVæ¨ç†åœºæ™¯ï¼Œè·³è¿‡LLMåˆ†æï¼Œç›´æ¥è¿›å…¥æ¨ç†æ¨¡å¼")
                print(f"   - CSVæ–‡ä»¶: {os.path.basename(csv_file_path)}")
                print(f"   - ç”¨æˆ·æ¶ˆæ¯: {user_message}")
                skip_llm_analysis = True

        # åªæœ‰åœ¨éCSVæ¨ç†åœºæ™¯ä¸‹æ‰è¿›è¡ŒLLMåˆ†æ
        task_analysis = None
        if not skip_llm_analysis:
            # æ–°çš„LLMé©±åŠ¨çš„ä»»åŠ¡åˆ†æé€»è¾‘
            print(f"å¼€å§‹LLMä»»åŠ¡åˆ†æï¼Œç”¨æˆ·æ¶ˆæ¯: {user_message}")

            # ä½¿ç”¨LLMä»»åŠ¡åˆ†æå™¨åˆ†æç”¨æˆ·æ„å›¾
            task_analysis = llm_task_analyzer.process(
                user_message,
                conversation_id,
                current_conversation.get('messages', [])
            )

            print(f"LLMä»»åŠ¡åˆ†æç»“æœ: {task_analysis}")

        # å¦‚æœåˆ†ææˆåŠŸä¸”éœ€è¦æ‰§è¡Œæ™ºèƒ½ä½“ä»»åŠ¡
        if (task_analysis and task_analysis.get('success') and
            task_analysis.get('analysis', {}).get('task_type') in ['single', 'workflow']):

            # æ£€æŸ¥å‰ç½®æ¡ä»¶
            prerequisites = task_analysis.get('prerequisites', {})

            # å¦‚æœæ‰€æœ‰å‰ç½®æ¡ä»¶éƒ½æ»¡è¶³ï¼Œæ‰§è¡Œä»»åŠ¡
            if prerequisites.get('all_satisfied', False):
                print("å‰ç½®æ¡ä»¶æ»¡è¶³ï¼Œå¼€å§‹æ‰§è¡Œæ™ºèƒ½ä½“ä»»åŠ¡")

                # ä½¿ç”¨æ™ºèƒ½ä½“æ‰§è¡Œå™¨æ‰§è¡Œä»»åŠ¡
                execution_result = agent_executor.process(
                    task_analysis['execution_plan'],
                    conversation_id,
                    user_message
                )

                if execution_result.get('success'):
                    # è·å–LLMç”Ÿæˆçš„å“åº”
                    assistant_message = execution_result['llm_response']

                    # æ£€æŸ¥æ˜¯å¦æœ‰ç®¡ç½‘åˆ†æç»“æœï¼Œå¦‚æœæœ‰åˆ™æ·»åŠ è¯¦ç»†çš„ç®¡ç½‘ä¿¡æ¯
                    execution_results = execution_result.get('execution_results', [])
                    for step_result in execution_results:
                        if (step_result.get('step_name') == 'ç®¡ç½‘åˆ†æ' and
                            step_result.get('result') and
                            step_result['result'].get('network_info')):
                            network_info = step_result['result']['network_info']
                            network_details = f"""

## ğŸ“Š ç®¡ç½‘è¯¦ç»†ä¿¡æ¯

### ğŸ—ï¸ ç½‘ç»œç»“æ„
- **èŠ‚ç‚¹æ€»æ•°**: {network_info['nodes']['total']} ä¸ª
  - æ¥ç‚¹: {network_info['nodes']['junctions']} ä¸ª
  - æ°´åº“: {network_info['nodes']['reservoirs']} ä¸ª
  - æ°´å¡”: {network_info['nodes']['tanks']} ä¸ª

- **ç®¡æ®µæ€»æ•°**: {network_info['links']['total']} ä¸ª
  - ç®¡é“: {network_info['links']['pipes']} ä¸ª
  - æ°´æ³µ: {network_info['links']['pumps']} ä¸ª
  - é˜€é—¨: {network_info['links']['valves']} ä¸ª

### ğŸ“ ç½‘ç»œå‚æ•°
- **ç®¡ç½‘æ€»é•¿åº¦**: {network_info['network_stats']['total_length']:.2f} ç±³
- **ä»¿çœŸæ—¶é•¿**: {network_info['network_stats']['simulation_duration']} ç§’
- **æ°´åŠ›æ—¶é—´æ­¥é•¿**: {network_info['network_stats']['hydraulic_timestep']} ç§’
- **æ¨¡å¼æ—¶é—´æ­¥é•¿**: {network_info['network_stats']['pattern_timestep']} ç§’

### ğŸ¯ åˆ†æå»ºè®®
åŸºäºä»¥ä¸Šç®¡ç½‘ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è¿›è¡Œä»¥ä¸‹è¿›ä¸€æ­¥åˆ†æï¼š
- ğŸ”„ **æ°´åŠ›ä»¿çœŸ**: è®¡ç®—èŠ‚ç‚¹å‹åŠ›å’Œç®¡æ®µæµé‡
- ğŸ—‚ï¸ **ç®¡ç½‘åˆ†åŒº**: å°†ç®¡ç½‘åˆ’åˆ†ä¸ºç®¡ç†åŒºåŸŸ
- ğŸ“ **ä¼ æ„Ÿå™¨å¸ƒç½®**: ä¼˜åŒ–ç›‘æµ‹ç‚¹ä½ç½®
- ğŸ” **æ¼æŸæ£€æµ‹**: è®­ç»ƒå’Œåº”ç”¨æ¼æŸæ£€æµ‹æ¨¡å‹
"""
                            assistant_message += network_details
                            break

                    # æ”¶é›†ä¸‹è½½æ–‡ä»¶ä¿¡æ¯
                    downloads = []
                    for exec_result in execution_result['execution_results']:
                        if exec_result.get('success') and exec_result.get('result'):
                            agent_result = exec_result['result']

                            # æ£€æŸ¥CSVæ–‡ä»¶
                            if agent_result.get('csv_info') and agent_result['csv_info'].get('success'):
                                downloads.append({
                                    'type': 'csv',
                                    'step': exec_result['step_name'],
                                    'filename': agent_result['csv_info']['filename'],
                                    'url': agent_result['csv_info']['download_url'],
                                    'size': agent_result['csv_info']['file_size']
                                })

                            # æ£€æŸ¥å¯è§†åŒ–å›¾ç‰‡
                            if agent_result.get('visualization'):
                                viz_info = agent_result['visualization']
                                if viz_info.get('filename') and viz_info.get('path'):
                                    viz_filename = viz_info['filename']
                                    viz_url = f"/static_files/{viz_filename}"

                                    try:
                                        viz_size = os.path.getsize(viz_info['path'])
                                    except:
                                        viz_size = 0

                                    downloads.append({
                                        'type': 'image',
                                        'step': exec_result['step_name'],
                                        'filename': viz_filename,
                                        'url': viz_url,
                                        'size': viz_size,
                                        'display_url': viz_url
                                    })

                            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆç‰¹åˆ«å¤„ç†æ¼æŸæ£€æµ‹æ¨¡å‹è®­ç»ƒçš„æ–‡ä»¶ï¼‰
                            if agent_result.get('files'):
                                for file_type, file_info in agent_result['files'].items():
                                    if file_info.get('success'):
                                        # æ ¹æ®æ–‡ä»¶ç±»å‹å’Œæ‰©å±•åç¡®å®šä¸‹è½½ç±»å‹
                                        download_type = file_type
                                        filename = file_info['filename']

                                        # ç‰¹æ®Šå¤„ç†æ¼æŸæ£€æµ‹æ¨¡å‹çš„æ–‡ä»¶ç±»å‹
                                        if exec_result['step_name'] == 'æ¼æŸæ¨¡å‹è®­ç»ƒ':
                                            if filename.endswith('.csv'):
                                                download_type = 'csv'
                                            elif filename.endswith('.pth'):
                                                download_type = 'model'

                                        downloads.append({
                                            'type': download_type,
                                            'step': exec_result['step_name'],
                                            'filename': filename,
                                            'url': file_info['download_url'],
                                            'size': file_info['file_size']
                                        })

                    # ä¿å­˜åˆ°å¯¹è¯å†å²
                    message_data = {
                        'user': user_message,
                        'assistant': assistant_message,
                        'timestamp': datetime.now().isoformat(),
                        'intent': task_analysis['analysis']['standard_phrase'],
                        'confidence': task_analysis['analysis']['confidence'],
                        'task_analysis': task_analysis,
                        'execution_results': execution_result['execution_results']
                    }

                    # æ·»åŠ ä¸‹è½½ä¿¡æ¯åˆ°å¯¹è¯å†å²
                    if downloads:
                        message_data['downloads'] = downloads

                    # å¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ï¼Œè®°å½•æ–‡ä»¶ä¿¡æ¯
                    if is_inp_file and inp_file_path:
                        message_data.update({
                            'has_file': True,
                            'file_type': 'inp',
                            'file_path': inp_file_path
                        })
                    elif is_csv_file and csv_file_path:
                        message_data.update({
                            'has_file': True,
                            'file_type': 'csv',
                            'file_path': csv_file_path
                        })

                    current_conversation['messages'].append(message_data)
                    current_conversation['updated_at'] = datetime.now().isoformat()

                    # æ›´æ–°å¯¹è¯æ ‡é¢˜
                    if len(current_conversation['messages']) == 1 and current_conversation['title'] == 'æ–°å¯¹è¯':
                        current_conversation['title'] = generate_conversation_title(user_message)

                    # ä¿å­˜å¯¹è¯
                    save_conversation_to_file(conversation_id, current_conversation)
                    all_conversations = load_all_conversations()
                    all_conversations[conversation_id] = current_conversation
                    save_conversations_index(all_conversations)
                    session.modified = True

                    # æ„å»ºå“åº”æ•°æ®
                    response_data = {
                        'success': True,
                        'response': assistant_message,
                        'conversation_id': conversation_id,
                        'intent': task_analysis['analysis']['standard_phrase'],
                        'confidence': task_analysis['analysis']['confidence'],
                        'task_analysis': task_analysis,
                        'execution_summary': {
                            'total_steps': execution_result['total_steps'],
                            'completed_steps': execution_result['completed_steps'],
                            'execution_results': execution_result['execution_results']
                        }
                    }

                    # æ·»åŠ ä¸‹è½½æ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if downloads:
                        response_data['downloads'] = downloads

                    print(f"LLMé©±åŠ¨çš„ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼Œè¿”å›å“åº”")
                    return safe_jsonify(response_data)

                else:
                    # æ‰§è¡Œå¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯ä¿¡æ¯
                    error_message = f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {execution_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                    print(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {error_message}")

            else:
                # å‰ç½®æ¡ä»¶ä¸æ»¡è¶³ï¼Œç”Ÿæˆæç¤ºä¿¡æ¯
                missing = prerequisites.get('missing_prerequisites', [])
                missing_text = []

                if 'inp_file' in missing:
                    missing_text.append("ç®¡ç½‘INPæ–‡ä»¶")
                if 'partition_csv' in missing:
                    missing_text.append("åˆ†åŒºCSVæ–‡ä»¶ï¼ˆéœ€è¦å…ˆè¿›è¡Œç®¡ç½‘åˆ†åŒºï¼‰")
                if 'trained_model' in missing:
                    missing_text.append("è®­ç»ƒå¥½çš„æ¼æŸæ£€æµ‹æ¨¡å‹")

                error_message = f"ç¼ºå°‘å¿…è¦çš„å‰ç½®æ¡ä»¶: {', '.join(missing_text)}ã€‚è¯·å…ˆå®Œæˆç›¸å…³æ­¥éª¤ã€‚"
                print(f"å‰ç½®æ¡ä»¶ä¸æ»¡è¶³: {error_message}")

        else:
            # CSVæ¨ç†åœºæ™¯ï¼Œç›´æ¥è·³è½¬åˆ°ç®€åŒ–æ¨ç†é€»è¾‘
            print("ğŸ¯ è·³è¿‡LLMåˆ†æï¼Œç›´æ¥è¿›å…¥CSVæ¨ç†æ¨¡å¼")

        # å¦‚æœLLMåˆ†æå¤±è´¥æˆ–ä¸éœ€è¦æ‰§è¡Œæ™ºèƒ½ä½“ä»»åŠ¡ï¼Œå›é€€åˆ°åŸæœ‰é€»è¾‘
        print("å›é€€åˆ°åŸæœ‰çš„æ™ºèƒ½ä½“å¤„ç†é€»è¾‘")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ä¸“é—¨çš„æ™ºèƒ½ä½“å¤„ç†
        should_use_partition_sim = False
        should_use_hydro_sim = False
        should_use_sensor_placement = False
        should_use_leak_detection = False
        agent_inp_file_path = None
        partition_csv_path = None

        # å¤„ç†CSVæ–‡ä»¶çš„æ¼æŸæ£€æµ‹ - ç®€åŒ–æ¨ç†æ¨¡å¼
        if is_csv_file and csv_file_path:
            print(f"\n" + "="*60)
            print(f"ğŸ” æ£€æµ‹åˆ°CSVæ–‡ä»¶ä¸Šä¼ ï¼Œå¼€å§‹æ™ºèƒ½æ¨ç†æ¨¡å¼...")
            print(f"ğŸ“‚ CSVæ–‡ä»¶: {os.path.basename(csv_file_path)}")
            print(f"ğŸ†” å¯¹è¯ID: {conversation_id}")
            print(f"ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            print("="*60)

            # ç®€åŒ–çš„å‰ç½®æ¡ä»¶æ£€æŸ¥ï¼šåªéœ€è¦è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
            missing_prerequisites = []

            # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶è¿›è¡Œç»´åº¦å…¼å®¹æ€§æ£€æŸ¥
            model_file_path = None
            if os.path.exists('downloads'):
                # é¦–å…ˆè¯»å–CSVæ–‡ä»¶ç¡®å®šåˆ—æ•°
                csv_columns = 0
                try:
                    import pandas as pd
                    df_temp = pd.read_csv(csv_file_path)
                    csv_columns = len(df_temp.columns)
                    print(f"ğŸ“Š CSVæ–‡ä»¶åˆ—æ•°: {csv_columns}")
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–CSVæ–‡ä»¶åˆ—æ•°: {e}")

                model_files = []
                compatible_models = []

                # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
                for filename in os.listdir('downloads'):
                    if ('leak_detection_model' in filename and filename.endswith('.pth')):
                        model_files.append(filename)

                print(f"ğŸ“‹ æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œæ£€æŸ¥å…¼å®¹æ€§...")

                # æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§
                for model_file in model_files:
                    model_path = os.path.join('downloads', model_file)
                    try:
                        import torch
                        checkpoint = torch.load(model_path, map_location='cpu')

                        if 'model_state_dict' in checkpoint:
                            state_dict = checkpoint['model_state_dict']
                        else:
                            state_dict = checkpoint

                        # æŸ¥æ‰¾ç¬¬ä¸€å±‚çš„æƒé‡æ¥ç¡®å®šè¾“å…¥ç»´åº¦
                        first_layer_key = None
                        for key in state_dict.keys():
                            if 'weight' in key and ('fc1' in key or 'linear' in key or '0' in key):
                                first_layer_key = key
                                break

                        if first_layer_key:
                            input_dim = state_dict[first_layer_key].shape[1]
                            print(f"   {model_file}: è¾“å…¥ç»´åº¦={input_dim}", end="")

                            if csv_columns > 0 and input_dim == csv_columns:
                                compatible_models.append(model_file)
                                print(f" âœ… å…¼å®¹")
                            else:
                                print(f" âŒ ä¸å…¼å®¹ (éœ€è¦{input_dim}åˆ—ï¼ŒCSVæœ‰{csv_columns}åˆ—)")
                        else:
                            print(f"   {model_file}: â“ æ— æ³•ç¡®å®šè¾“å…¥ç»´åº¦")

                    except Exception as e:
                        print(f"   {model_file}: âŒ æ£€æŸ¥å¤±è´¥: {e}")

                # é€‰æ‹©æ¨¡å‹ - ä¸¥æ ¼åŸºäºå¯¹è¯IDåŒ¹é…
                if compatible_models:
                    selected_model = None
                    conversation_prefix = conversation_id[:8]

                    print(f"ğŸ¯ æ¨¡å‹é€‰æ‹©ç­–ç•¥:")
                    print(f"   - å½“å‰å¯¹è¯IDå‰ç¼€: {conversation_prefix}")
                    print(f"   - å…¼å®¹æ¨¡å‹æ•°é‡: {len(compatible_models)}")

                    # åªé€‰æ‹©å½“å‰å¯¹è¯IDçš„å…¼å®¹æ¨¡å‹
                    for model in compatible_models:
                        if conversation_prefix in model:
                            selected_model = model
                            print(f"   âœ… æ‰¾åˆ°å½“å‰å¯¹è¯IDçš„å…¼å®¹æ¨¡å‹: {model}")
                            break

                    # å¦‚æœå½“å‰å¯¹è¯æ²¡æœ‰å¯¹åº”æ¨¡å‹ï¼Œç›´æ¥è¿”å›é”™è¯¯
                    if not selected_model:
                        print(f"   âŒ å½“å‰å¯¹è¯ID {conversation_prefix} æ²¡æœ‰å¯¹åº”çš„è®­ç»ƒæ¨¡å‹")
                        print(f"   ğŸ’¡ å¯ç”¨çš„å…¼å®¹æ¨¡å‹å¯¹è¯ID:")
                        available_conversation_ids = set()
                        for model in compatible_models:
                            # æå–æ¨¡å‹æ–‡ä»¶ä¸­çš„å¯¹è¯ID
                            parts = model.split('_')
                            if len(parts) >= 4:
                                model_conversation_id = parts[3]
                                available_conversation_ids.add(model_conversation_id)

                        for conv_id in sorted(available_conversation_ids):
                            print(f"     - {conv_id}")

                        missing_prerequisites.append(f"å¯¹è¯ID {conversation_prefix} å¯¹åº”çš„æ¼æŸæ£€æµ‹æ¨¡å‹")
                        print(f"   ğŸ”§ å»ºè®®: è¯·å…ˆä¸ºå½“å‰å¯¹è¯è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨æœ‰å¯¹åº”æ¨¡å‹çš„å¯¹è¯")
                    else:
                        model_file_path = os.path.join('downloads', selected_model)
                        print(f"ğŸ† æœ€ç»ˆé€‰æ‹©æ¨¡å‹: {selected_model}")
                else:
                    missing_prerequisites.append("å…¼å®¹çš„æ¼æŸæ£€æµ‹æ¨¡å‹")
                    print(f"âŒ æœªæ‰¾åˆ°ä¸CSVæ–‡ä»¶å…¼å®¹çš„æ¨¡å‹")
                    if model_files:
                        print(f"ğŸ’¡ å»ºè®®: ä½¿ç”¨å¯¹åº”çš„è®­ç»ƒæ¨¡å‹æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹")
            else:
                missing_prerequisites.append("æ¼æŸæ£€æµ‹æ¨¡å‹")
                print(f"âŒ downloadsç›®å½•ä¸å­˜åœ¨")

            # å‰ç½®æ¡ä»¶æ£€æŸ¥ç»“æœ
            if missing_prerequisites:
                print(f"âŒ ç¼ºå°‘å‰ç½®æ¡ä»¶: {', '.join(missing_prerequisites)}")

                # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¹è¯IDä¸åŒ¹é…çš„é—®é¢˜
                conversation_prefix = conversation_id[:8]
                is_conversation_mismatch = any(f"å¯¹è¯ID {conversation_prefix}" in item for item in missing_prerequisites)

                if is_conversation_mismatch:
                    detailed_error = f"""
âŒ **æ— æ³•æ‰¾åˆ°å¯¹åº”çš„PTHæ¨¡å‹æ–‡ä»¶**

**é—®é¢˜**ï¼šå½“å‰å¯¹è¯ID `{conversation_prefix}` æ²¡æœ‰å¯¹åº”çš„è®­ç»ƒæ¨¡å‹

**åŸå› **ï¼šæ¨ç†åŠŸèƒ½éœ€è¦ä½¿ç”¨ä¸å½“å‰å¯¹è¯IDåŒ¹é…çš„è®­ç»ƒæ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **æ¨èæ–¹æ¡ˆ**ï¼šè¯·å…ˆä¸ºå½“å‰å¯¹è¯è¿›è¡Œæ¼æŸæ¨¡å‹è®­ç»ƒ
   - è¾“å…¥è®­ç»ƒæŒ‡ä»¤ï¼ˆå¦‚ï¼š"æ¼æŸæ¨¡å‹è®­ç»ƒï¼Œè¿­ä»£æ¬¡æ•°ä¸º100æ¬¡ï¼Œç”Ÿæˆæ•°æ®ä¸º50ç»„"ï¼‰
   - ç­‰å¾…è®­ç»ƒå®Œæˆåå†è¿›è¡Œæ¨ç†

2. **æ›¿ä»£æ–¹æ¡ˆ**ï¼šä½¿ç”¨å·²æœ‰æ¨¡å‹çš„å¯¹è¯è¿›è¡Œæ¨ç†
   - åˆ‡æ¢åˆ°æœ‰å¯¹åº”è®­ç»ƒæ¨¡å‹çš„å¯¹è¯
   - ä¸Šä¼ ç›¸åŒæ ¼å¼çš„CSVæ–‡ä»¶è¿›è¡Œæ¨ç†

**æŠ€æœ¯è¯´æ˜**ï¼šç³»ç»Ÿé‡‡ç”¨ä¸¥æ ¼çš„å¯¹è¯IDåŒ¹é…ç­–ç•¥ï¼Œç¡®ä¿æ¨ç†ç»“æœçš„å‡†ç¡®æ€§å’Œå¯è¿½æº¯æ€§ã€‚
"""
                else:
                    detailed_error = f"""
âŒ **æ¼æŸæ£€æµ‹æ¨ç†å¤±è´¥**

ğŸš« **ç¼ºå°‘å¿…è¦æ¡ä»¶**

{chr(10).join([f'â€¢ {item}' for item in missing_prerequisites])}

ğŸ“‹ **è§£å†³æ–¹æ¡ˆ**ï¼š
1. **å¦‚æœæ²¡æœ‰æ¨¡å‹**: è¯·å…ˆå®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š
   - ä¸Šä¼ ç®¡ç½‘INPæ–‡ä»¶
   - è¿›è¡Œç®¡ç½‘åˆ†åŒºåˆ†æ
   - è¿›è¡Œä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–
   - è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹

2. **å¦‚æœæœ‰æ¨¡å‹**: è¯·ç¡®è®¤æ¨¡å‹æ–‡ä»¶åœ¨downloadsç›®å½•ä¸­

ğŸ’¡ **å¿«é€Ÿæ£€æŸ¥**: åœ¨downloadsç›®å½•ä¸­æŸ¥æ‰¾ç±»ä¼¼ `leak_detection_model_{conversation_prefix}_*.pth` çš„æ–‡ä»¶
"""

                return jsonify({
                    'response': detailed_error,
                    'conversation_id': conversation_id,
                    'error': True,
                    'error_type': 'missing_model' if is_conversation_mismatch else 'missing_prerequisites',
                    'missing_prerequisites': missing_prerequisites,
                    'conversation_mismatch': is_conversation_mismatch
                })

            # å‰ç½®æ¡ä»¶æ»¡è¶³ï¼Œè¿›è¡Œç®€åŒ–æ¨ç†
            if model_file_path:
                try:
                    # ç›´æ¥è¿›è¡Œæ¼æŸæ£€æµ‹æ¨ç†ï¼Œç®€åŒ–æµç¨‹
                    print(f"ğŸš€ å¼€å§‹æ¼æŸæ£€æµ‹æ¨ç†...")
                    print(f"   ğŸ“‚ æ¨¡å‹æ–‡ä»¶: {os.path.basename(model_file_path)}")
                    print(f"   ğŸ“Š ä¼ æ„Ÿå™¨æ•°æ®: {os.path.basename(csv_file_path)}")

                    # ç›´æ¥è°ƒç”¨æ¨ç†æ–¹æ³•ï¼Œä¼ é€’conversation_idä»¥è¯»å–åˆ†åŒºæ–‡ä»¶
                    detection_result = leak_detection_agent.detect_leak_from_file(csv_file_path, model_file_path, conversation_id)

                    # å¤„ç†æ¨ç†ç»“æœ
                    if detection_result.get('success'):
                        results = detection_result.get('results', [])
                        summary = detection_result.get('summary', {})

                        # æ‰“å°è¯¦ç»†æ¨ç†ç»“æœ
                        print("\n" + "="*60)
                        print("ğŸ¯ æ¼æŸæ£€æµ‹æ¨ç†ç»“æœ")
                        print("="*60)

                        print(f"ğŸ“Š æ¨ç†ç»“æœæ‘˜è¦:")
                        print(f"   - æ€»æ ·æœ¬æ•°: {summary.get('total_samples', 0)}")
                        print(f"   - æ­£å¸¸æ ·æœ¬: {summary.get('normal_samples', 0)}")
                        print(f"   - å¼‚å¸¸æ ·æœ¬: {summary.get('anomaly_samples', 0)}")

                        print(f"\nğŸ“‹ è¯¦ç»†æ¨ç†ç»“æœ:")
                        for result in results:
                            sample_id = result.get('sample_id', 0)
                            status = result.get('status', 'N/A')
                            partition = result.get('partition', None)
                            confidence = result.get('confidence', 0)

                            if status == 'æ­£å¸¸':
                                print(f"   æ ·æœ¬{sample_id}: {status} (ç½®ä¿¡åº¦: {confidence:.3f})")
                            else:
                                print(f"   æ ·æœ¬{sample_id}: {status} - åˆ†åŒº{partition} (ç½®ä¿¡åº¦: {confidence:.3f})")

                        # å¼‚å¸¸åˆ†å¸ƒç»Ÿè®¡
                        if summary.get('anomaly_samples', 0) > 0:
                            partition_stats = {}
                            for result in results:
                                if result.get('status') == 'å¼‚å¸¸':
                                    partition = result.get('partition')
                                    if partition:
                                        partition_stats[partition] = partition_stats.get(partition, 0) + 1

                            if partition_stats:
                                print(f"\nâš ï¸ å¼‚å¸¸åˆ†å¸ƒç»Ÿè®¡:")
                                for partition, count in sorted(partition_stats.items()):
                                    print(f"   - åˆ†åŒº{partition}: {count}ä¸ªå¼‚å¸¸æ ·æœ¬")

                        print("="*60)

                        # ç”Ÿæˆæ¨ç†ç»“æœCSVæ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        conversation_prefix = conversation_id[:8]
                        inference_result_filename = f"leak_inference_result_{conversation_prefix}_{timestamp}.csv"
                        inference_result_path = os.path.join(DOWNLOADS_FOLDER, inference_result_filename)

                        # ä¿å­˜æ¨ç†ç»“æœåˆ°CSV
                        import pandas as pd
                        results_data = []
                        for result in results:
                            results_data.append({
                                'æ ·æœ¬åºå·': result.get('sample_id', 0),
                                'æ£€æµ‹çŠ¶æ€': result.get('status', 'N/A'),
                                'å¼‚å¸¸åˆ†åŒº': result.get('partition', '') if result.get('status') == 'å¼‚å¸¸' else '',
                                'ç½®ä¿¡åº¦': f"{result.get('confidence', 0):.4f}",
                                'ç½®ä¿¡åº¦ç™¾åˆ†æ¯”': f"{result.get('confidence', 0):.1%}"
                            })

                        df_results = pd.DataFrame(results_data)
                        df_results.to_csv(inference_result_path, index=False, encoding='utf-8-sig')

                        # æ·»åŠ åˆ°ä¸‹è½½æ–‡ä»¶åˆ—è¡¨
                        download_url = f"/download/{inference_result_filename}"
                        file_size = os.path.getsize(inference_result_path) if os.path.exists(inference_result_path) else 0

                        detection_result['download_files'] = [{
                            'filename': inference_result_filename,
                            'path': inference_result_path,
                            'url': download_url,
                            'download_url': download_url,
                            'type': 'csv',
                            'size': file_size,
                            'description': 'æ¼æŸæ£€æµ‹æ¨ç†ç»“æœ',
                            'step': 'æ¼æŸæ£€æµ‹æ¨ç†',
                            'records_count': len(results)
                        }]

                        print(f"ğŸ’¾ æ¨ç†ç»“æœå·²ä¿å­˜: {inference_result_filename}")
                        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {inference_result_path}")
                        print(f"ğŸ”— ä¸‹è½½URL: {download_url}")
                        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

                    if detection_result['success']:
                        # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„ä¸“ä¸špromptè°ƒç”¨GPT
                        prompt = leak_detection_agent.build_response_prompt(detection_result, user_message, "detection")

                        # æ·»åŠ æ¨ç†ç»“æœæ–‡ä»¶ä¿¡æ¯åˆ°promptä¸­
                        if detection_result.get('download_files'):
                            prompt += f"""

ğŸ“ **æ¨ç†ç»“æœæ–‡ä»¶å·²ç”Ÿæˆ**

ç³»ç»Ÿå·²ç”Ÿæˆè¯¦ç»†çš„æ¨ç†ç»“æœæ–‡ä»¶ï¼ŒåŒ…å«æ¯ä¸ªæ ·æœ¬çš„æ£€æµ‹çŠ¶æ€ã€å¼‚å¸¸åˆ†åŒºå’Œç½®ä¿¡åº¦ä¿¡æ¯ï¼š

"""
                            for file_info in detection_result.get('download_files', []):
                                prompt += f"â€¢ **{file_info['description']}**: `{file_info['filename']}`\n"

                        prompt += f"""

ğŸ¯ **æ¨ç†æ¨¡å¼è¯´æ˜**: ç³»ç»Ÿæ£€æµ‹åˆ°å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼Œç›´æ¥è¿›è¡Œæ¨ç†åˆ†æï¼Œè·³è¿‡äº†åˆ†åŒºåˆ†æã€ä¼ æ„Ÿå™¨å¸ƒç½®ã€æ¨¡å‹è®­ç»ƒç­‰æ­¥éª¤ï¼Œå¤§å¹…æå‡äº†å¤„ç†æ•ˆç‡ã€‚

ğŸ“Š **ä½¿ç”¨çš„èµ„æº**:
- æ¨¡å‹æ–‡ä»¶: `{os.path.basename(model_file_path)}`
- ä¼ æ„Ÿå™¨æ•°æ®: `{os.path.basename(csv_file_path)}`

è¯·åŸºäºä»¥ä¸Šæ¨ç†ç»“æœç”Ÿæˆä¸“ä¸šçš„æ¼æŸæ£€æµ‹åˆ†ææŠ¥å‘Šã€‚
"""

                        messages = [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»™æ°´ç®¡ç½‘æ¼æŸæ£€æµ‹ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„å¼‚å¸¸æ£€æµ‹å’Œæ•…éšœè¯Šæ–­ç»éªŒã€‚è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š\n\nç¥å¥½ï¼Œ\n\nTianwei Mu\nGuangzhou Institute of Industrial Intelligence"}
                        ]

                        # æ·»åŠ å½“å‰å¯¹è¯çš„å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘10è½®ï¼‰
                        for msg in current_conversation['messages'][-10:]:
                            messages.append({"role": "user", "content": msg['user']})
                            messages.append({"role": "assistant", "content": msg['assistant']})

                        messages.append({"role": "user", "content": prompt})

                        # è°ƒç”¨OpenAI API
                        response = openai.ChatCompletion.create(
                            model="gpt-4-turbo-preview",
                            messages=messages,
                            max_tokens=4000,
                            temperature=0.7
                        )

                        assistant_message = response.choices[0].message.content

                        # ä¿å­˜åˆ°å½“å‰å¯¹è¯
                        message_data = {
                            'user': user_message,
                            'assistant': assistant_message,
                            'timestamp': datetime.now().isoformat(),
                            'intent': 'leak_detection',
                            'confidence': 0.9,
                            'has_file': True,
                            'file_type': 'csv',
                            'file_path': csv_file_path,
                            'detection_results': detection_result
                        }

                        current_conversation['messages'].append(message_data)

                        # ä¿å­˜åˆ°æ–‡ä»¶
                        save_conversation_to_file(conversation_id, current_conversation)
                        # æ›´æ–°å¯¹è¯ç´¢å¼•
                        all_conversations = load_all_conversations()
                        all_conversations[conversation_id] = current_conversation
                        save_conversations_index(all_conversations)
                        session.modified = True

                        # æ„å»ºå“åº”
                        response_data = {
                            'success': True,
                            'response': assistant_message,
                            'conversation_id': conversation_id,
                            'intent': 'leak_detection_inference',
                            'confidence': 0.9,
                            'detection_results': detection_result,
                            'inference_mode': True,
                            'model_used': os.path.basename(model_file_path),
                            'workflow_skipped': ['åˆ†åŒºåˆ†æ', 'ä¼ æ„Ÿå™¨å¸ƒç½®', 'æ¨¡å‹è®­ç»ƒ']
                        }

                        # æ·»åŠ ä¸‹è½½æ–‡ä»¶ä¿¡æ¯
                        download_files = detection_result.get('download_files', [])
                        if download_files:
                            response_data['downloads'] = download_files
                            print(f"ğŸ“ æ·»åŠ ä¸‹è½½æ–‡ä»¶åˆ°å“åº”: {len(download_files)} ä¸ªæ–‡ä»¶")
                            for i, file_info in enumerate(download_files):
                                print(f"   æ–‡ä»¶{i+1}: {file_info.get('filename', 'N/A')}")
                                print(f"     - URL: {file_info.get('url', 'N/A')}")
                                print(f"     - å¤§å°: {file_info.get('size', 'N/A')} å­—èŠ‚")
                        else:
                            print(f"âŒ æ²¡æœ‰ä¸‹è½½æ–‡ä»¶æ·»åŠ åˆ°å“åº”")

                        print(f"ğŸ“¤ è¿”å›æ¨ç†å“åº”ï¼ŒåŒ…å« {len(response_data.get('downloads', []))} ä¸ªä¸‹è½½æ–‡ä»¶")
                        return jsonify(response_data)

                    else:
                        # æ£€æµ‹å¤±è´¥
                        full_message = f"æ¼æŸæ£€æµ‹å¤±è´¥ï¼š{detection_result.get('error', 'æœªçŸ¥é”™è¯¯')}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

                except Exception as e:
                    print(f"æ¼æŸæ£€æµ‹å¤„ç†é”™è¯¯: {e}")
                    full_message = f"å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

            else:
                # æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶
                full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ä¼ æ„Ÿå™¨æ•°æ®CSVæ–‡ä»¶ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æ¼æŸæ£€æµ‹æ¨¡å‹ã€‚è¯·å…ˆè®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

        # ç¡®å®šå¯ç”¨çš„inpæ–‡ä»¶è·¯å¾„
        elif is_inp_file and inp_file_path:
            # æ–°ä¸Šä¼ çš„.inpæ–‡ä»¶
            agent_inp_file_path = inp_file_path
        elif current_conversation and has_inp_file_in_conversation_history(current_conversation):
            # å¯¹è¯å†å²ä¸­æœ‰.inpæ–‡ä»¶
            historical_inp_path = get_inp_file_from_conversation_history(current_conversation)
            if historical_inp_path:
                agent_inp_file_path = historical_inp_path

        # å¦‚æœæœ‰å¯ç”¨çš„inpæ–‡ä»¶ï¼Œåˆ¤æ–­ä½¿ç”¨å“ªä¸ªæ™ºèƒ½ä½“
        if agent_inp_file_path:
            # å®šä¹‰å…³é”®è¯
            partition_keywords = ['åˆ†åŒº', 'èšç±»', 'FCM', 'æ¨¡ç³Šèšç±»', 'clustering', 'partition', 'åŒºåŸŸåˆ’åˆ†', 'ç®¡ç½‘åˆ’åˆ†', 'ç¦»ç¾¤ç‚¹']
            sensor_keywords = ['ä¼ æ„Ÿå™¨', 'ç›‘æµ‹ç‚¹', 'å‹åŠ›ç›‘æµ‹', 'sensor', 'monitoring', 'éŸ§æ€§', 'æ•æ„Ÿåº¦', 'å¸ƒç½®', 'ä¼˜åŒ–', 'æ£€æµ‹ç‚¹']
            leak_keywords = ['æ¼æŸ', 'æ³„æ¼', 'æ¼æ°´', 'å¼‚å¸¸æ£€æµ‹', 'æ•…éšœæ£€æµ‹', 'leak', 'leakage', 'æ¼æŸæ£€æµ‹', 'æ¼æŸåˆ†æ', 'è®­ç»ƒæ¨¡å‹', 'æ¼æŸæ¨¡å‹']

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¼æŸæ£€æµ‹ç›¸å…³çš„è¯·æ±‚
            if any(keyword in user_message for keyword in leak_keywords):
                should_use_leak_detection = True
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼ æ„Ÿå™¨å¸ƒç½®ç›¸å…³çš„è¯·æ±‚
            elif any(keyword in user_message for keyword in sensor_keywords):
                should_use_sensor_placement = True
                # æ£€æŸ¥æ˜¯å¦æœ‰å†å²åˆ†åŒºç»“æœ
                if current_conversation:
                    partition_csv_path = get_partition_csv_from_conversation_history(current_conversation)
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†åŒºç›¸å…³çš„è¯·æ±‚
            elif any(keyword in user_message for keyword in partition_keywords):
                should_use_partition_sim = True
            else:
                # é»˜è®¤ä½¿ç”¨æ°´åŠ›ä»¿çœŸæ™ºèƒ½ä½“
                should_use_hydro_sim = True

        # ä½¿ç”¨PartitionSimæ™ºèƒ½ä½“å¤„ç†
        if should_use_partition_sim and agent_inp_file_path:
            try:
                partition_result = partition_sim_agent.process(agent_inp_file_path, user_message, conversation_id)

                if partition_result['success']:
                    # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„ä¸“ä¸špromptè°ƒç”¨GPT
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»™æ°´ç®¡ç½‘åˆ†åŒºåˆ†æä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„ç®¡ç½‘èšç±»å’Œåˆ†åŒºä¼˜åŒ–ç»éªŒã€‚è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š\n\nç¥å¥½ï¼Œ\n\nTianwei Mu\nGuangzhou Institute of Industrial Intelligence"}
                    ]

                    # æ·»åŠ å½“å‰å¯¹è¯çš„å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘10è½®ï¼‰
                    for msg in current_conversation['messages'][-10:]:
                        messages.append({"role": "user", "content": msg['user']})
                        messages.append({"role": "assistant", "content": msg['assistant']})

                    # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„ä¸“ä¸šprompt
                    messages.append({"role": "user", "content": partition_result['prompt']})

                    # è°ƒç”¨OpenAI API
                    response = openai.ChatCompletion.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        max_tokens=4000,
                        temperature=0.7
                    )

                    assistant_message = response.choices[0].message.content

                    # ä¿å­˜åˆ°å½“å‰å¯¹è¯
                    message_data = {
                        'user': user_message,
                        'assistant': assistant_message,
                        'timestamp': datetime.now().isoformat(),
                        'intent': partition_result['intent'],
                        'confidence': partition_result['confidence']
                    }

                    # å¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œä¿å­˜æ–‡ä»¶ä¿¡æ¯
                    if is_inp_file and inp_file_path:
                        message_data.update({
                            'has_file': True,
                            'file_type': 'inp',
                            'file_path': inp_file_path
                        })
                    else:
                        # ä½¿ç”¨å†å²æ–‡ä»¶çš„å¯¹è¯
                        message_data.update({
                            'has_file': False,
                            'uses_historical_file': True,
                            'historical_file_path': agent_inp_file_path
                        })

                    current_conversation['messages'].append(message_data)

                    # æ›´æ–°å¯¹è¯æ—¶é—´
                    current_conversation['updated_at'] = datetime.now().isoformat()

                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸”æ ‡é¢˜æ˜¯é»˜è®¤çš„ï¼Œæ›´æ–°æ ‡é¢˜
                    if len(current_conversation['messages']) == 1 and current_conversation['title'] == 'æ–°å¯¹è¯':
                        current_conversation['title'] = generate_conversation_title(user_message or "ç®¡ç½‘åˆ†åŒºåˆ†æ")

                    # é™åˆ¶æ¯ä¸ªå¯¹è¯çš„æ¶ˆæ¯æ•°é‡
                    if len(current_conversation['messages']) > 50:
                        current_conversation['messages'] = current_conversation['messages'][-50:]

                    # ä¿å­˜åˆ°æ–‡ä»¶
                    save_conversation_to_file(conversation_id, current_conversation)
                    # æ›´æ–°å¯¹è¯ç´¢å¼•
                    all_conversations = load_all_conversations()
                    all_conversations[conversation_id] = current_conversation
                    save_conversations_index(all_conversations)
                    session.modified = True

                    # æ„å»ºå“åº”
                    response_data = {
                        'success': True,
                        'response': assistant_message,
                        'conversation_id': conversation_id,
                        'intent': partition_result['intent'],
                        'confidence': partition_result['confidence'],
                        'partition_info': partition_result.get('partition_info', {})
                    }

                    # å¦‚æœæœ‰CSVæ–‡ä»¶ç”Ÿæˆï¼Œæ·»åŠ ä¸‹è½½ä¿¡æ¯
                    if partition_result.get('csv_info') and partition_result['csv_info']['success']:
                        response_data['download'] = {
                            'available': True,
                            'filename': partition_result['csv_info']['filename'],
                            'url': partition_result['csv_info']['download_url'],
                            'size': partition_result['csv_info']['file_size'],
                            'records_count': partition_result['csv_info']['records_count']
                        }

                    # å¦‚æœæœ‰å¯è§†åŒ–å›¾åƒç”Ÿæˆï¼Œæ·»åŠ æ˜¾ç¤ºä¿¡æ¯
                    if partition_result.get('visualization'):
                        response_data['visualization'] = {
                            'available': True,
                            'filename': partition_result['visualization']['filename'],
                            'url': f'/static_files/{partition_result["visualization"]["filename"]}',
                            'download_url': f'/download/{partition_result["visualization"]["filename"]}'
                        }

                    return jsonify(response_data)

                else:
                    # æ™ºèƒ½ä½“å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ–¹å¼å¤„ç†
                    full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†åˆ†åŒºåˆ†ææ—¶é‡åˆ°é—®é¢˜ï¼š{partition_result.get('response', 'æœªçŸ¥é”™è¯¯')}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

            except Exception as e:
                print(f"PartitionSimæ™ºèƒ½ä½“å¤„ç†é”™è¯¯: {e}")
                full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†åˆ†åŒºæ™ºèƒ½ä½“å¤„ç†æ—¶å‡ºç°é”™è¯¯ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

        # ä½¿ç”¨SensorPlacementæ™ºèƒ½ä½“å¤„ç†
        elif should_use_sensor_placement and agent_inp_file_path:
            try:
                # å¦‚æœæ²¡æœ‰åˆ†åŒºç»“æœï¼Œå…ˆè¿›è¡Œè‡ªåŠ¨åˆ†åŒº
                if not partition_csv_path:
                    print("æ²¡æœ‰æ‰¾åˆ°å†å²åˆ†åŒºç»“æœï¼Œå¼€å§‹è‡ªåŠ¨åˆ†åŒº...")
                    partition_result = partition_sim_agent.process(
                        agent_inp_file_path,
                        "è‡ªåŠ¨åˆ†åŒºç”¨äºä¼ æ„Ÿå™¨å¸ƒç½®ï¼Œåˆ†æˆ6ä¸ªåˆ†åŒº",
                        conversation_id
                    )

                    if partition_result['success'] and partition_result.get('csv_info'):
                        partition_csv_path = partition_result['csv_info']['filepath']
                        print(f"è‡ªåŠ¨åˆ†åŒºå®Œæˆï¼Œåˆ†åŒºæ–‡ä»¶: {partition_csv_path}")
                    else:
                        return jsonify({
                            'success': False,
                            'error': f'è‡ªåŠ¨åˆ†åŒºå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œä¼ æ„Ÿå™¨å¸ƒç½®: {partition_result.get("response", "æœªçŸ¥é”™è¯¯")}'
                        })
                else:
                    print(f"ä½¿ç”¨å†å²åˆ†åŒºç»“æœ: {partition_csv_path}")

                # è¿›è¡Œä¼ æ„Ÿå™¨å¸ƒç½®
                sensor_result = sensor_placement_agent.process(
                    agent_inp_file_path,
                    partition_csv_path,
                    user_message,
                    conversation_id
                )

                if sensor_result['success']:
                    # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„ä¸“ä¸špromptè°ƒç”¨GPT
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»™æ°´ç®¡ç½‘ä¼ æ„Ÿå™¨å¸ƒç½®ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„å‹åŠ›ç›‘æµ‹ç‚¹ä¼˜åŒ–å’ŒéŸ§æ€§åˆ†æç»éªŒã€‚è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š\n\nç¥å¥½ï¼Œ\n\nTianwei Mu\nGuangzhou Institute of Industrial Intelligence"}
                    ]

                    # æ·»åŠ å½“å‰å¯¹è¯çš„å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘10è½®ï¼‰
                    for msg in current_conversation['messages'][-10:]:
                        messages.append({"role": "user", "content": msg['user']})
                        messages.append({"role": "assistant", "content": msg['assistant']})

                    # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„ä¸“ä¸šprompt
                    messages.append({"role": "user", "content": sensor_result['prompt']})

                    # è°ƒç”¨OpenAI API
                    response = openai.ChatCompletion.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        max_tokens=4000,
                        temperature=0.7
                    )

                    assistant_message = response.choices[0].message.content

                    # ä¿å­˜åˆ°å½“å‰å¯¹è¯
                    message_data = {
                        'user': user_message,
                        'assistant': assistant_message,
                        'timestamp': datetime.now().isoformat(),
                        'intent': 'sensor_placement',
                        'confidence': 0.9
                    }

                    # å¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œä¿å­˜æ–‡ä»¶ä¿¡æ¯
                    if is_inp_file and inp_file_path:
                        message_data.update({
                            'has_file': True,
                            'file_type': 'inp',
                            'file_path': inp_file_path
                        })
                    else:
                        # ä½¿ç”¨å†å²æ–‡ä»¶çš„å¯¹è¯
                        message_data.update({
                            'has_file': False,
                            'uses_historical_file': True,
                            'historical_file_path': agent_inp_file_path
                        })

                    # æ·»åŠ ä¼ æ„Ÿå™¨å¸ƒç½®ç»“æœä¿¡æ¯
                    if sensor_result.get('csv_info'):
                        message_data['csv_info'] = sensor_result['csv_info']
                        print(message_data['csv_info'])
                    # æ·»åŠ éŸ§æ€§åˆ†æç»“æœä¿¡æ¯
                    if sensor_result.get('resilience_csv_info'):
                        message_data['resilience_csv_info'] = sensor_result['resilience_csv_info']
                        print(message_data['resilience_csv_info'])
                    current_conversation['messages'].append(message_data)

                    # ä¿å­˜åˆ°æ–‡ä»¶
                    save_conversation_to_file(conversation_id, current_conversation)
                    # æ›´æ–°å¯¹è¯ç´¢å¼•
                    all_conversations = load_all_conversations()
                    all_conversations[conversation_id] = current_conversation
                    save_conversations_index(all_conversations)
                    session.modified = True

                    # æ„å»ºå“åº”
                    response_data = {
                        'success': True,
                        'response': assistant_message,
                        'conversation_id': conversation_id,
                        'intent': 'sensor_placement',
                        'confidence': 0.9,
                        'sensor_info': sensor_result.get('sensor_info', {})
                    }

                    # å¦‚æœæœ‰CSVæ–‡ä»¶ç”Ÿæˆï¼Œæ·»åŠ ä¸‹è½½ä¿¡æ¯
                    if sensor_result.get('csv_info') and sensor_result['csv_info']['success']:
                        response_data['download'] = {
                            'available': True,
                            'filename': sensor_result['csv_info']['filename'],
                            'url': sensor_result['csv_info']['download_url'],
                            'size': sensor_result['csv_info']['file_size'],
                            'sensor_count': sensor_result['csv_info']['sensor_count']
                        }

                    # å¦‚æœæœ‰éŸ§æ€§åˆ†ææ–‡ä»¶ç”Ÿæˆï¼Œæ·»åŠ ä¸‹è½½ä¿¡æ¯
                    if sensor_result.get('resilience_csv_info'):
                        response_data['resilience_download'] = {
                            'available': True,
                            'filename': os.path.basename(sensor_result['resilience_csv_info']),
                            'url': f'/download/{os.path.basename(sensor_result["resilience_csv_info"])}'
                        }

                    # å¦‚æœæœ‰å¯è§†åŒ–å›¾åƒç”Ÿæˆï¼Œæ·»åŠ æ˜¾ç¤ºä¿¡æ¯
                    if sensor_result.get('visualization'):
                        response_data['visualization'] = {
                            'available': True,
                            'filename': sensor_result['visualization']['filename'],
                            'url': f'/static_files/{sensor_result["visualization"]["filename"]}',
                            'download_url': f'/download/{sensor_result["visualization"]["filename"]}'
                        }

                    return jsonify(response_data)

                else:
                    # æ™ºèƒ½ä½“å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ–¹å¼å¤„ç†
                    full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†ä¼ æ„Ÿå™¨å¸ƒç½®æ—¶é‡åˆ°é—®é¢˜ï¼š{sensor_result.get('response', 'æœªçŸ¥é”™è¯¯')}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

            except Exception as e:
                print(f"SensorPlacementæ™ºèƒ½ä½“å¤„ç†é”™è¯¯: {e}")
                full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†ä¼ æ„Ÿå™¨å¸ƒç½®æ™ºèƒ½ä½“å¤„ç†æ—¶å‡ºç°é”™è¯¯ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

        # ä½¿ç”¨LeakDetectionAgentæ™ºèƒ½ä½“å¤„ç†
        elif should_use_leak_detection and agent_inp_file_path:
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è®­ç»ƒè¯·æ±‚
                training_keywords = ['è®­ç»ƒ', 'æ¨¡å‹', 'train', 'model', 'æœºå™¨å­¦ä¹ ', 'AI', 'å­¦ä¹ ']
                is_training_request = any(keyword in user_message for keyword in training_keywords)

                if is_training_request:
                    # è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹
                    print("å¼€å§‹è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹...")

                    # æå–è®­ç»ƒå‚æ•°
                    num_scenarios = 50  # é»˜è®¤å€¼
                    epochs = 100  # é»˜è®¤å€¼

                    # æ™ºèƒ½æå–è®­ç»ƒå‚æ•°
                    import re
                    num_scenarios, epochs = extract_training_parameters(user_message, num_scenarios, epochs)
                    print(f"è§£æçš„è®­ç»ƒå‚æ•°: æ ·æœ¬æ•°={num_scenarios}, è¿­ä»£æ¬¡æ•°={epochs}")

                    leak_result = leak_detection_agent.train_leak_detection_model(
                        agent_inp_file_path,
                        conversation_id,
                        num_scenarios,
                        epochs
                    )

                    if leak_result['success']:
                        # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„ä¸“ä¸špromptè°ƒç”¨GPT
                        prompt = leak_detection_agent.build_response_prompt(leak_result, user_message, "training")

                        messages = [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»™æ°´ç®¡ç½‘æ¼æŸæ£€æµ‹ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„æœºå™¨å­¦ä¹ å’Œå¼‚å¸¸æ£€æµ‹ç»éªŒã€‚è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š\n\nç¥å¥½ï¼Œ\n\nTianwei Mu\nGuangzhou Institute of Industrial Intelligence"}
                        ]

                        # æ·»åŠ å½“å‰å¯¹è¯çš„å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘10è½®ï¼‰
                        for msg in current_conversation['messages'][-10:]:
                            messages.append({"role": "user", "content": msg['user']})
                            messages.append({"role": "assistant", "content": msg['assistant']})

                        messages.append({"role": "user", "content": prompt})

                        # è°ƒç”¨OpenAI API
                        response = openai.ChatCompletion.create(
                            model="gpt-4-turbo-preview",
                            messages=messages,
                            max_tokens=4000,
                            temperature=0.7
                        )

                        assistant_message = response.choices[0].message.content

                        # ä¿å­˜åˆ°å½“å‰å¯¹è¯
                        message_data = {
                            'user': user_message,
                            'assistant': assistant_message,
                            'timestamp': datetime.now().isoformat(),
                            'intent': 'leak_detection_training',
                            'confidence': 0.9
                        }

                        # å¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œä¿å­˜æ–‡ä»¶ä¿¡æ¯
                        if is_inp_file and inp_file_path:
                            message_data.update({
                                'has_file': True,
                                'file_type': 'inp',
                                'file_path': inp_file_path
                            })
                        else:
                            # ä½¿ç”¨å†å²æ–‡ä»¶çš„å¯¹è¯
                            message_data.update({
                                'has_file': False,
                                'uses_historical_file': True,
                                'historical_file_path': agent_inp_file_path
                            })

                        # æ·»åŠ è®­ç»ƒç»“æœä¿¡æ¯
                        if leak_result.get('files'):
                            message_data['leak_training_files'] = leak_result['files']

                        current_conversation['messages'].append(message_data)

                        # ä¿å­˜åˆ°æ–‡ä»¶
                        save_conversation_to_file(conversation_id, current_conversation)
                        # æ›´æ–°å¯¹è¯ç´¢å¼•
                        all_conversations = load_all_conversations()
                        all_conversations[conversation_id] = current_conversation
                        save_conversations_index(all_conversations)
                        session.modified = True

                        # æ„å»ºå“åº”
                        response_data = {
                            'success': True,
                            'response': assistant_message,
                            'conversation_id': conversation_id,
                            'intent': 'leak_detection_training',
                            'confidence': 0.9,
                            'model_info': leak_result.get('model_info', {}),
                            'evaluation': leak_result.get('evaluation', {})
                        }

                        # æ·»åŠ ä¸‹è½½ä¿¡æ¯
                        if leak_result.get('files'):
                            response_data['downloads'] = []
                            for file_type, file_info in leak_result['files'].items():
                                if file_info.get('success'):
                                    response_data['downloads'].append({
                                        'type': file_type,
                                        'filename': file_info['filename'],
                                        'url': file_info['download_url'],
                                        'size': file_info['file_size']
                                    })

                        return jsonify(response_data)

                    else:
                        # è®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ–¹å¼å¤„ç†
                        full_message = f"æ¼æŸæ£€æµ‹æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{leak_result.get('error', 'æœªçŸ¥é”™è¯¯')}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

                else:
                    # æ£€æµ‹è¯·æ±‚ - éœ€è¦ä¸Šä¼ çš„ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶
                    full_message = f"ç”¨æˆ·æƒ³è¦è¿›è¡Œæ¼æŸæ£€æµ‹ã€‚è¯·æé†’ç”¨æˆ·éœ€è¦ï¼š\n1. å…ˆè®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹\n2. ä¸Šä¼ ä¼ æ„Ÿå™¨å‹åŠ›æ•°æ®CSVæ–‡ä»¶è¿›è¡Œæ£€æµ‹\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

            except Exception as e:
                print(f"LeakDetectionAgentæ™ºèƒ½ä½“å¤„ç†é”™è¯¯: {e}")
                full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†æ¼æŸæ£€æµ‹æ™ºèƒ½ä½“å¤„ç†æ—¶å‡ºç°é”™è¯¯ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

        # ä½¿ç”¨HydroSimæ™ºèƒ½ä½“å¤„ç†
        elif should_use_hydro_sim and agent_inp_file_path:
            try:
                hydro_result = hydro_sim_agent.process(agent_inp_file_path, user_message, conversation_id)

                if hydro_result['success']:
                    # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„promptè°ƒç”¨GPT
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»™æ°´ç®¡ç½‘åˆ†æä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„æ°´åŠ›è®¡ç®—å’Œç®¡ç½‘åˆ†æç»éªŒã€‚è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š\n\nç¥å¥½ï¼Œ\n\nTianwei Mu\nGuangzhou Institute of Industrial Intelligence"}
                    ]

                    # æ·»åŠ å½“å‰å¯¹è¯çš„å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘10è½®ï¼‰
                    for msg in current_conversation['messages'][-10:]:
                        messages.append({"role": "user", "content": msg['user']})
                        messages.append({"role": "assistant", "content": msg['assistant']})

                    # æ·»åŠ æ™ºèƒ½ä½“ç”Ÿæˆçš„prompt
                    messages.append({"role": "user", "content": hydro_result['prompt']})

                    # è°ƒç”¨OpenAI API
                    response = openai.ChatCompletion.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        max_tokens=4000,
                        temperature=0.7
                    )

                    assistant_message = response.choices[0].message.content

                    # è°ƒè¯•ä¿¡æ¯
                    print(f"ğŸ” è°ƒè¯•: hydro_result keys: {list(hydro_result.keys())}")
                    print(f"ğŸ” è°ƒè¯•: network_infoå­˜åœ¨: {hydro_result.get('network_info') is not None}")
                    if hydro_result.get('network_info'):
                        print(f"ğŸ” è°ƒè¯•: network_infoç±»å‹: {type(hydro_result['network_info'])}")

                    # å¦‚æœæœ‰ç®¡ç½‘ä¿¡æ¯ï¼Œåœ¨å›å¤åæ·»åŠ è¯¦ç»†çš„ç®¡ç½‘ä¿¡æ¯
                    if hydro_result.get('network_info'):
                        network_info = hydro_result['network_info']
                        network_details = f"""

## ğŸ“Š ç®¡ç½‘è¯¦ç»†ä¿¡æ¯

### ğŸ—ï¸ ç½‘ç»œç»“æ„
- **èŠ‚ç‚¹æ€»æ•°**: {network_info['nodes']['total']} ä¸ª
  - æ¥ç‚¹: {network_info['nodes']['junctions']} ä¸ª
  - æ°´åº“: {network_info['nodes']['reservoirs']} ä¸ª
  - æ°´å¡”: {network_info['nodes']['tanks']} ä¸ª

- **ç®¡æ®µæ€»æ•°**: {network_info['links']['total']} ä¸ª
  - ç®¡é“: {network_info['links']['pipes']} ä¸ª
  - æ°´æ³µ: {network_info['links']['pumps']} ä¸ª
  - é˜€é—¨: {network_info['links']['valves']} ä¸ª

### ğŸ“ ç½‘ç»œå‚æ•°
- **ç®¡ç½‘æ€»é•¿åº¦**: {network_info['network_stats']['total_length']:.2f} ç±³
- **ä»¿çœŸæ—¶é•¿**: {network_info['network_stats']['simulation_duration']} ç§’
- **æ°´åŠ›æ—¶é—´æ­¥é•¿**: {network_info['network_stats']['hydraulic_timestep']} ç§’
- **æ¨¡å¼æ—¶é—´æ­¥é•¿**: {network_info['network_stats']['pattern_timestep']} ç§’

### ğŸ¯ åˆ†æå»ºè®®
åŸºäºä»¥ä¸Šç®¡ç½‘ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è¿›è¡Œä»¥ä¸‹è¿›ä¸€æ­¥åˆ†æï¼š
- ğŸ”„ **æ°´åŠ›ä»¿çœŸ**: è®¡ç®—èŠ‚ç‚¹å‹åŠ›å’Œç®¡æ®µæµé‡
- ğŸ—‚ï¸ **ç®¡ç½‘åˆ†åŒº**: å°†ç®¡ç½‘åˆ’åˆ†ä¸ºç®¡ç†åŒºåŸŸ
- ğŸ“ **ä¼ æ„Ÿå™¨å¸ƒç½®**: ä¼˜åŒ–ç›‘æµ‹ç‚¹ä½ç½®
- ğŸ” **æ¼æŸæ£€æµ‹**: è®­ç»ƒå’Œåº”ç”¨æ¼æŸæ£€æµ‹æ¨¡å‹
"""
                        assistant_message += network_details

                    # ä¿å­˜åˆ°å½“å‰å¯¹è¯
                    message_data = {
                        'user': user_message,
                        'assistant': assistant_message,
                        'timestamp': datetime.now().isoformat(),
                        'intent': hydro_result['intent'],
                        'confidence': hydro_result['confidence']
                    }

                    # å¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œä¿å­˜æ–‡ä»¶ä¿¡æ¯
                    if is_inp_file and inp_file_path:
                        message_data.update({
                            'has_file': True,
                            'file_type': 'inp',
                            'file_path': inp_file_path  # ä½¿ç”¨åŸå§‹çš„inp_file_path
                        })
                    else:
                        # ä½¿ç”¨å†å²æ–‡ä»¶çš„å¯¹è¯
                        message_data.update({
                            'has_file': False,
                            'uses_historical_file': True,
                            'historical_file_path': agent_inp_file_path
                        })

                    current_conversation['messages'].append(message_data)

                    # æ›´æ–°å¯¹è¯æ—¶é—´
                    current_conversation['updated_at'] = datetime.now().isoformat()

                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸”æ ‡é¢˜æ˜¯é»˜è®¤çš„ï¼Œæ›´æ–°æ ‡é¢˜
                    if len(current_conversation['messages']) == 1 and current_conversation['title'] == 'æ–°å¯¹è¯':
                        current_conversation['title'] = generate_conversation_title(user_message or "ç®¡ç½‘åˆ†æ")

                    # é™åˆ¶æ¯ä¸ªå¯¹è¯çš„æ¶ˆæ¯æ•°é‡
                    if len(current_conversation['messages']) > 50:
                        current_conversation['messages'] = current_conversation['messages'][-50:]

                    # ä¿å­˜åˆ°æ–‡ä»¶
                    save_conversation_to_file(conversation_id, current_conversation)
                    # æ›´æ–°å¯¹è¯ç´¢å¼•
                    all_conversations = load_all_conversations()
                    all_conversations[conversation_id] = current_conversation
                    save_conversations_index(all_conversations)
                    session.modified = True

                    # æ„å»ºå“åº”
                    response_data = {
                        'success': True,
                        'response': assistant_message,
                        'conversation_id': conversation_id,
                        'intent': hydro_result['intent'],
                        'confidence': hydro_result['confidence'],
                        'network_info': hydro_result['network_info']
                    }

                    # å¦‚æœæœ‰CSVæ–‡ä»¶ç”Ÿæˆï¼Œæ·»åŠ ä¸‹è½½ä¿¡æ¯
                    if hydro_result['csv_info'] and hydro_result['csv_info']['success']:
                        response_data['download'] = {
                            'available': True,
                            'filename': hydro_result['csv_info']['filename'],
                            'url': hydro_result['csv_info']['download_url'],
                            'size': hydro_result['csv_info']['file_size'],
                            'records_count': hydro_result['csv_info']['records_count']
                        }

                    return jsonify(response_data)

                else:
                    # æ™ºèƒ½ä½“å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ–¹å¼å¤„ç†
                    full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†å¤„ç†æ—¶é‡åˆ°é—®é¢˜ï¼š{hydro_result.get('response', 'æœªçŸ¥é”™è¯¯')}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

            except Exception as e:
                print(f"HydroSimæ™ºèƒ½ä½“å¤„ç†é”™è¯¯: {e}")
                full_message = f"ç”¨æˆ·ä¸Šä¼ äº†ç®¡ç½‘æ–‡ä»¶(.inpæ ¼å¼)ï¼Œä½†æ™ºèƒ½ä½“å¤„ç†æ—¶å‡ºç°é”™è¯¯ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

        else:
            # æ™®é€šæ–‡ä»¶å¤„ç†
            full_message = user_message
            if file_content:
                full_message = f"ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶å†…å®¹ï¼š\n\n{file_content}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}" if user_message else f"ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶å†…å®¹ï¼š\n\n{file_content}\n\nè¯·åˆ†æè¿™ä¸ªæ–‡ä»¶çš„å†…å®¹ã€‚"
        
        # æ„å»ºæ¶ˆæ¯å†å²ï¼ˆOpenAIæ ¼å¼ï¼‰
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŸºäºGPT-4çš„é«˜çº§AIåŠ©æ‰‹ï¼Œå…·æœ‰å¼ºå¤§çš„åˆ†æå’Œæ¨ç†èƒ½åŠ›ã€‚ä½ å¯ä»¥ï¼š\n1. æ·±å…¥åˆ†æå„ç§æ–‡ä»¶å†…å®¹ï¼ˆä»£ç ã€æ–‡æ¡£ã€æ•°æ®ç­‰ï¼‰\n2. æä¾›ä¸“ä¸šçš„æŠ€æœ¯å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ\n3. è¿›è¡Œå¤æ‚çš„é€»è¾‘æ¨ç†å’Œé—®é¢˜è§£å†³\n4. æ”¯æŒå¤šè¯­è¨€äº¤æµï¼Œä½†è¯·ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡å›ç­”\n5. æä¾›è¯¦ç»†ã€å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”\n\nè¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚æä¾›æœ€ä½³çš„å¸®åŠ©ã€‚è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š\n\nç¥å¥½ï¼Œ\n\nTianwei Mu\nGuangzhou Institute of Industrial Intelligence"}
        ]

        # æ·»åŠ å½“å‰å¯¹è¯çš„å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘10è½®ï¼‰
        for msg in current_conversation['messages'][-10:]:
            messages.append({"role": "user", "content": msg['user']})
            messages.append({"role": "assistant", "content": msg['assistant']})

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": full_message})
        
        # è°ƒç”¨OpenAI API - ä½¿ç”¨GPT-4æœ€æ–°ç‰ˆæœ¬
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo-preview",  # GPT-4 Turboæœ€æ–°ç‰ˆæœ¬ï¼Œæ›´å¿«æ›´å¼º
            messages=messages,
            max_tokens=4000,  # GPT-4æ”¯æŒæ›´é•¿çš„è¾“å‡º
            temperature=0.7
        )
        
        assistant_message = response.choices[0].message.content

        # ä¿å­˜åˆ°å½“å‰å¯¹è¯
        current_conversation['messages'].append({
            'user': user_message,
            'assistant': assistant_message,
            'timestamp': datetime.now().isoformat(),
            'has_file': bool(file_content)
        })

        # æ›´æ–°å¯¹è¯æ—¶é—´
        current_conversation['updated_at'] = datetime.now().isoformat()

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸”æ ‡é¢˜æ˜¯é»˜è®¤çš„ï¼Œæ›´æ–°æ ‡é¢˜
        if len(current_conversation['messages']) == 1 and current_conversation['title'] == 'æ–°å¯¹è¯':
            current_conversation['title'] = generate_conversation_title(user_message)

        # é™åˆ¶æ¯ä¸ªå¯¹è¯çš„æ¶ˆæ¯æ•°é‡
        if len(current_conversation['messages']) > 50:
            current_conversation['messages'] = current_conversation['messages'][-50:]

        # ä¿å­˜åˆ°æ–‡ä»¶
        save_conversation_to_file(conversation_id, current_conversation)
        # æ›´æ–°å¯¹è¯ç´¢å¼•
        all_conversations = load_all_conversations()
        all_conversations[conversation_id] = current_conversation
        save_conversations_index(all_conversations)

        session.modified = True

        return jsonify({
            'success': True,
            'response': assistant_message,
            'conversation_id': conversation_id
        })
        
    except Exception as e:
        return jsonify({'error': f'å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}'}), 500

@app.route('/new_conversation', methods=['POST'])
def new_conversation():
    """åˆ›å»ºæ–°å¯¹è¯"""
    init_session()
    session['current_conversation_id'] = None
    session['is_new_conversation'] = True  # æ ‡è®°ç”¨æˆ·ä¸»åŠ¨åˆ›å»ºæ–°å¯¹è¯
    session.modified = True
    return jsonify({'success': True})

@app.route('/get_conversations')
def get_conversations():
    """è·å–æ‰€æœ‰å¯¹è¯åˆ—è¡¨"""
    init_session()
    # ä»æ–‡ä»¶åŠ è½½æ‰€æœ‰å¯¹è¯
    all_conversations = load_all_conversations()
    conversations = list(all_conversations.values())
    pinned_ids = session.get('pinned_conversations', [])

    # åˆ†ç¦»ç½®é¡¶å’Œæ™®é€šå¯¹è¯
    pinned_conversations = []
    regular_conversations = []

    for conv in conversations:
        # æ·»åŠ ç½®é¡¶æ ‡è®°
        conv['is_pinned'] = conv['id'] in pinned_ids
        if conv['is_pinned']:
            pinned_conversations.append(conv)
        else:
            regular_conversations.append(conv)

    # ç½®é¡¶å¯¹è¯æŒ‰ç½®é¡¶æ—¶é—´æ’åºï¼ˆæœ€æ–°ç½®é¡¶çš„åœ¨å‰ï¼‰
    pinned_conversations.sort(key=lambda x: pinned_ids.index(x['id']) if x['id'] in pinned_ids else 999)
    # æ™®é€šå¯¹è¯æŒ‰æ›´æ–°æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    regular_conversations.sort(key=lambda x: x['updated_at'], reverse=True)

    # å¦‚æœæ²¡æœ‰å½“å‰å¯¹è¯IDï¼Œä½†æœ‰å¯¹è¯å­˜åœ¨ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€è¿‘çš„å¯¹è¯
    # ä½†æ˜¯å¦‚æœç”¨æˆ·ä¸»åŠ¨åˆ›å»ºäº†æ–°å¯¹è¯ï¼Œåˆ™ä¸è‡ªåŠ¨é€‰æ‹©
    current_conversation_id = session.get('current_conversation_id')
    is_new_conversation = session.get('is_new_conversation', False)

    if not current_conversation_id and conversations and not is_new_conversation:
        # é€‰æ‹©æœ€è¿‘æ›´æ–°çš„å¯¹è¯ï¼ˆä¸åŒºåˆ†ç½®é¡¶å’Œæ™®é€šï¼‰
        latest_conversation = max(conversations, key=lambda x: x['updated_at'])

        current_conversation_id = latest_conversation['id']
        session['current_conversation_id'] = current_conversation_id
        session.modified = True

    return jsonify({
        'pinned_conversations': pinned_conversations,
        'regular_conversations': regular_conversations,
        'current_conversation_id': current_conversation_id,
        'auto_selected': current_conversation_id != session.get('original_current_conversation_id')
    })

@app.route('/load_conversation/<conversation_id>')
def load_conversation(conversation_id):
    """åŠ è½½æŒ‡å®šå¯¹è¯"""
    init_session()
    # ä»æ–‡ä»¶åŠ è½½å¯¹è¯
    conversation = load_conversation_from_file(conversation_id)
    if conversation:
        session['current_conversation_id'] = conversation_id
        session.modified = True
        return jsonify({
            'success': True,
            'conversation': conversation
        })
    else:
        return jsonify({'error': 'å¯¹è¯ä¸å­˜åœ¨'}), 404

@app.route('/delete_conversation/<conversation_id>', methods=['DELETE'])
def delete_conversation(conversation_id):
    """åˆ é™¤æŒ‡å®šå¯¹è¯"""
    init_session()
    # æ£€æŸ¥å¯¹è¯æ˜¯å¦å­˜åœ¨
    conversation = load_conversation_from_file(conversation_id)
    if conversation:
        # åˆ é™¤æ–‡ä»¶
        delete_conversation_file(conversation_id)
        # æ›´æ–°ç´¢å¼•
        all_conversations = load_all_conversations()
        if conversation_id in all_conversations:
            del all_conversations[conversation_id]
        save_conversations_index(all_conversations)

        # å¦‚æœæ˜¯ç½®é¡¶å¯¹è¯ï¼Œä¹Ÿè¦ä»ç½®é¡¶åˆ—è¡¨ä¸­ç§»é™¤
        if conversation_id in session.get('pinned_conversations', []):
            session['pinned_conversations'].remove(conversation_id)
            save_pinned_conversations(session['pinned_conversations'])

        # å¦‚æœåˆ é™¤çš„æ˜¯å½“å‰å¯¹è¯ï¼Œæ¸…ç©ºå½“å‰çŠ¶æ€
        if session.get('current_conversation_id') == conversation_id:
            session['current_conversation_id'] = None
        session.modified = True
        return jsonify({'success': True})
    else:
        return jsonify({'error': 'å¯¹è¯ä¸å­˜åœ¨'}), 404

@app.route('/pin_conversation/<conversation_id>', methods=['POST'])
def pin_conversation(conversation_id):
    """ç½®é¡¶å¯¹è¯"""
    init_session()

    # ä»æ–‡ä»¶æ£€æŸ¥å¯¹è¯æ˜¯å¦å­˜åœ¨
    all_conversations = load_all_conversations()
    if conversation_id in all_conversations:
        pinned_list = session.get('pinned_conversations', [])
        if conversation_id not in pinned_list:
            # æ·»åŠ åˆ°ç½®é¡¶åˆ—è¡¨çš„å¼€å¤´ï¼ˆæœ€æ–°ç½®é¡¶çš„åœ¨å‰ï¼‰
            pinned_list.insert(0, conversation_id)
            session['pinned_conversations'] = pinned_list
            save_pinned_conversations(pinned_list)
            session.modified = True
        return jsonify({'success': True, 'is_pinned': True})
    else:
        return jsonify({'error': 'å¯¹è¯ä¸å­˜åœ¨'}), 404

@app.route('/unpin_conversation/<conversation_id>', methods=['POST'])
def unpin_conversation(conversation_id):
    """å–æ¶ˆç½®é¡¶å¯¹è¯"""
    init_session()
    pinned_list = session.get('pinned_conversations', [])
    if conversation_id in pinned_list:
        pinned_list.remove(conversation_id)
        session['pinned_conversations'] = pinned_list
        save_pinned_conversations(pinned_list)
        session.modified = True
    return jsonify({'success': True, 'is_pinned': False})

@app.route('/clear_history', methods=['POST'])
def clear_history():
    """æ¸…é™¤å½“å‰èŠå¤©å†å²ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    init_session()
    session['current_conversation_id'] = None
    session.modified = True
    return jsonify({'success': True})

@app.route('/get_history')
def get_history():
    """è·å–å½“å‰èŠå¤©å†å²ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    init_session()
    current_id = session.get('current_conversation_id')
    history = []
    if current_id:
        conversation = load_conversation_from_file(current_id)
        if conversation:
            history = conversation['messages']

    return jsonify({
        'history': history,
        'current_conversation_id': current_id
    })

@app.route('/get_current_conversation')
def get_current_conversation():
    """è·å–å½“å‰æ´»è·ƒå¯¹è¯çš„å®Œæ•´ä¿¡æ¯"""
    init_session()
    current_id = session.get('current_conversation_id')

    # å¦‚æœæ²¡æœ‰å½“å‰å¯¹è¯IDï¼Œå°è¯•è‡ªåŠ¨é€‰æ‹©æœ€è¿‘çš„å¯¹è¯
    if not current_id:
        all_conversations = load_all_conversations()
        if all_conversations:
            conversations = list(all_conversations.values())
            latest_conversation = max(conversations, key=lambda x: x['updated_at'])
            current_id = latest_conversation['id']
            session['current_conversation_id'] = current_id
            session.modified = True

    if current_id:
        conversation = load_conversation_from_file(current_id)
        if conversation:
            return jsonify({
                'success': True,
                'conversation': conversation,
                'current_conversation_id': current_id
            })

    return jsonify({
        'success': False,
        'conversation': None,
        'current_conversation_id': None
    })

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½æ–‡ä»¶ï¼ˆCSVã€å›¾ç‰‡ç­‰ï¼‰"""
    try:
        # å®‰å…¨æ–‡ä»¶åæ£€æŸ¥
        safe_filename = secure_filename(filename)
        file_path = os.path.join(DOWNLOADS_FOLDER, safe_filename)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            abort(404)

        # æ ¹æ®æ–‡ä»¶ç±»å‹ç¡®å®šMIMEç±»å‹å’Œä¸‹è½½æ–¹å¼
        mimetype, _ = mimetypes.guess_type(file_path)

        if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg'):
            # å›¾ç‰‡æ–‡ä»¶ï¼šç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤º
            return send_file(
                file_path,
                mimetype=mimetype or 'image/png',
                as_attachment=False  # ä¸å¼ºåˆ¶ä¸‹è½½ï¼Œåœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤º
            )
        else:
            # å…¶ä»–æ–‡ä»¶ï¼ˆå¦‚CSVï¼‰ï¼šä½œä¸ºé™„ä»¶ä¸‹è½½
            return send_file(
                file_path,
                as_attachment=True,
                download_name=safe_filename,
                mimetype=mimetype or 'text/csv'
            )
    except Exception as e:
        print(f"ä¸‹è½½æ–‡ä»¶é”™è¯¯: {e}")
        abort(500)

@app.route('/download_upload/<filename>')
def download_upload_file(filename):
    """ä¸‹è½½ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        # å®‰å…¨æ–‡ä»¶åæ£€æŸ¥
        safe_filename = secure_filename(filename)
        file_path = os.path.join(UPLOAD_FOLDER, safe_filename)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            abort(404)

        # æ ¹æ®æ–‡ä»¶ç±»å‹ç¡®å®šMIMEç±»å‹å’Œä¸‹è½½æ–¹å¼
        mimetype, _ = mimetypes.guess_type(file_path)

        # ä¸Šä¼ çš„æ–‡ä»¶é€šå¸¸ä½œä¸ºé™„ä»¶ä¸‹è½½
        return send_file(
            file_path,
            as_attachment=True,
            download_name=safe_filename,
            mimetype=mimetype or 'application/octet-stream'
        )
    except Exception as e:
        print(f"ä¸‹è½½ä¸Šä¼ æ–‡ä»¶é”™è¯¯: {e}")
        abort(500)

@app.route('/static_files/<filename>')
def serve_static_file(filename):
    """æä¾›é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆä¸“é—¨ç”¨äºå›¾ç‰‡æ˜¾ç¤ºï¼‰"""
    try:
        # å®‰å…¨æ–‡ä»¶åæ£€æŸ¥
        safe_filename = secure_filename(filename)
        file_path = os.path.join(DOWNLOADS_FOLDER, safe_filename)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            abort(404)

        # åªå…è®¸å›¾ç‰‡æ–‡ä»¶
        if not (filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg')):
            abort(403)

        # å‘é€å›¾ç‰‡æ–‡ä»¶
        return send_file(file_path, mimetype='image/png')
    except Exception as e:
        print(f"é™æ€æ–‡ä»¶æœåŠ¡é”™è¯¯: {e}")
        abort(500)

@app.route('/file_manager')
def file_manager():
    """æ–‡ä»¶ç®¡ç†é¡µé¢"""
    try:
        init_session()
        current_conversation_id = session.get('current_conversation_id')

        # è·å–è¿‡æ»¤å‚æ•°
        filter_type = request.args.get('filter', 'current')  # 'current', 'all'

        files = []

        # æ‰«æä¸‹è½½æ–‡ä»¶å¤¹ï¼ˆæ™ºèƒ½ä½“ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
        if os.path.exists(DOWNLOADS_FOLDER):
            for filename in os.listdir(DOWNLOADS_FOLDER):
                file_path = os.path.join(DOWNLOADS_FOLDER, filename)
                if os.path.isfile(file_path):
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºå½“å‰å¯¹è¯
                    belongs_to_current = False
                    if current_conversation_id:
                        conversation_prefix = current_conversation_id[:8]
                        belongs_to_current = conversation_prefix in filename

                    # æ ¹æ®è¿‡æ»¤ç±»å‹å†³å®šæ˜¯å¦åŒ…å«æ–‡ä»¶
                    if filter_type == 'current' and not belongs_to_current:
                        continue

                    file_stat = os.stat(file_path)
                    file_info = {
                        'filename': filename,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        'type': 'image' if filename.endswith(('.png', '.jpg', '.jpeg')) else 'csv',
                        'url': f'/download/{filename}' if filename.endswith('.csv') else f'/static_files/{filename}',
                        'belongs_to_current': belongs_to_current,
                        'source': 'generated'  # æ ‡è®°ä¸ºæ™ºèƒ½ä½“ç”Ÿæˆçš„æ–‡ä»¶
                    }
                    files.append(file_info)

        # æ‰«æä¸Šä¼ æ–‡ä»¶å¤¹ï¼ˆç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                file_path = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(file_path):
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºå½“å‰å¯¹è¯
                    belongs_to_current = False
                    if current_conversation_id:
                        conversation_prefix = current_conversation_id[:8]
                        belongs_to_current = conversation_prefix in filename

                    # æ ¹æ®è¿‡æ»¤ç±»å‹å†³å®šæ˜¯å¦åŒ…å«æ–‡ä»¶
                    if filter_type == 'current' and not belongs_to_current:
                        continue

                    file_stat = os.stat(file_path)
                    file_info = {
                        'filename': filename,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        'type': 'upload',  # æ ‡è®°ä¸ºä¸Šä¼ æ–‡ä»¶
                        'url': f'/download_upload/{filename}',  # ä½¿ç”¨æ–°çš„ä¸‹è½½è·¯ç”±
                        'belongs_to_current': belongs_to_current,
                        'source': 'uploaded'  # æ ‡è®°ä¸ºç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶
                    }
                    files.append(file_info)

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        files.sort(key=lambda x: x['modified'], reverse=True)

        return render_template('file_manager.html',
                             files=files,
                             current_conversation_id=current_conversation_id,
                             filter_type=filter_type)
    except Exception as e:
        return f"æ–‡ä»¶ç®¡ç†å™¨é”™è¯¯: {str(e)}", 500

@app.route('/api/files')
def get_files_api():
    """è·å–æ–‡ä»¶åˆ—è¡¨API"""
    try:
        init_session()
        current_conversation_id = session.get('current_conversation_id')

        # è·å–è¿‡æ»¤å‚æ•°
        filter_type = request.args.get('filter', 'current')  # 'current', 'all'

        files = []

        # æ‰«æä¸‹è½½æ–‡ä»¶å¤¹ï¼ˆæ™ºèƒ½ä½“ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
        if os.path.exists(DOWNLOADS_FOLDER):
            for filename in os.listdir(DOWNLOADS_FOLDER):
                file_path = os.path.join(DOWNLOADS_FOLDER, filename)
                if os.path.isfile(file_path):
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºå½“å‰å¯¹è¯
                    belongs_to_current = False
                    if current_conversation_id:
                        conversation_prefix = current_conversation_id[:8]
                        belongs_to_current = conversation_prefix in filename

                    # æ ¹æ®è¿‡æ»¤ç±»å‹å†³å®šæ˜¯å¦åŒ…å«æ–‡ä»¶
                    if filter_type == 'current' and not belongs_to_current:
                        continue

                    file_stat = os.stat(file_path)
                    file_info = {
                        'filename': filename,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        'type': 'image' if filename.endswith(('.png', '.jpg', '.jpeg')) else 'csv',
                        'url': f'/download/{filename}' if filename.endswith('.csv') else f'/static_files/{filename}',
                        'download_url': f'/download/{filename}',
                        'belongs_to_current': belongs_to_current,
                        'source': 'generated'
                    }
                    files.append(file_info)

        # æ‰«æä¸Šä¼ æ–‡ä»¶å¤¹ï¼ˆç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                file_path = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(file_path):
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºå½“å‰å¯¹è¯
                    belongs_to_current = False
                    if current_conversation_id:
                        conversation_prefix = current_conversation_id[:8]
                        belongs_to_current = conversation_prefix in filename

                    # æ ¹è¿‡æ»¤ç±»å‹å†³å®šæ˜¯å¦åŒ…å«æ–‡ä»¶
                    if filter_type == 'current' and not belongs_to_current:
                        continue

                    file_stat = os.stat(file_path)
                    file_info = {
                        'filename': filename,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        'type': 'upload',
                        'url': f'/download_upload/{filename}',
                        'download_url': f'/download_upload/{filename}',
                        'belongs_to_current': belongs_to_current,
                        'source': 'uploaded'
                    }
                    files.append(file_info)

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        files.sort(key=lambda x: x['modified'], reverse=True)

        return jsonify({
            'success': True,
            'files': files,
            'current_conversation_id': current_conversation_id,
            'filter_type': filter_type
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/delete_file/<filename>', methods=['DELETE'])
def delete_file(filename):
    """åˆ é™¤æ–‡ä»¶API"""
    try:
        # å®‰å…¨æ–‡ä»¶åæ£€æŸ¥
        safe_filename = secure_filename(filename)

        # å…ˆå°è¯•åœ¨downloadsæ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾
        file_path = os.path.join(DOWNLOADS_FOLDER, safe_filename)
        if os.path.exists(file_path):
            os.remove(file_path)
            return jsonify({'success': True, 'message': f'æ–‡ä»¶ {filename} å·²åˆ é™¤'})

        # å¦‚æœdownloadsä¸­æ²¡æœ‰ï¼Œå°è¯•åœ¨uploadsæ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾
        file_path = os.path.join(UPLOAD_FOLDER, safe_filename)
        if os.path.exists(file_path):
            os.remove(file_path)
            return jsonify({'success': True, 'message': f'æ–‡ä»¶ {filename} å·²åˆ é™¤'})

        # æ–‡ä»¶ä¸å­˜åœ¨
        return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/topology')
def topology_page():
    """ç®¡ç½‘æ‹“æ‰‘å›¾é¡µé¢"""
    return render_template('topology.html')



@app.route('/api/inp_files')
def get_inp_files():
    """è·å–æ‰€æœ‰ä¸Šä¼ çš„.inpæ–‡ä»¶åˆ—è¡¨"""
    try:
        inp_files = []

        # æ‰«æuploadsç›®å½•ï¼ˆç”¨æˆ·ä¸Šä¼ æ–‡ä»¶ï¼‰
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                if filename.endswith('.inp'):
                    file_path = os.path.join(UPLOAD_FOLDER, filename)
                    file_stat = os.stat(file_path)
                    inp_files.append({
                        'filename': filename,
                        'path': file_path,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        'source': 'uploaded'
                    })

        # æ‰«æinpfileç›®å½•ï¼ˆç¤ºä¾‹æ–‡ä»¶ï¼‰
        uploaded_filenames = {f['filename'] for f in inp_files}  # å·²ä¸Šä¼ çš„æ–‡ä»¶åé›†åˆ
        if os.path.exists('inpfile'):
            for filename in os.listdir('inpfile'):
                if filename.endswith('.inp'):
                    # å¦‚æœåŒåæ–‡ä»¶å·²ç»åœ¨uploadsä¸­å­˜åœ¨ï¼Œè·³è¿‡ç¤ºä¾‹æ–‡ä»¶ï¼ˆä¼˜å…ˆæ˜¾ç¤ºç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
                    if filename in uploaded_filenames:
                        continue

                    file_path = os.path.join('inpfile', filename)
                    file_stat = os.stat(file_path)
                    inp_files.append({
                        'filename': filename,
                        'path': file_path,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        'source': 'example'
                    })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        inp_files.sort(key=lambda x: x['modified'], reverse=True)

        return jsonify({
            'success': True,
            'files': inp_files
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/list_downloads')
def list_downloads():
    """åˆ—å‡ºå¯ä¸‹è½½çš„æ–‡ä»¶"""
    try:
        files = []
        for filename in os.listdir(DOWNLOADS_FOLDER):
            if filename.endswith('.csv'):
                file_path = os.path.join(DOWNLOADS_FOLDER, filename)
                file_info = {
                    'filename': filename,
                    'size': os.path.getsize(file_path),
                    'created_time': os.path.getctime(file_path)
                }
                files.append(file_info)

        return jsonify({'files': files})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/csv_files')
def get_csv_files():
    """è·å–æ‰€æœ‰CSVæ–‡ä»¶åˆ—è¡¨"""
    try:
        csv_files = []

        if os.path.exists(DOWNLOADS_FOLDER):
            for filename in os.listdir(DOWNLOADS_FOLDER):
                if filename.endswith('.csv'):
                    file_path = os.path.join(DOWNLOADS_FOLDER, filename)
                    file_stat = os.stat(file_path)
                    csv_files.append({
                        'filename': filename,
                        'path': file_path,
                        'size': file_stat.st_size,
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat()
                    })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        csv_files.sort(key=lambda x: x['modified'], reverse=True)

        return jsonify({
            'success': True,
            'files': csv_files
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/network_topology/<path:inp_file_path>')
def get_network_topology(inp_file_path):
    """è·å–ç®¡ç½‘æ‹“æ‰‘ç»“æ„"""
    try:
        # å®‰å…¨æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if not (inp_file_path.startswith('uploads/') or inp_file_path.startswith('inpfile/')):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶è·¯å¾„'}), 400

        if not os.path.exists(inp_file_path):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404

        # ä½¿ç”¨HydroSimè§£æç½‘ç»œ
        network_info = hydro_sim_agent.parse_network(inp_file_path)

        if 'error' in network_info:
            return jsonify({'success': False, 'error': network_info['error']}), 500

        return jsonify({
            'success': True,
            'topology': network_info
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/csv_data/<path:csv_file_path>')
def get_csv_data(csv_file_path):
    """è·å–CSVæ•°æ®"""
    try:
        # å®‰å…¨æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if not csv_file_path.startswith('downloads/'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶è·¯å¾„'}), 400

        if not os.path.exists(csv_file_path):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404

        # è¯»å–CSVæ•°æ®
        import pandas as pd
        df = pd.read_csv(csv_file_path)

        # è·å–æ—¶é—´æ­¥é•¿åˆ—è¡¨
        time_steps = sorted(df['æ—¶é—´(å°æ—¶)'].unique()) if 'æ—¶é—´(å°æ—¶)' in df.columns else [0]

        # æŒ‰æ—¶é—´å’Œæ•°æ®ç±»å‹ç»„ç»‡æ•°æ®
        organized_data = {}
        for time_step in time_steps:
            time_data = df[df['æ—¶é—´(å°æ—¶)'] == time_step]
            organized_data[str(time_step)] = {
                'node_pressure': {},
                'node_demand': {},
                'link_flow': {},
                'link_velocity': {}
            }

            # èŠ‚ç‚¹å‹åŠ›æ•°æ®
            pressure_data = time_data[time_data['æ•°æ®ç±»å‹'] == 'èŠ‚ç‚¹å‹åŠ›']
            for _, row in pressure_data.iterrows():
                if pd.notna(row['èŠ‚ç‚¹ID']):
                    organized_data[str(time_step)]['node_pressure'][str(row['èŠ‚ç‚¹ID'])] = float(row['æ•°å€¼'])

            # èŠ‚ç‚¹éœ€æ°´é‡æ•°æ®
            demand_data = time_data[time_data['æ•°æ®ç±»å‹'] == 'èŠ‚ç‚¹éœ€æ°´é‡']
            for _, row in demand_data.iterrows():
                if pd.notna(row['èŠ‚ç‚¹ID']):
                    organized_data[str(time_step)]['node_demand'][str(row['èŠ‚ç‚¹ID'])] = float(row['æ•°å€¼'])

            # ç®¡æ®µæµé‡æ•°æ®
            flow_data = time_data[time_data['æ•°æ®ç±»å‹'] == 'ç®¡æ®µæµé‡']
            for _, row in flow_data.iterrows():
                if pd.notna(row['ç®¡æ®µID']):
                    organized_data[str(time_step)]['link_flow'][str(row['ç®¡æ®µID'])] = float(row['æ•°å€¼'])

            # ç®¡æ®µæµé€Ÿæ•°æ®
            velocity_data = time_data[time_data['æ•°æ®ç±»å‹'] == 'ç®¡æ®µæµé€Ÿ']
            for _, row in velocity_data.iterrows():
                if pd.notna(row['ç®¡æ®µID']):
                    organized_data[str(time_step)]['link_velocity'][str(row['ç®¡æ®µID'])] = float(row['æ•°å€¼'])

        # è½¬æ¢ä¸ºJSONæ ¼å¼
        csv_data = {
            'time_steps': time_steps,
            'data_by_time': organized_data,
            'summary': {
                'total_records': len(df),
                'time_steps_count': len(time_steps),
                'data_types': df['æ•°æ®ç±»å‹'].value_counts().to_dict() if 'æ•°æ®ç±»å‹' in df.columns else {},
                'time_range': {
                    'min': float(min(time_steps)),
                    'max': float(max(time_steps))
                },
                'nodes_count': len(df[df['èŠ‚ç‚¹ID'].notna()]['èŠ‚ç‚¹ID'].unique()) if 'èŠ‚ç‚¹ID' in df.columns else 0,
                'links_count': len(df[df['ç®¡æ®µID'].notna()]['ç®¡æ®µID'].unique()) if 'ç®¡æ®µID' in df.columns else 0
            }
        }

        return jsonify({
            'success': True,
            'csv_data': csv_data
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/validate_compatibility', methods=['POST'])
def validate_compatibility():
    """éªŒè¯INPæ–‡ä»¶å’ŒCSVæ–‡ä»¶çš„å…¼å®¹æ€§"""
    try:
        data = request.get_json()
        inp_file_path = data.get('inp_file_path')
        csv_file_path = data.get('csv_file_path')

        if not inp_file_path or not csv_file_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶è·¯å¾„'}), 400

        # è·å–ç½‘ç»œæ‹“æ‰‘ä¿¡æ¯
        network_info = hydro_sim_agent.parse_network(inp_file_path)
        if 'error' in network_info:
            return jsonify({'success': False, 'error': f'è§£æINPæ–‡ä»¶å¤±è´¥: {network_info["error"]}'}), 500

        # è¯»å–CSVæ•°æ®
        import pandas as pd
        df = pd.read_csv(csv_file_path)

        # éªŒè¯å…¼å®¹æ€§
        compatibility = {
            'compatible': True,
            'issues': [],
            'network_nodes': network_info['nodes']['total'],
            'network_links': network_info['links']['total'],
            'csv_records': len(df)
        }

        # æ£€æŸ¥CSVä¸­çš„èŠ‚ç‚¹ID
        if 'èŠ‚ç‚¹ID' in df.columns:
            csv_nodes = set(df['èŠ‚ç‚¹ID'].dropna().astype(str).unique())
            network_nodes = set([node['id'] for node in network_info['topology']['nodes']]) if 'topology' in network_info else set()

            missing_nodes = csv_nodes - network_nodes
            if missing_nodes:
                compatibility['issues'].append(f'CSVä¸­åŒ…å«ç½‘ç»œä¸­ä¸å­˜åœ¨çš„èŠ‚ç‚¹: {list(missing_nodes)[:5]}')
                compatibility['compatible'] = False

        # æ£€æŸ¥CSVä¸­çš„ç®¡æ®µID
        if 'ç®¡æ®µID' in df.columns:
            csv_links = set(df['ç®¡æ®µID'].dropna().astype(str).unique())
            network_links = set([link['id'] for link in network_info['topology']['links']]) if 'topology' in network_info else set()

            missing_links = csv_links - network_links
            if missing_links:
                compatibility['issues'].append(f'CSVä¸­åŒ…å«ç½‘ç»œä¸­ä¸å­˜åœ¨çš„ç®¡æ®µ: {list(missing_links)[:5]}')
                compatibility['compatible'] = False

        return jsonify({
            'success': True,
            'compatibility': compatibility
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    # å¯åŠ¨æ–‡ä»¶æ¸…ç†è°ƒåº¦å™¨
    start_file_cleanup_scheduler()

    # å¯åŠ¨åº”ç”¨æ—¶è¿›è¡Œä¸€æ¬¡æ–‡ä»¶æ¸…ç†
    cleanup_old_files()

    print("ğŸš€ LeakAgent Web Chat åº”ç”¨å¯åŠ¨ä¸­...")
    print(f"ğŸ“ ä¸Šä¼ æ–‡ä»¶å¤¹: {UPLOAD_FOLDER}")
    print(f"ğŸ“ ä¸‹è½½æ–‡ä»¶å¤¹: {DOWNLOADS_FOLDER}")
    print(f"ğŸ“ å¯¹è¯å­˜å‚¨: {CONVERSATIONS_FOLDER}")
    print(f"ğŸ”§ æ–‡ä»¶ç®¡ç†: æœ€å¤§{MAX_FILES_COUNT}ä¸ªæ–‡ä»¶, {MAX_FOLDER_SIZE/1024/1024:.0f}MB, ä¿ç•™{FILE_RETENTION_DAYS}å¤©")
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:5000")
    print("ğŸ“Š æ–‡ä»¶ç®¡ç†å™¨: http://localhost:5000/file_manager")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")

    app.run(debug=True, host='0.0.0.0', port=5000)
