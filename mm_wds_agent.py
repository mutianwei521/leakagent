import os
import wntr
import networkx as nx
import json
import sys

# Ensure local modules are findable
sys.path.append(os.getcwd())

from typing import List, Optional, Dict, Any
from optimization_utils.objectives import calculate_fef, calculate_nr, run_simulation

try:
    from langchain_openai import ChatOpenAI
    from langchain_core.tools import tool
    from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
except ImportError as e:
    print(f"CRITICAL ERROR: {e}")
    sys.exit(1)

from dotenv import load_dotenv
load_dotenv()

# API Configuration
# Please set your API keys in the .env file
# You can copy .env.example to .env and fill in your keys
if not os.getenv("OPENAI_API_KEY"):
    print("âš ï¸  WARNING: OPENAI_API_KEY not found. Please set it in your .env file or environment variables.")
    print("   Example: OPENAI_API_KEY=sk-...")

# Optional: Set custom API base URL if needed
# os.environ["OPENAI_API_BASE"] = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")

from optimization_utils.rag_manager import ingest_inp_file, retrieve_knowledge

@tool
def hydraulic_inspector(inp_file: str, query: str):
    """
    æ°´åŠ›æ£€æŸ¥å·¥å…·ã€‚
    å½“ç”¨æˆ·è¯¢é—®æ°´åŠ›å±æ€§ã€èŠ‚ç‚¹æ•°é‡ã€ç®¡é“æ•°é‡æˆ–æ¨¡æ‹Ÿç»“æœæ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚
    ä¸è¦ç›²ç›®è¿è¡Œæ¨¡æ‹Ÿã€‚æ­¤å·¥å…·ä¼šé¦–å…ˆæ£€æŸ¥çŸ¥è¯†å›¾è°±ã€‚
    """
    try:
        # æ‘„å–/æ£€ç´¢æ•°æ®
        data = retrieve_knowledge(inp_file, query_type="summary")
        if "error" in data:
            return json.dumps({"status": "error", "error": data["error"]})

        # å°†ç»“æ„åŒ–æ‘˜è¦ç›´æ¥è¿”å›ç»™å¤§æ¨¡å‹ï¼ˆLLMï¼‰
        # å¤§æ¨¡å‹å°†è§£æ JSON ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼ˆä¾‹å¦‚â€œè®¡ç®—èŠ‚ç‚¹æ•°é‡â€ï¼‰ã€‚
        return json.dumps({
            "status": "success",
            "source": "GraphRAG Cache",
            "file_info": data["filename"],
            "statistics": data["stats"],
            "hydraulics": data["hydraulics"]
        }, ensure_ascii=False)

    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)})

@tool
def reliability_assessor(inp_file: str):
    """
    å¯é æ€§è¯„ä¼°å·¥å…·ã€‚
    è®¡ç®—æµé‡ç†µï¼ˆFEFï¼‰å’Œç®¡ç½‘å¼¹æ€§ï¼ˆNRï¼‰ã€‚
    """
    try:
        wn = wntr.network.WaterNetworkModel(inp_file)
        results = run_simulation(wn) 
        fef = calculate_fef(wn, results)
        try:
             nr = calculate_nr(wn, results)
        except:
             nr = 0.5 
             
        return json.dumps({
            "metric_type": "Reliability",
            "FEF": fef,
            "NR": nr,
            "interpretation": "Values > 0.6 indicate high resilience." if fef > 0.6 else "Values < 0.5 indicate redundancy deficit."
        })
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)})

@tool
def graph_rag_retriever(inp_file: str, entity_id: str):
    """
    æ‹“æ‰‘è¯­ä¹‰ GraphRAG å·¥å…·ã€‚
    ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å…³äºèŠ‚ç‚¹æˆ–é“¾è·¯ï¼ˆä¾‹å¦‚ 'J-10' æˆ– 'Pipe-1'ï¼‰çš„å…·ä½“ç»†èŠ‚ã€‚
    å½“ç”¨æˆ·è¯¢é—®ç‰¹å®šå…ƒç´ æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    try:
        # ä¼ é€’ query_type="entity" ä»¥è·å–è¯¦ç»†æ•°æ®
        data = retrieve_knowledge(inp_file, query_type="entity", entity_id=entity_id)
        
        if "error" in data:
            return json.dumps({"status": "error", "error": data["error"]})
            
        return json.dumps({
            "status": "found",
            "entity": entity_id,
            "details": data
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)})

@tool
def network_partitioner(inp_file: str, num_partitions: Optional[int] = None, algorithm: Optional[str] = None):
    """
    ç®¡ç½‘åˆ†åŒºå·¥å…·ã€‚
    å½“ç”¨æˆ·è¦æ±‚å°†ç®¡ç½‘â€œåˆ†åŒºâ€æˆ–â€œåˆ’åˆ†â€ä¸ºåŒºåŸŸ/ç¤¾åŒºæ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    ç®—æ³•é€‰æ‹©ï¼š
    - é»˜è®¤ï¼šâ€œlouvainâ€ï¼ˆä½¿ç”¨æ¨¡å—åŒ–ä¼˜åŒ–çš„ç¤¾åŒºæ£€æµ‹ï¼‰
    - å¤‡é€‰ï¼šâ€œfcmâ€ï¼ˆåŸºäºå‹åŠ›æ•æ„Ÿæ€§çš„æ¨¡ç³Š C å‡å€¼èšç±»ï¼‰
    
    å½“ç”¨æˆ·æ˜ç¡®æåˆ°â€œFCMâ€ã€â€œæ¨¡ç³Šâ€ã€â€œæ¨¡ç³Š C å‡å€¼â€ã€â€œåŸºäºæ•æ„Ÿæ€§â€æˆ–â€œå‹åŠ›æ•æ„Ÿæ€§â€åˆ†åŒºæ—¶ä½¿ç”¨ FCMã€‚
    
    é‡è¦ - â€œnum_partitionsâ€ å¦‚ä½•å·¥ä½œï¼š
    - Louvainï¼šäº§ç”Ÿç¦»æ•£çš„åˆ†åŒºè®¡æ•°ï¼›å¦‚æœç›®æ ‡è¾ƒä½ï¼Œåˆ™åˆå¹¶ç¤¾åŒºã€‚
    - FCMï¼šç›´æ¥ä½¿ç”¨æŒ‡å®šçš„èšç±»æ•°é‡ã€‚
    
    å¦‚æœç”¨æˆ·æŒ‡å®šäº†æ•°é‡ï¼ˆä¾‹å¦‚ï¼Œâ€œ5 ä¸ªåŒºåŸŸâ€ï¼Œâ€œåˆ’åˆ†ä¸º 3 ä¸ªâ€ï¼‰ï¼Œè¯·å°†å…¶ä½œä¸º 'num_partitions' ä¼ é€’ã€‚
    ä»…åœ¨ç”¨æˆ·æ˜ç¡®è¯·æ±‚åŸºäº FCM çš„åˆ†åŒºæ—¶ä¼ é€’ algorithm="fcm"ã€‚
    """
    try:
        # é»˜è®¤ä¸º Louvain ç®—æ³•
        use_fcm = algorithm and algorithm.lower() in ['fcm', 'fuzzy', 'fuzzy-c-means', 'fuzzycmeans']
        
        if use_fcm:
            # ä½¿ç”¨ FCM åˆ†åŒº
            from partition_utils.fcm_partition import run_fcm_partitioning_for_agent
            
            result = run_fcm_partitioning_for_agent(
                inp_file, 
                num_partitions=num_partitions or 5,
                fuzziness=1.5
            )
            
            if result["status"] == "error":
                return json.dumps({"status": "error", "error": result["error"]})
            
            # æ ¼å¼åŒ–é’ˆå¯¹ FCM çš„å“åº”
            base_url = "http://127.0.0.1:5000"
            
            response_text = f"## âœ… {result.get('msg', 'FCM partitioning completed.')}\n\n"
            response_text += "---\n\n"
            response_text += "### ğŸ“Š FCM Partitioning Results\n\n"
            
            # åˆ†åŒºç»Ÿè®¡è¡¨
            response_text += "| Partition | Node Count |\n"
            response_text += "|-----------|------------|\n"
            for partition, count in result['partition_stats'].items():
                response_text += f"| {partition} | {count} |\n"
            response_text += "\n"
            
            # æŒ‡æ ‡
            response_text += "### ğŸ“ˆ Clustering Metrics\n\n"
            response_text += f"- **Fuzzy Partition Coefficient (FPC):** {result['metrics']['fpc']:.4f}\n"
            response_text += f"- **Convergence Iterations:** {result['metrics']['iterations']}\n"
            response_text += f"- **Fuzziness Parameter (m):** {result['fuzziness']}\n\n"
            
            # å¯è§†åŒ–
            if result.get('viz_file'):
                viz_filename = os.path.basename(result['viz_file'])
                response_text += "### ğŸ–¼ï¸ Visualization\n\n"
                response_text += f"![FCM Partition]({base_url}/partition_results/{viz_filename})\n\n"
                response_text += f"[Download Visualization]({base_url}/partition_results/{viz_filename})\n\n"
            
            # Summary JSON
            if result.get('summary_json'):
                response_text += f"ğŸ“„ [Download Summary JSON]({base_url}/{result['summary_json']})\n\n"
            
            response_text += "---\n\n"
            response_text += "### ğŸ”§ Recommended Next Steps\n\n"
            response_text += "1. **Review partition boundaries** - Check that zones are spatially coherent\n"
            response_text += "2. **Analyze boundary pipes** - Use boundary_analyzer for valve placement\n"
            response_text += "3. **Place sensors** - Use sensor_placer for optimal monitoring points\n"
            
            return json.dumps({
                "status": "success", 
                "msg": response_text,
                "algorithm": "FCM",
                "raw_data": result
            }, ensure_ascii=False)
            
        else:
            # ä½¿ç”¨ Louvain ç®—æ³•ï¼ˆé»˜è®¤ï¼‰
            from optimization_utils.partition_manager import run_partitioning_for_agent
            
            result = run_partitioning_for_agent(inp_file, target_k=num_partitions)
            
            if result["status"] == "error":
                 return json.dumps({"status": "error", "error": result["error"]})
            
            # ä½¿ç”¨å¢å¼ºçš„ Markdown æ ¼å¼åŒ–ç”¨æˆ·å‹å¥½çš„å“åº”
            base_url = "http://127.0.0.1:5000"
            
            response_text = f"## âœ… {result.get('msg', 'Partitioning completed.')}\n\n"
            response_text += "---\n\n"
            response_text += "### ğŸ“Š Output Files\n\n"
            
            # å›¾åƒ
            for plot_path in result['plots']:
                 filename = os.path.basename(plot_path)
                 label = filename.replace(".png", "").replace("_", " ").title()
                 response_text += f"**Visualization:**\n\n"
                 response_text += f"![{label}]({base_url}/{plot_path})\n\n"
                 response_text += f"ğŸ–¼ï¸ [Download Image]({base_url}/{plot_path})\n\n"
                 
            # Summary JSON
            response_text += f"ğŸ“„ [Download Summary JSON]({base_url}/{result['summary_json']})\n\n"
            
            response_text += "---\n\n"
            response_text += "### ğŸ”§ Recommended Next Steps\n\n"
            response_text += "1. **Review partition boundaries** - Check that zones are geographically sensible\n"
            response_text += "2. **Verify hydraulic performance** - Run simulation for each zone\n"
            response_text += "3. **Identify isolation valves** - Locate boundary pipes for valve placement\n"
            
            return json.dumps({
                "status": "success", 
                "msg": response_text,
                "algorithm": "Louvain",
                "raw_data": result
            }, ensure_ascii=False)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return json.dumps({"status": "error", "error": str(e)})


@tool
def boundary_analyzer(inp_file: str, num_partitions: Optional[int] = None):
    """
    åˆ†æåˆ†åŒºä¹‹é—´çš„è¾¹ç•Œç®¡é“ã€‚
    å½“ç”¨æˆ·è¯¢é—®â€œè¾¹ç•Œç®¡é“â€ã€â€œåˆ‡å‰²è¾¹â€ã€â€œéš”ç¦»é˜€â€ã€â€œåŒºåŸŸä¹‹é—´çš„è¿æ¥â€æˆ–â€œè¾¹ç•Œç®¡æ®µâ€æ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    è¯¥å·¥å…·ä¼šè‡ªåŠ¨æ ¹æ® INP æ–‡ä»¶æŸ¥æ‰¾åˆ†åŒºç»“æœã€‚
    æ— éœ€å•ç‹¬ä¸Šä¼  partition_summary.jsonã€‚
    
    å¦‚æœæœªæŒ‡å®š num_partitionsï¼Œå®ƒå°†ä½¿ç”¨ç°æœ‰çš„åˆ†åŒºç»“æœã€‚
    """
    try:
        from optimization_utils.zone_optimizer import analyze_boundary_pipes
        
        result = analyze_boundary_pipes(inp_file, target_k=num_partitions)
        
        if result["status"] == "error":
            return json.dumps({"status": "error", "error": result["error"]})
        
        # ä½¿ç”¨å¢å¼ºçš„ Markdown æ ¼å¼åŒ–å“åº”
        base_url = "http://127.0.0.1:5000"
        
        response_text = f"## ğŸ”— Boundary Pipe Analysis ({result['partition_count']} Zones)\n\n"
        response_text += f"**Total Boundary Pipes:** {result['boundary_pipe_count']}\n\n"
        response_text += "---\n\n"
        response_text += "### ğŸ“‹ Boundary Pipe List\n\n"
        response_text += "| Pipe | From Node | To Node | Zoneâ†’Zone | Diameter (mm) | Length (m) |\n"
        response_text += "|------|-----------|---------|-----------|---------------|------------|\n"
        
        for p in result['boundary_pipes'][:20]:  # ä¸ºäº†å¯è¯»æ€§é™åˆ¶ä¸º 20 ä¸ª
            response_text += f"| {p['pipe']} | {p['from_node']} | {p['to_node']} | {p['zone_from']}â†’{p['zone_to']} | {p['diameter_mm']} | {p['length_m']} |\n"
        
        if result['boundary_pipe_count'] > 20:
            response_text += f"\n*...and {result['boundary_pipe_count'] - 20} more pipes*\n"
        
        response_text += "\n---\n\n"
        response_text += "### ğŸ”§ Engineering Recommendations\n\n"
        response_text += "1. **Isolation Valves**: Install valves on boundary pipes to enable zone isolation\n"
        response_text += "2. **Flow Meters**: Consider metering at zone entry points for leak detection\n"
        response_text += "3. **Prioritize**: Focus on larger diameter pipes for valve placement\n"
        
        return json.dumps({
            "status": "success",
            "msg": response_text,
            "raw_data": result
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)})

@tool
def zone_optimizer(inp_file: str, num_partitions: Optional[int] = None, 
                   pop_size: int = 20, n_gen: int = 50):
    """
    å¯¹åˆ†åŒºè¾¹ç•Œé…ç½®è¿è¡Œ NSGA-II ä¼˜åŒ–ã€‚
    ç›®æ ‡ï¼šæœ€å¤§åŒ– FEFã€HREã€MREã€NRï¼›æœ€å°åŒ–å¼€å¯ç®¡é“ã€‚
    å½“ç”¨æˆ·è¦æ±‚â€œä¼˜åŒ–â€ã€â€œæ”¹è¿›â€æˆ–å¯»æ‰¾â€œæœ€ä½³é…ç½®â€ä»¥è·å¾—ç‰¹å®šæ•°é‡çš„åŒºåŸŸæ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    è¯¥å·¥å…·é’ˆå¯¹å•ä¸€åˆ†åŒºè®¡æ•°ï¼ˆä¾‹å¦‚ 10 ä¸ªåŒºåŸŸï¼‰ä¼˜åŒ–è¾¹ç•Œç®¡é“çš„é˜€é—¨çŠ¶æ€ï¼ˆå¼€å¯/å…³é—­ï¼‰ã€‚
    å®ƒä¸ä¼šå¯»æ‰¾æœ€ä½³åˆ†åŒºæ•°é‡ã€‚
    """
    try:
        from optimization_utils.zone_optimizer import run_zone_optimization
        
        # é™¤éç”¨æˆ·å¦æœ‰æŒ‡å®šï¼Œå¦åˆ™é»˜è®¤ä½¿ç”¨ä½è¿­ä»£æ¬¡æ•°ä»¥ä¿è¯äº¤äº’é€Ÿåº¦
        result = run_zone_optimization(
            inp_file, 
            target_k=num_partitions,
            pop_size=pop_size, 
            n_gen=n_gen
        )
        
        if result["status"] == "error":
            return json.dumps({"status": "error", "error": result["error"]})
            
        # å¢å¼ºçš„ Markdown è¾“å‡º
        base_url = "http://127.0.0.1:5000"
        obj = result['best_objectives']
        
        response_text = f"## ğŸš€ Optimization Complete ({result.get('msg', '').split('for ')[-1]})\n\n"
        response_text += "---\n\n"
        response_text += "### ğŸ† Best Solution Objectives\n\n"
        
        response_text += f"| Objective | Value | Description |\n"
        response_text += f"|-----------|-------|-------------|\n"
        response_text += f"| **FEF** | {obj['FEF']:.4f} | Flow Entropy (Reliability) |\n"
        response_text += f"| **HRE** | {obj['HRE']:.4f} | Hydraulic Resilience |\n"
        response_text += f"| **MRE** | {obj['MRE']:.4f} | Mechanical Reliability |\n"
        response_text += f"| **NR** | {obj['NR']:.4f} | Network Resilience |\n"
        response_text += f"| **Open Valves** | {obj['open_pipes']} | Boundary Connections |\n\n"
        
        response_text += f"ğŸ“„ [Download Optimization Results JSON]({base_url}/{result['optimization_file']})\n\n"
        response_text += "---\n"
        response_text += "### ğŸ’¡ Interpretation\n"
        response_text += f"- **Values Optimized**: {result['boundary_count']} boundary valve settings found.\n"
        response_text += "- **Balance**: Solution balances reliability (Entropy) with isolation (Closed Valves).\n"
        
        return json.dumps({
            "status": "success",
            "msg": response_text,
            "raw_data": result
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)})

@tool
def visual_analyzer(inp_file: str, analysis_type: Optional[str] = "combined"):
    """
    è§†è§‰æ„ŸçŸ¥åˆ†æå·¥å…·ã€‚
    å½“ç”¨æˆ·è¦æ±‚â€œå¯è§†åŒ–â€ã€â€œæ˜¾ç¤ºçƒ­åŠ›å›¾â€ã€â€œåˆ†æå‹åŠ›åˆ†å¸ƒâ€ã€â€œæ˜¾ç¤ºæµé‡æ¨¡å¼â€ã€â€œè§†è§‰åˆ†æâ€æˆ–â€œç”Ÿæˆç®¡ç½‘å›¾â€æ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    è¯¥å·¥å…·ç”Ÿæˆè§†è§‰çƒ­å›¾å¹¶ä»ç®¡ç½‘ä¸­æå–è§†è§‰ç‰¹å¾ï¼š
    - å‹åŠ›çƒ­å›¾ï¼ˆè“è‰²=ä½å‹ï¼Œçº¢è‰²=é«˜å‹ï¼‰
    - æµé‡å¯è§†åŒ–ï¼ˆçº¿å®½ = é€Ÿåº¦ï¼‰
    - æ‹“æ‰‘å¼‚å¸¸æ£€æµ‹ï¼ˆæœ«ç«¯ã€æ¡¥æ¥ï¼‰
    
    analysis_type é€‰é¡¹ï¼š
    - "pressure": ä»…å‹åŠ›çƒ­å›¾
    - "flow": ä»…æµé‡å¯è§†åŒ–  
    - "combined": å‹åŠ›å’Œæµé‡ï¼ˆé»˜è®¤ï¼‰
    - "features": æå–è§†è§‰ç‰¹å¾è€Œä¸ç”Ÿæˆå›¾åƒ
    """
    try:
        from partition_utils.visual_perception import (
            analyze_network_visually,
            extract_visual_features,
            get_vlm_prompt_template
        )
        import wntr
        
        base_url = "http://127.0.0.1:5000"
        
        # è¿è¡Œè§†è§‰åˆ†æ
        result = analyze_network_visually(inp_file)
        
        if "error" in result:
            return json.dumps({"status": "error", "error": result["error"]})
        
        # Format response with enhanced markdown
        response_text = f"## ğŸ¨ Visual Analysis Complete: {result['network_name']}\n\n"
        response_text += f"**Network Size:** {result['node_count']} nodes, {result['link_count']} links\n\n"
        response_text += "---\n\n"
        
        # çƒ­å›¾å›¾åƒ
        response_text += "### ğŸ“Š Generated Visualizations\n\n"
        
        for viz_type, path in result['heatmap_paths'].items():
            if analysis_type == "combined" or analysis_type == viz_type:
                filename = os.path.basename(path)
                label = viz_type.replace("_", " ").title()
                # ä½¿ç”¨ç›¸å¯¹è·¯å¾„è¿›è¡Œç½‘é¡µæ˜¾ç¤º
                rel_path = path.replace("\\", "/")
                response_text += f"**{label} Heatmap:**\n\n"
                response_text += f"![{label}]({base_url}/visual_outputs/{filename})\n\n"
                response_text += f"ğŸ–¼ï¸ [Download {label} Image]({base_url}/visual_outputs/{filename})\n\n"
        
        # è§†è§‰ç‰¹å¾
        features = result['visual_features']
        response_text += "---\n\n"
        response_text += "### ğŸ” Extracted Visual Features\n\n"
        
        # æ‹“æ‰‘å¼‚å¸¸
        topo = features.get('topological_anomalies', {})
        response_text += "**Topological Analysis:**\n"
        response_text += f"- ğŸŒ‰ Bridge Nodes (Critical): {topo.get('bridge_count', 0)}\n"
        response_text += f"- ğŸŒ¿ Dead-End Nodes: {topo.get('dead_end_count', 0)}\n\n"
        
        # å‹åŠ›æ¨¡å¼
        pressure = features.get('pressure_patterns', {})
        if pressure:
            response_text += "**Pressure Patterns:**\n"
            response_text += f"- Mean Pressure: {pressure.get('mean', 0):.2f} m\n"
            response_text += f"- Pressure Range: {pressure.get('range', 0):.2f} m\n"
            response_text += f"- Uniformity (CV): {pressure.get('cv', 0):.3f}\n\n"
        
        # æµé‡æ¨¡å¼
        flow = features.get('flow_patterns', {})
        if flow:
            response_text += "**Flow Patterns:**\n"
            response_text += f"- Mean Velocity: {flow.get('mean_velocity', 0):.3f} m/s\n"
            response_text += f"- Max Velocity: {flow.get('max_velocity', 0):.3f} m/s\n"
            response_text += f"- High-Flow Pipes: {flow.get('high_flow_pipe_count', 0)} (top 10%)\n\n"
        
        # å¯¹ç§°æ€§æŒ‡æ ‡
        symmetry = features.get('symmetry_metrics', {})
        if symmetry:
            balance_score = symmetry.get('flow_balance_score', 0)
            balance_emoji = "âœ…" if balance_score > 0.6 else "âš ï¸" if balance_score > 0.4 else "âŒ"
            response_text += "**Flow Balance:**\n"
            response_text += f"- Balance Score: {balance_score:.3f} {balance_emoji}\n"
            response_text += f"- Interpretation: {'Well balanced' if balance_score > 0.6 else 'Moderately balanced' if balance_score > 0.4 else 'Unbalanced flow distribution'}\n\n"
        
        # VLM åˆ†ææç¤ºè¯ï¼ˆé€‚ç”¨äºé«˜çº§ç”¨æˆ·ï¼‰
        response_text += "---\n\n"
        response_text += "### ğŸ¤– VLM Analysis Ready\n"
        response_text += "The generated heatmaps can be analyzed by Vision-Language Models (GPT-4o, Gemini-2.0-Pro) "
        response_text += "for advanced pattern recognition. Use the prompt template below:\n\n"
        response_text += "```\n"
        response_text += result['vlm_prompt'][:500] + "...\n"
        response_text += "```\n"
        
        return json.dumps({
            "status": "success",
            "msg": response_text,
            "raw_data": {
                "heatmap_paths": result['heatmap_paths'],
                "visual_features": result['visual_features'],
                "network_name": result['network_name']
            }
        }, ensure_ascii=False)
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return json.dumps({"status": "error", "error": str(e)})

@tool
def sensor_placer(inp_file: str, num_partitions: Optional[int] = None):
    """
    ä¼ æ„Ÿå™¨å¸ƒç½®å·¥å…· - è‡ªåŠ¨ç¡®å®šæœ€ä½³ä¼ æ„Ÿå™¨ä½ç½®ã€‚
    å½“ç”¨æˆ·è¦æ±‚â€œå¸ƒç½®ä¼ æ„Ÿå™¨â€ã€â€œä¼ æ„Ÿå™¨å¸ƒç½®â€ã€â€œç›‘æµ‹ç‚¹â€ã€â€œä¼ æ„Ÿå™¨ä¼˜åŒ–â€æˆ–â€œä¼ æ„Ÿå™¨ä½ç½®â€æ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    é‡è¦ï¼šè¯¥å·¥å…·åŸºäºå‹åŠ›æ•æ„Ÿæ€§åˆ†æè‡ªåŠ¨è®¡ç®—ä¼ æ„Ÿå™¨çš„æœ€ä½³æ•°é‡å’Œä½ç½®ã€‚
    æ‚¨æ— éœ€è¯¢é—®ç”¨æˆ·ä¼ æ„Ÿå™¨è®¡æ•°æˆ–ä»»ä½•å…¶ä»–å‚æ•°â€”â€”åªéœ€ä½¿ç”¨ inp_file è°ƒç”¨æ­¤å·¥å…·å³å¯ã€‚
    
    è¯¥å·¥å…·ä½¿ç”¨å‹åŠ›æ‰°åŠ¨åˆ†ææ¥å¯»æ‰¾å…·æœ‰æœ€å¤§æ£€æµ‹è¦†ç›–èŒƒå›´çš„èŠ‚ç‚¹ã€‚
    æ¯ä¸ªåˆ†åŒºçš„ä¼ æ„Ÿå™¨è®¡æ•°æ˜¯æ ¹æ®åˆ†åŒºå¤§å°è‡ªåŠ¨è®¡ç®—çš„ï¼ˆé€šå¸¸æ¯ä¸ªåŒºåŸŸ 2-10 ä¸ªï¼‰ã€‚
    
    è¦æ±‚ï¼šå¿…é¡»å…ˆä½¿ç”¨ 'network_partitioner' å¯¹ç½‘ç®¡è¿›è¡Œåˆ†åŒºã€‚
    å¦‚æœä¸å­˜åœ¨åˆ†åŒºç»“æœï¼Œæ­¤å·¥å…·å°†è¿”å›é”™è¯¯ï¼Œè¦æ±‚ç”¨æˆ·å…ˆå¯¹ç®¡ç½‘è¿›è¡Œåˆ†åŒºã€‚
    
    num_partitionsï¼šå¯é€‰ã€‚æŒ‡å®šç”¨äºä¼ æ„Ÿå™¨å¸ƒç½®çš„åˆ†åŒºè®¡æ•°ã€‚
    å¦‚æœæœªæŒ‡å®šï¼Œå°†æ ¹æ®ç°æœ‰çš„åˆ†åŒºç»“æœè‡ªåŠ¨é€‰æ‹©ã€‚
    """
    try:
        from optimization_utils.sensor_manager import run_sensor_placement_for_agent
        
        result = run_sensor_placement_for_agent(inp_file, num_partitions)
        
        # å¤„ç†ä¸å­˜åœ¨åˆ†åŒºçš„æƒ…å†µ
        if result["status"] == "no_partition":
            return json.dumps({
                "status": "error",
                "error": result["error"],
                "suggestion": "Please use network_partitioner to partition the network first, then run sensor placement."
            }, ensure_ascii=False)
        
        if result["status"] == "error":
            return json.dumps({"status": "error", "error": result["error"]})
        
        # ä½¿ç”¨å¢å¼ºçš„ Markdown æ ¼å¼åŒ–æˆåŠŸå“åº”
        base_url = "http://127.0.0.1:5000"
        summary = result['summary']
        
        response_text = f"## âœ… {result['msg']}\n\n"
        response_text += "---\n\n"
        
        # æ¦‚è§ˆéƒ¨åˆ†
        response_text += "### ğŸ“Š Placement Overview\n\n"
        response_text += f"| Metric | Value |\n"
        response_text += f"|--------|-------|\n"
        response_text += f"| **Total Sensors** | {summary['total_sensors']} |\n"
        response_text += f"| **Partitions** | {summary['num_partitions']} |\n"
        response_text += f"| **Sensitivity Threshold** | {summary['threshold']} |\n"
        response_text += f"| **Optimization Score** | {summary['score']:.4f} |\n\n"
        
        # åˆ†åŒºè¯¦æƒ…
        response_text += "### ğŸ“ˆ Partition Details\n\n"
        response_text += "| Partition | Sensors | Resilience | Coverage | Sensor Nodes |\n"
        response_text += "|-----------|---------|------------|----------|---------------|\n"
        
        for pid, details in summary['partition_details'].items():
            nodes_str = ", ".join(details['sensor_nodes'][:5])
            if len(details['sensor_nodes']) > 5:
                nodes_str += f" +{len(details['sensor_nodes'])-5}..."
            coverage = details.get('full_coverage_rate', 1.0) * 100
            response_text += f"| {pid} | {details['count']} | {details['resilience']:.4f} | {coverage:.1f}% | {nodes_str} |\n"
        
        response_text += "\n---\n\n"
        
        # è¾“å‡ºæ–‡ä»¶
        response_text += "### ğŸ“ Output Files\n\n"
        
        # å¯è§†åŒ–
        viz_filename = os.path.basename(result['viz_file'])
        response_text += f"**Visualization:**\n\n"
        response_text += f"![Sensor Placement]({base_url}/sensor_results/{viz_filename})\n\n"
        response_text += f"ğŸ–¼ï¸ [Download Visualization]({base_url}/sensor_results/{viz_filename})\n\n"
        
        # CSV æ–‡ä»¶
        csv_filename = os.path.basename(result['sensor_file'])
        response_text += f"ğŸ“„ [Download Sensor Placement CSV]({base_url}/sensor_results/{csv_filename})\n\n"
        
        # å»ºè®®
        response_text += "---\n\n"
        response_text += "### ğŸ”§ Recommendations\n\n"
        response_text += "1. **Installation Location** - Install pressure sensors at recommended nodes\n"
        response_text += "2. **Resilience Score** - Indicates detection capability when some sensors fail\n"
        response_text += "3. **Priority** - Prioritize sensors with higher coverage rates\n"

        
        return json.dumps({
            "status": "success",
            "msg": response_text,
            "raw_data": result
        }, ensure_ascii=False)
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return json.dumps({"status": "error", "error": str(e)})

@tool
def leak_detector_trainer(inp_file: str, num_partitions: Optional[int] = None, n_scenarios: int = 3000):
    """
    Leak detection model training tool.
    Use when user asks to "train leak detection", "train LTFM model", "train anomaly detection",
    "è®­ç»ƒæ¼æŸæ£€æµ‹", "è®­ç»ƒæ³„æ¼æ¨¡å‹", or "å¼€å§‹æ¼æŸè®­ç»ƒ".

    Prerequisites: The network must have been partitioned (network_partitioner) AND
    sensors must have been placed (sensor_placer) before training.
    If prerequisites are not met, this tool will return an error with instructions.

    Parameters:
    - inp_file: EPANET INP file path
    - num_partitions: Optional partition count (auto-detected if not specified)
    - n_scenarios: Number of training scenarios (default: 3000, adjustable by user)
    """
    try:
        import glob
        import threading

        # --- Prerequisite check ---
        # Check partition results
        partition_files = glob.glob('partition_results/*partition_summary.json')
        if not partition_files:
            return json.dumps({
                "status": "error",
                "error": "No partition results found. Please run network_partitioner first.",
                "suggestion": "Use network_partitioner to partition the network, then sensor_placer to place sensors, before training."
            }, ensure_ascii=False)

        # Check sensor results
        sensor_files = glob.glob('sensor_results/sensor_placement_*.csv')
        if not sensor_files:
            return json.dumps({
                "status": "error",
                "error": "No sensor placement results found. Please run sensor_placer first.",
                "suggestion": "Use sensor_placer to place sensors before training the leak detection model."
            }, ensure_ascii=False)

        # Find best partition file (prefer FCM)
        fcm_files = [f for f in partition_files if 'fcm' in f.lower()]
        partition_file = fcm_files[0] if fcm_files else partition_files[0]

        # Auto-detect num_partitions if not specified
        if num_partitions is None:
            with open(partition_file, 'r', encoding='utf-8') as f:
                pdata = json.load(f)
            available_keys = [int(k) for k in pdata.keys() if k.isdigit()]
            num_partitions = max(available_keys) if available_keys else 5

        # --- Run training ---
        from wds_leak_main import load_config, setup_logging, train_mode
        import argparse

        config = load_config()
        setup_logging(config)
        os.makedirs(config['data']['output_dir'], exist_ok=True)
        os.makedirs(os.path.join(config['data']['output_dir'], 'checkpoints'), exist_ok=True)

        # Create args namespace
        args = argparse.Namespace(
            inp=inp_file,
            partition=partition_file,
            num_partitions=num_partitions,
            n_scenarios=n_scenarios,
            skip_stage1=False,
            config=None
        )

        print(f"[LeakDetector] Starting training: inp={inp_file}, partition={partition_file}, "
              f"k={num_partitions}, scenarios={n_scenarios}")

        success = train_mode(config, args)

        if not success:
            return json.dumps({
                "status": "error",
                "error": "Leak detection model training failed. Check logs for details."
            })

        # --- Format success response ---
        base_url = "http://127.0.0.1:5000"
        output_dir = config['data']['output_dir']

        response_text = "## âœ… Leak Detection Model Training Complete\n\n"
        response_text += "---\n\n"
        response_text += "### ğŸ“Š Training Summary\n\n"
        response_text += f"| Parameter | Value |\n"
        response_text += f"|-----------|-------|\n"
        response_text += f"| **INP File** | {inp_file} |\n"
        response_text += f"| **Partition File** | {os.path.basename(partition_file)} |\n"
        response_text += f"| **Partitions** | {num_partitions} |\n"
        response_text += f"| **Training Scenarios** | {n_scenarios} |\n\n"

        response_text += "### ğŸ“ Model Files\n\n"
        response_text += f"ğŸ“¥ [Download LTFM Model (best_model.pth)]({base_url}/leak_detection_output/checkpoints/best_model.pth)\n\n"
        response_text += f"ğŸ“¥ [Download NodeLocalizer Model (best_node_localizer.pth)]({base_url}/leak_detection_output/checkpoints/best_node_localizer.pth)\n\n"
        response_text += f"ğŸ“¥ [Download Graph2Vec Model (graph2vec_model.pth)]({base_url}/leak_detection_output/graph2vec_model.pth)\n\n"

        response_text += "---\n\n"
        response_text += "### ğŸ”§ Next Steps\n\n"
        response_text += "1. **Run Inference** - Upload a CSV file with pressure data and ask to predict leaks\n"
        response_text += "2. **Download Template** - Use the inference template CSV as a reference for data format\n"
        response_text += f"3. **Template Download** - ğŸ“„ [Download Inference CSV Template]({base_url}/leak_detection_output/inference_template.csv)\n"

        return json.dumps({
            "status": "success",
            "msg": response_text
        }, ensure_ascii=False)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return json.dumps({"status": "error", "error": str(e)})


@tool
def leak_detector_predictor(inp_file: str, csv_file: str, num_partitions: Optional[int] = None):
    """
    Leak detection inference tool - predict anomalies from pressure data.
    Use when user asks to "predict leaks", "detect leaks", "run inference",
    "æ¼æŸæ£€æµ‹", "æ³„æ¼é¢„æµ‹", "æ¨ç†", or "anomaly detection".

    Prerequisites: A trained LTFM model must exist (run leak_detector_trainer first).

    Parameters:
    - inp_file: EPANET INP file path
    - csv_file: Path to CSV file containing pressure measurement data.
                Format: rows=timesteps, columns=node names, values=pressure (m).
    - num_partitions: Optional partition count (auto-detected if not specified)
    """
    try:
        import glob

        # --- Check trained model exists ---
        model_path = os.path.join('leak_detection_output', 'checkpoints', 'best_model.pth')
        if not os.path.exists(model_path):
            return json.dumps({
                "status": "error",
                "error": "No trained LTFM model found. Please run leak_detector_trainer first.",
                "suggestion": "Train the model first: 'Train leak detection model for <inp_file>'"
            }, ensure_ascii=False)

        # --- Check CSV file exists ---
        if not os.path.exists(csv_file):
            return json.dumps({
                "status": "error",
                "error": f"CSV file not found: {csv_file}",
                "suggestion": "Please upload a CSV file with pressure data first."
            })

        # Find partition file
        partition_files = glob.glob('partition_results/*partition_summary.json')
        if not partition_files:
            return json.dumps({
                "status": "error",
                "error": "No partition results found."
            })

        fcm_files = [f for f in partition_files if 'fcm' in f.lower()]
        partition_file = fcm_files[0] if fcm_files else partition_files[0]

        # Auto-detect num_partitions
        if num_partitions is None:
            with open(partition_file, 'r', encoding='utf-8') as f:
                pdata = json.load(f)
            available_keys = [int(k) for k in pdata.keys() if k.isdigit()]
            num_partitions = max(available_keys) if available_keys else 5

        # Find sensor file (optional)
        sensor_files = glob.glob('sensor_results/sensor_placement_*.csv')
        sensor_file = sensor_files[-1] if sensor_files else None

        # --- Run inference ---
        from wds_leak_main import load_config, setup_logging, inference_mode
        import argparse

        config = load_config()
        setup_logging(config)

        args = argparse.Namespace(
            inp=inp_file,
            partition=partition_file,
            num_partitions=num_partitions,
            sensor=sensor_file,
            graph2vec_model=None,
            ltfm_checkpoint=None,
            test_data=csv_file,
            output=os.path.join(config['data']['output_dir'], 'prediction_results.csv'),
            config=None
        )

        print(f"[LeakDetector] Starting inference: inp={inp_file}, csv={csv_file}, k={num_partitions}")

        success = inference_mode(config, args)

        if not success:
            return json.dumps({
                "status": "error",
                "error": "Leak detection inference failed. Check logs for details."
            })

        # --- Format success response ---
        base_url = "http://127.0.0.1:5000"
        output_dir = config['data']['output_dir']

        # Read prediction results if available
        result_path = os.path.join(output_dir, 'prediction_results.csv')
        result_summary = ""
        if os.path.exists(result_path):
            import pandas as pd
            df = pd.read_csv(result_path)
            result_summary = f"\n\n### ğŸ“‹ Prediction Results\n\n"
            result_summary += f"| Metric | Value |\n"
            result_summary += f"|--------|-------|\n"
            for col in df.columns:
                val = df[col].iloc[0] if len(df) > 0 else "N/A"
                result_summary += f"| **{col}** | {val} |\n"

        response_text = "## ğŸ” Leak Detection Inference Complete\n\n"
        response_text += "---\n\n"
        response_text += "### ğŸ“Š Input Summary\n\n"
        response_text += f"| Parameter | Value |\n"
        response_text += f"|-----------|-------|\n"
        response_text += f"| **INP File** | {inp_file} |\n"
        response_text += f"| **Pressure Data** | {os.path.basename(csv_file)} |\n"
        response_text += f"| **Partitions** | {num_partitions} |\n"
        if sensor_file:
            response_text += f"| **Sensor File** | {os.path.basename(sensor_file)} |\n"
        response_text += result_summary

        response_text += "\n---\n\n"
        response_text += "### ğŸ“ Output Files\n\n"
        response_text += f"ğŸ“„ [Download Prediction Results]({base_url}/leak_detection_output/prediction_results.csv)\n\n"

        return json.dumps({
            "status": "success",
            "msg": response_text
        }, ensure_ascii=False)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return json.dumps({"status": "error", "error": str(e)})


tools = [hydraulic_inspector, reliability_assessor, graph_rag_retriever, network_partitioner, boundary_analyzer, zone_optimizer, visual_analyzer, sensor_placer, leak_detector_trainer, leak_detector_predictor]
tools_map = {
    "hydraulic_inspector": hydraulic_inspector,
    "graph_rag_retriever": graph_rag_retriever,
    "reliability_assessor": reliability_assessor,
    "network_partitioner": network_partitioner,
    "boundary_analyzer": boundary_analyzer,
    "zone_optimizer": zone_optimizer,
    "visual_analyzer": visual_analyzer,
    "sensor_placer": sensor_placer,
    "leak_detector_trainer": leak_detector_trainer,
    "leak_detector_predictor": leak_detector_predictor
}

# --- æ‰‹åŠ¨ä»£ç†æ‰§è¡Œå™¨ï¼ˆå¯¹å¯¼å…¥é”™è¯¯å…·æœ‰é²æ£’æ€§ï¼‰ ---

class SimpleAgent:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
        self.llm_with_tools = self.llm.bind_tools(tools)
        
        # Redis è¿æ¥
        try:
            import redis
            self.redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
            self.redis_client.ping()
            print(">>> Redis Connected for History Persistence")
        except Exception as e:
            print(f"!!! Redis Connection Failed: {e} - Falling back to Memory")
            self.redis_client = None
            self.memory_sessions = {}

        self.system_text = """You are MM-WDS, a Physics-Informed Multi-Agent System for Water Distribution.
            Strategy:
            - If asked about "Reliability", ALWAYS CALL 'reliability_assessor'.
            - If asked about network statistics (nodes, pipes), hydraulic values (pressure), or "summary", Use 'hydraulic_inspector'.
            - If asked about a SPECIFIC node/link (e.g. "J-10"), use 'graph_rag_retriever'.
            - If asked to "partition", "divide", or "zone" the network (DMA design), use 'network_partitioner'.
            - If asked about "boundary pipes", "cut edges", "isolation valves", or "è¾¹ç•Œç®¡æ®µ", use 'boundary_analyzer'.
              This tool automatically finds partition results - DO NOT ask user for partition_summary.json.
            - If asked to "optimize", "improve partition", or "find best valve config" for a specific zone count, use 'zone_optimizer'.
              Explain that this optimizes boundary valve status (open/closed) using NSGA-II.
            - If asked to "visualize", "show heatmap", "analyze pressure distribution", "show flow patterns", 
              "visual analysis", "ç”Ÿæˆçƒ­å›¾", or "å¯è§†åŒ–åˆ†æ", use 'visual_analyzer'.
            - If asked to "place sensors", "sensor placement", "monitoring points", "å¸ƒç½®ä¼ æ„Ÿå™¨", "ç›‘æµ‹ç‚¹",
              "ä¼ æ„Ÿå™¨ä¼˜åŒ–", "ä¼ æ„Ÿå™¨ä½ç½®", or "å¸ƒç½®ç›‘æµ‹ç‚¹", use 'sensor_placer'.
            - If asked to "train leak detection", "train LTFM", "train anomaly detection", "è®­ç»ƒæ¼æŸæ£€æµ‹",
              "è®­ç»ƒæ³„æ¼æ¨¡å‹", or "å¼€å§‹æ¼æŸè®­ç»ƒ", use 'leak_detector_trainer'.
              This tool checks prerequisites (partition + sensor placement) automatically.
              Default training scenarios: 3000. User can adjust via natural language (e.g. "use 1000 scenarios").
              Pass the user-specified n_scenarios parameter if mentioned.
            - If asked to "predict leaks", "detect leaks", "run inference", "æ¼æŸæ£€æµ‹", "æ³„æ¼é¢„æµ‹",
              "æ¨ç†", or "anomaly detection" with uploaded CSV data, use 'leak_detector_predictor'.
              The user must first upload a CSV file with pressure data. Pass the CSV file path.
            - Provide professional engineering diagnosis based on tool outputs.
            - The 'hydraulic_inspector' returns a JSON with 'statistics' and 'hydraulics'. READ IT CAREFULLY.
            
            IMPORTANT for Partitioning - Algorithm Selection:
            1. DEFAULT (Louvain Algorithm):
               - Use when user simply says "partition", "divide", or "zone" without specifying method.
            2. FCM Algorithm (Fuzzy C-Means):
               - Use when user EXPLICITLY mentions "FCM", "fuzzy", "sensitivity-based".
               - Pass algorithm="fcm" to the network_partitioner tool.
            
            IMPORTANT for Leak Detection:
            - Training requires: partition results + sensor results. If missing, instruct user.
            - Inference requires: trained model + CSV pressure data. CSV must have node names as columns.
            - When user mentions scenario count (e.g. "500 scenarios", "ç”¨500ä¸ªåœºæ™¯"), pass as n_scenarios.
            
            CRITICAL - Output Formatting:
            - When a tool returns markdown links like [Download Image](http://...), you MUST include them
              EXACTLY as provided in your response. Do NOT convert them to plain text URLs.
            - This ensures users see clickable hyperlinks instead of raw URLs.
            """

        self.system_prompt = SystemMessage(content=self.system_text)

    def _serialize_msg(self, msg) -> str:
        data = {"type": msg.type, "content": msg.content}
        if isinstance(msg, ToolMessage):
            data["tool_call_id"] = msg.tool_call_id
        if hasattr(msg, 'tool_calls') and msg.tool_calls:
            data["tool_calls"] = msg.tool_calls
        if msg.additional_kwargs:
            data["additional_kwargs"] = msg.additional_kwargs
        return json.dumps(data)

    def _deserialize_msg(self, data_str: str):
        try:
            data = json.loads(data_str)
            msg_type = data.get("type")
            content = data.get("content", "")
            
            if msg_type == "human": return HumanMessage(content=content)
            if msg_type == "ai": 
                msg = AIMessage(content=content)
                if "tool_calls" in data: msg.tool_calls = data["tool_calls"]
                if "additional_kwargs" in data: msg.additional_kwargs = data["additional_kwargs"]
                return msg
            if msg_type == "tool": return ToolMessage(tool_call_id=data.get("tool_call_id"), content=content)
            if msg_type == "system": return SystemMessage(content=content)
            return HumanMessage(content=content) # å¤‡ç”¨æ–¹æ¡ˆ
        except:
            return HumanMessage(content="")

    def get_session_history(self, session_id: str) -> List:
        history = []
        if self.redis_client:
            redis_key = f"chat:{session_id}"
            # ä» Redis åŠ è½½
            raw_msgs = self.redis_client.lrange(redis_key, 0, -1)
            if not raw_msgs:
                # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯åˆå§‹åŒ–
                self.save_message(session_id, self.system_prompt)
                history = [self.system_prompt]
            else:
                history = [self._deserialize_msg(m) for m in raw_msgs]
        else:
            # å¤‡ç”¨åˆ°å†…å­˜
            if session_id not in self.memory_sessions:
                self.memory_sessions[session_id] = [self.system_prompt]
            history = self.memory_sessions[session_id]
        
        # è‡ªæ„ˆï¼šæ£€æŸ¥æŸåçš„å·¥å…·è°ƒç”¨åºåˆ—
        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¸¦æœ‰ tool_calls çš„ AIMessageï¼Œä½†åé¢æ²¡æœ‰ ToolMessageï¼Œåˆ™è¿½åŠ ä¸€ä¸ªè™šæ‹Ÿé”™è¯¯ã€‚
        if history and isinstance(history[-1], AIMessage) and history[-1].tool_calls:
            print(f"[{session_id}] Detected broken tool call sequence. Auto-fixing...")
            for tool_call in history[-1].tool_calls:
                dummy_tool_msg = ToolMessage(
                    tool_call_id=tool_call['id'],
                    content="Error: Tool execution interrupted or failed to save output. Please retry."
                )
                history.append(dummy_tool_msg)
                self.save_message(session_id, dummy_tool_msg)
                
        return history

    def summarize_conversation(self, messages: List[Any]) -> str:
        """æ ¹æ®æ¶ˆæ¯ç”Ÿæˆå¯¹è¯çš„ç®€çŸ­æ ‡é¢˜ã€‚"""
        try:
            # ä»æœ€åå‡ æ¡æ¶ˆæ¯ä¸­æå–æ–‡æœ¬è¿›è¡Œæ‘˜è¦
            text_context = ""
            for msg in messages[:6]:
                # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯ï¼ˆSystemMessageï¼‰
                if isinstance(msg, SystemMessage):
                    continue
                role = "User" if isinstance(msg, HumanMessage) else "Assistant"
                text_context += f"{role}: {msg.content}\n"
            
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼ˆ5-10ä¸ªå­—ï¼Œä¸è¦ä½¿ç”¨å¼•å·ï¼Œç›´æ¥è¿”å›æ ‡é¢˜æ–‡æœ¬ï¼‰ã€‚
            
            å¯¹è¯å†…å®¹ï¼š
            {text_context}
            
            æ ‡é¢˜ï¼š"""
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            title = response.content.strip().replace('"', '').replace("'", "")
            print(f"Generated title: {title}")
            return title
        except Exception as e:
            print(f"Title generation error: {e}")
            return "New Conversation"

    def save_message(self, session_id: str, msg):
        if self.redis_client:
            redis_key = f"chat:{session_id}"
            self.redis_client.rpush(redis_key, self._serialize_msg(msg))
            # å¯é€‰ï¼š7 å¤©åè¿‡æœŸ
            self.redis_client.expire(redis_key, 60*60*24*7)
        else:
            self.memory_sessions[session_id].append(msg)

    def invoke(self, input_dict: Dict[str, Any], config: Optional[Dict] = None) -> Dict[str, Any]:
        session_id = "default"
        if config and "configurable" in config:
            session_id = config["configurable"].get("session_id", "default")
            
        history = self.get_session_history(session_id)
        
        user_msg = HumanMessage(content=input_dict["input"])
        history.append(user_msg) 
        self.save_message(session_id, user_msg)
        
        max_turns = 5
        turn = 0
        
        try:
            while turn < max_turns:
                turn += 1
                # 1. è°ƒç”¨å¤§æ¨¡å‹ï¼ˆLLMï¼‰
                response = self.llm_with_tools.invoke(history)
                history.append(response)
                self.save_message(session_id, response)

                # 2. æ£€æŸ¥å·¥å…·è°ƒç”¨
                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        tool_name = tool_call["name"]
                        tool_args = tool_call["args"]
                        tool_call_id = tool_call["id"]
                        func = tools_map.get(tool_name)
                        
                        try:
                            if func:
                                print(f"[{session_id}] Executing tool: {tool_name} with {tool_args}")
                                tool_output = func.invoke(tool_args)
                                tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(tool_output))
                            else:
                                print(f"[{session_id}] Warning: Tool '{tool_name}' not found.")
                                tool_msg = ToolMessage(tool_call_id=tool_call_id, content=f"Error: Tool '{tool_name}' not available.")
                        except Exception as e:
                            print(f"[{session_id}] Error executing tool {tool_name}: {e}")
                            tool_msg = ToolMessage(tool_call_id=tool_call_id, content=f"Error executing tool: {str(e)}")
                        
                        history.append(tool_msg)
                        self.save_message(session_id, tool_msg)
                    
                    # å¾ªç¯ç»§ç»­åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Œè®©å¤§æ¨¡å‹å¤„ç†å·¥å…·è¾“å‡º
                    continue
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç»ˆå›ç­”
                    return {"output": response.content}
            
            return {"output": "Max turns reached. Please refine your query."}

        except Exception as e:
            print(f"Agent Loop Error: {e}")
            import traceback
            traceback.print_exc()
            return {"output": f"System Error: {str(e)}. Please reset conversation."}

# å…¨å±€å®ä¾‹
chain_with_history = SimpleAgent()

if __name__ == "__main__":
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    print(">>> MM-WDS Agent Initialized (Manual Mode).")
    print(">>> Testing...")
    import time
    session_id = f"manual_test_{int(time.time())}"
    print(f">>> Usage Session ID: {session_id}")
    # è¯·æ±‚ä½¿ç”¨ FCM åˆ†åŒºè¿›è¡Œä¼ æ„Ÿå™¨å¸ƒç½®ï¼ˆè‡ªåŠ¨ä¼ æ„Ÿå™¨è®¡æ•°ï¼‰
    res = chain_with_history.invoke(
        {"input": "Place sensors for dataset/Exa7.inp. Use the existing FCM partition with 5 zones."},
        config={"configurable": {"session_id": session_id}}
    )
    print(res)
