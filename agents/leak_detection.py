"""
æ¼æŸæ£€æµ‹æ™ºèƒ½ä½“ - LeakDetectionAgent
åŸºäºå‹åŠ›æ•æ„Ÿåº¦åˆ†æå’Œæœºå™¨å­¦ä¹ çš„ç®¡ç½‘æ¼æŸæ£€æµ‹ç³»ç»Ÿ

ä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®å‡†å¤‡ï¼šæ£€æŸ¥åˆ†åŒºå’Œä¼ æ„Ÿå™¨é…ç½®ï¼Œç¼ºå¤±æ—¶è°ƒç”¨å…¶ä»–æ™ºèƒ½ä½“
2. æ•æ„Ÿåº¦è®¡ç®—ï¼šæ¨¡æ‹Ÿæ¼æŸåœºæ™¯ï¼Œè®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µ
3. æ•°æ®ç”Ÿæˆï¼šç”Ÿæˆå¹³è¡¡çš„è®­ç»ƒæ•°æ®é›†ï¼ˆå¼‚å¸¸+æ­£å¸¸ï¼‰
4. æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨MLPè¿›è¡Œæ¼æŸæ£€æµ‹æ¨¡å‹è®­ç»ƒ
5. æ¨ç†é¢„æµ‹ï¼šå¯¹æ–°çš„ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œæ¼æŸæ£€æµ‹

ä½œè€…ï¼šLeakAgent Team
æ—¥æœŸï¼š2025-09-18
"""

import os
import sys
import json
import uuid
import random
import logging
import numpy as np
import pandas as pd
import networkx as nx
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

# æœºå™¨å­¦ä¹ ç›¸å…³
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# æ°´ç½‘ç»œåˆ†æ
import wntr
import matplotlib.pyplot as plt
import seaborn as sns

# åŸºç¡€æ™ºèƒ½ä½“
from .base_agent import BaseAgent


class LeakDetectionMLP(nn.Module):
    """æ¼æŸæ£€æµ‹å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹"""
    
    def __init__(self, input_size: int, num_partitions: int, hidden_sizes: List[int] = [128, 64, 32], num_classes: int = None):
        super(LeakDetectionMLP, self).__init__()

        self.input_size = input_size
        self.num_classes = num_classes if num_classes is not None else (num_partitions + 1)  # +1 for normal class (0)
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, self.num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)


class LeakDetectionAgent(BaseAgent):
    """æ¼æŸæ£€æµ‹æ™ºèƒ½ä½“"""
    
    def __init__(self):
        super().__init__("LeakDetectionAgent")
        self.agent_name = "LeakDetectionAgent"
        self.downloads_folder = "downloads"
        self.uploads_folder = "uploads"
        
        # ç¡®ä¿ä¸‹è½½æ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs(self.downloads_folder, exist_ok=True)
        
        # æ¨¡å‹ç›¸å…³
        self.model = None
        self.scaler = StandardScaler()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ•°æ®ç¼“å­˜
        self.partition_data = None
        self.sensor_data = None
        self.network_model = None
        
        self.log_info(f"æ¼æŸæ£€æµ‹æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def check_dependencies(self, conversation_id: str, inp_file_path: str = None) -> Dict[str, Any]:
        """æ™ºèƒ½æ£€æŸ¥åˆ†åŒºå’Œä¼ æ„Ÿå™¨é…ç½®æ–‡ä»¶ï¼Œä¼˜å…ˆå¤ç”¨å·²æœ‰æ–‡ä»¶"""
        try:
            self.log_info("ğŸ” æ™ºèƒ½æ£€æŸ¥åˆ†åŒºå’Œä¼ æ„Ÿå™¨é…ç½®æ–‡ä»¶...")

            # æŸ¥æ‰¾ç›¸å…³æ–‡ä»¶
            partition_file = None
            sensor_file = None
            sensor_files = []  # å­˜å‚¨æ‰€æœ‰æ‰¾åˆ°çš„ä¼ æ„Ÿå™¨æ–‡ä»¶

            # æ‰«æä¸‹è½½æ–‡ä»¶å¤¹
            if os.path.exists(self.downloads_folder):
                for filename in os.listdir(self.downloads_folder):
                    if conversation_id[:8] in filename:
                        if 'partition_results' in filename and filename.endswith('.csv'):
                            partition_file = os.path.join(self.downloads_folder, filename)
                            self.log_info(f"âœ… æ‰¾åˆ°åˆ†åŒºæ–‡ä»¶: {os.path.basename(partition_file)}")
                        elif 'sensor_placement' in filename and filename.endswith('.csv'):
                            sensor_file_path = os.path.join(self.downloads_folder, filename)
                            sensor_files.append(sensor_file_path)

            # é€‰æ‹©æœ€æ–°çš„ä¼ æ„Ÿå™¨æ–‡ä»¶
            if sensor_files:
                # æŒ‰æ–‡ä»¶åæ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
                sensor_files.sort()
                sensor_file = sensor_files[-1]
                self.log_info(f"âœ… æ‰¾åˆ°ä¼ æ„Ÿå™¨å¸ƒç½®æ–‡ä»¶: {os.path.basename(sensor_file)}")

                # æ˜¾ç¤ºä¼ æ„Ÿå™¨ä¿¡æ¯
                try:
                    sensor_df = pd.read_csv(sensor_file)
                    if 'èŠ‚ç‚¹åç§°' in sensor_df.columns:
                        sensor_nodes = sensor_df['èŠ‚ç‚¹åç§°'].tolist()
                        self.log_info(f"ğŸ“ æ£€æµ‹åˆ° {len(sensor_nodes)} ä¸ªä¼ æ„Ÿå™¨èŠ‚ç‚¹: {sensor_nodes}")
                    elif 'Node' in sensor_df.columns:
                        sensor_nodes = sensor_df['Node'].tolist()
                        self.log_info(f"ğŸ“ æ£€æµ‹åˆ° {len(sensor_nodes)} ä¸ªä¼ æ„Ÿå™¨èŠ‚ç‚¹: {sensor_nodes}")
                    else:
                        self.log_warning("ä¼ æ„Ÿå™¨æ–‡ä»¶æ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•è¯»å–èŠ‚ç‚¹ä¿¡æ¯")
                except Exception as e:
                    self.log_warning(f"è¯»å–ä¼ æ„Ÿå™¨æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")

            result = {
                'partition_file': partition_file,
                'sensor_file': sensor_file,
                'missing_files': [],
                'success': True,
                'reused_files': []
            }

            # æ™ºèƒ½å¤„ç†ç¼ºå¤±æ–‡ä»¶
            missing_files = []

            # æ£€æŸ¥ä¼ æ„Ÿå™¨æ–‡ä»¶
            if sensor_file:
                result['reused_files'].append('sensor_placement')
                self.log_info("â™»ï¸ å¤ç”¨å·²æœ‰ä¼ æ„Ÿå™¨å¸ƒç½®ï¼Œæ— éœ€é‡æ–°ç”Ÿæˆ")
            else:
                missing_files.append('sensor_placement')
                self.log_warning("âš ï¸ æœªæ‰¾åˆ°ä¼ æ„Ÿå™¨å¸ƒç½®æ–‡ä»¶")

            # æ£€æŸ¥åˆ†åŒºæ–‡ä»¶
            if partition_file:
                result['reused_files'].append('partition_results')
                self.log_info("â™»ï¸ å¤ç”¨å·²æœ‰åˆ†åŒºç»“æœï¼Œæ— éœ€é‡æ–°ç”Ÿæˆ")
            else:
                missing_files.append('partition_results')
                self.log_warning("âš ï¸ æœªæ‰¾åˆ°åˆ†åŒºç»“æœæ–‡ä»¶")

            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæœ‰ä¼ æ„Ÿå™¨æ–‡ä»¶ä½†æ²¡æœ‰åˆ†åŒºæ–‡ä»¶ï¼Œå°è¯•ä»ä¼ æ„Ÿå™¨æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯
            if sensor_file and not partition_file:
                self.log_info("ğŸ§  å°è¯•ä»ä¼ æ„Ÿå™¨æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯...")
                inferred_partition = self._infer_partition_from_sensors(sensor_file, conversation_id)
                if inferred_partition.get('success'):
                    result['partition_file'] = inferred_partition['partition_file']
                    if 'partition_results' in missing_files:
                        missing_files.remove('partition_results')
                    result['reused_files'].append('partition_results_inferred')
                    self.log_info("âœ… æˆåŠŸä»ä¼ æ„Ÿå™¨æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯")

            # åªæœ‰åœ¨çœŸæ­£ç¼ºå¤±ä¸”æä¾›äº†INPæ–‡ä»¶æ—¶æ‰ç”Ÿæˆ
            if missing_files and inp_file_path:
                self.log_info(f"ğŸ”§ éœ€è¦ç”Ÿæˆç¼ºå¤±æ–‡ä»¶: {missing_files}")

                # æ™ºèƒ½ç”Ÿæˆç­–ç•¥ï¼šä¼˜å…ˆä¿æŒå·²æœ‰æ–‡ä»¶ä¸å˜
                generated_files = self._generate_missing_files_smart(missing_files, inp_file_path, conversation_id, sensor_file)

                if generated_files.get('success'):
                    # æ›´æ–°ç»“æœ
                    if 'partition_results' in missing_files and generated_files.get('partition_file'):
                        result['partition_file'] = generated_files['partition_file']
                        missing_files.remove('partition_results')

                    if 'sensor_placement' in missing_files and generated_files.get('sensor_file'):
                        result['sensor_file'] = generated_files['sensor_file']
                        missing_files.remove('sensor_placement')
                else:
                    self.log_error("è‡ªåŠ¨ç”Ÿæˆä¾èµ–æ–‡ä»¶å¤±è´¥")
                    result['success'] = False
                    result['error'] = generated_files.get('error', 'æœªçŸ¥é”™è¯¯')
                    return result

            result['missing_files'] = missing_files

            if missing_files:
                self.log_error(f"âŒ ä»ç„¶ç¼ºå¤±æ–‡ä»¶: {missing_files}")
                result['success'] = False
                result['error'] = f"ç¼ºå¤±å¿…è¦çš„é…ç½®æ–‡ä»¶: {missing_files}"
                return result

            # åŠ è½½æ•°æ®
            self.partition_data = pd.read_csv(result['partition_file'])
            self.sensor_data = pd.read_csv(result['sensor_file'])

            self.log_info(f"ğŸ“‚ æˆåŠŸåŠ è½½åˆ†åŒºæ–‡ä»¶: {os.path.basename(result['partition_file'])}")
            self.log_info(f"ğŸ“‚ æˆåŠŸåŠ è½½ä¼ æ„Ÿå™¨æ–‡ä»¶: {os.path.basename(result['sensor_file'])}")

            if result['reused_files']:
                self.log_info(f"â™»ï¸ å¤ç”¨çš„æ–‡ä»¶: {', '.join(result['reused_files'])}")

            return result

        except Exception as e:
            error_msg = f"æ£€æŸ¥ä¾èµ–æ–‡ä»¶å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def _infer_partition_from_sensors(self, sensor_file: str, conversation_id: str) -> Dict[str, Any]:
        """ä»ä¼ æ„Ÿå™¨æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯"""
        try:
            self.log_info("ä»ä¼ æ„Ÿå™¨å¸ƒç½®æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯...")

            # è¯»å–ä¼ æ„Ÿå™¨æ•°æ®
            sensor_df = pd.read_csv(sensor_file)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†åŒºä¿¡æ¯
            partition_col = None
            node_col = None

            # è¯†åˆ«åˆ—å
            for col in sensor_df.columns:
                if col in ['åˆ†åŒºç¼–å·', 'partition', 'Partition']:
                    partition_col = col
                if col in ['èŠ‚ç‚¹åç§°', 'node_id', 'Node']:
                    node_col = col

            if partition_col and node_col:
                # åˆ›å»ºç®€åŒ–çš„åˆ†åŒºæ–‡ä»¶
                partition_data = []
                for _, row in sensor_df.iterrows():
                    node_id = row[node_col]
                    partition_id = row[partition_col]
                    partition_data.append({
                        'Node': node_id,
                        'Partition': partition_id
                    })

                if partition_data:
                    # ä¿å­˜æ¨æ–­çš„åˆ†åŒºæ–‡ä»¶
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    partition_filename = f"partition_results_{conversation_id[:8]}_{timestamp}_inferred.csv"
                    partition_filepath = os.path.join(self.downloads_folder, partition_filename)

                    partition_df = pd.DataFrame(partition_data)
                    partition_df.to_csv(partition_filepath, index=False)

                    self.log_info(f"âœ… æˆåŠŸæ¨æ–­å¹¶ä¿å­˜åˆ†åŒºæ–‡ä»¶: {partition_filename}")
                    return {
                        'success': True,
                        'partition_file': partition_filepath,
                        'method': 'inferred_from_sensors'
                    }

            self.log_warning("ä¼ æ„Ÿå™¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°åˆ†åŒºä¿¡æ¯ï¼Œæ— æ³•æ¨æ–­")
            return {'success': False, 'error': 'ä¼ æ„Ÿå™¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°åˆ†åŒºä¿¡æ¯'}

        except Exception as e:
            self.log_error(f"ä»ä¼ æ„Ÿå™¨æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯å¤±è´¥: {str(e)}")
            return {'success': False, 'error': str(e)}

    def _generate_missing_files_smart(self, missing_files: List[str], inp_file_path: str,
                                    conversation_id: str, existing_sensor_file: str = None) -> Dict[str, Any]:
        """æ™ºèƒ½ç”Ÿæˆç¼ºå¤±æ–‡ä»¶ï¼Œä¼˜å…ˆä¿æŒå·²æœ‰æ–‡ä»¶ä¸å˜"""
        try:
            self.log_info("ğŸ”§ æ™ºèƒ½ç”Ÿæˆç¼ºå¤±æ–‡ä»¶...")

            from agents.partition_sim import PartitionSim
            from agents.sensor_placement import SensorPlacement

            result = {'success': True}

            # å¦‚æœç¼ºå°‘åˆ†åŒºæ–‡ä»¶ï¼Œè°ƒç”¨åˆ†åŒºæ™ºèƒ½ä½“
            if 'partition_results' in missing_files:
                self.log_info("ğŸ”§ ç”Ÿæˆåˆ†åŒºé…ç½®...")

                partition_agent = PartitionSim()
                partition_result = partition_agent.process(
                    inp_file_path=inp_file_path,
                    user_message="è‡ªåŠ¨åˆ†åŒºä¸º3ä¸ªåŒºåŸŸï¼Œä½¿ç”¨FCMèšç±»ç®—æ³•",
                    conversation_id=conversation_id
                )

                if partition_result.get('success'):
                    # æŸ¥æ‰¾ç”Ÿæˆçš„åˆ†åŒºæ–‡ä»¶
                    partition_file = None
                    if os.path.exists(self.downloads_folder):
                        for filename in os.listdir(self.downloads_folder):
                            if (conversation_id[:8] in filename and
                                'partition_results' in filename and
                                filename.endswith('.csv')):
                                partition_file = os.path.join(self.downloads_folder, filename)
                                break

                    if partition_file:
                        result['partition_file'] = partition_file
                        self.log_info("âœ… åˆ†åŒºé…ç½®ç”ŸæˆæˆåŠŸ")
                    else:
                        self.log_error("åˆ†åŒºé…ç½®ç”Ÿæˆåæœªæ‰¾åˆ°æ–‡ä»¶")
                        result['success'] = False
                        result['error'] = "åˆ†åŒºé…ç½®ç”Ÿæˆåæœªæ‰¾åˆ°æ–‡ä»¶"
                        return result
                else:
                    self.log_error(f"åˆ†åŒºé…ç½®ç”Ÿæˆå¤±è´¥: {partition_result.get('response', 'æœªçŸ¥é”™è¯¯')}")
                    result['success'] = False
                    result['error'] = f"åˆ†åŒºé…ç½®ç”Ÿæˆå¤±è´¥: {partition_result.get('response', 'æœªçŸ¥é”™è¯¯')}"
                    return result

            # å¦‚æœç¼ºå°‘ä¼ æ„Ÿå™¨æ–‡ä»¶ï¼Œè°ƒç”¨ä¼ æ„Ÿå™¨å¸ƒç½®æ™ºèƒ½ä½“
            if 'sensor_placement' in missing_files:
                self.log_info("ğŸ”§ ç”Ÿæˆä¼ æ„Ÿå™¨é…ç½®...")

                # å¦‚æœå·²æœ‰ä¼ æ„Ÿå™¨æ–‡ä»¶ï¼Œè¯´æ˜è¿™æ˜¯ä¸åº”è¯¥å‘ç”Ÿçš„æƒ…å†µ
                if existing_sensor_file:
                    self.log_warning("âš ï¸ æ£€æµ‹åˆ°å·²æœ‰ä¼ æ„Ÿå™¨æ–‡ä»¶ï¼Œä½†ä»åœ¨ç¼ºå¤±åˆ—è¡¨ä¸­ï¼Œè¿™å¯èƒ½æ˜¯é€»è¾‘é”™è¯¯")
                    result['sensor_file'] = existing_sensor_file
                    return result

                sensor_agent = SensorPlacement()

                # ç¡®ä¿æœ‰åˆ†åŒºæ–‡ä»¶ï¼ˆå¯èƒ½åˆšåˆšç”Ÿæˆçš„ï¼‰
                partition_file = result.get('partition_file')
                if not partition_file:
                    # é‡æ–°æ‰«æåˆ†åŒºæ–‡ä»¶
                    if os.path.exists(self.downloads_folder):
                        for filename in os.listdir(self.downloads_folder):
                            if (conversation_id[:8] in filename and
                                'partition_results' in filename and
                                filename.endswith('.csv')):
                                partition_file = os.path.join(self.downloads_folder, filename)
                                break

                if not partition_file:
                    self.log_error("ä¼ æ„Ÿå™¨å¸ƒç½®éœ€è¦åˆ†åŒºæ–‡ä»¶ï¼Œä½†æœªæ‰¾åˆ°")
                    result['success'] = False
                    result['error'] = "ä¼ æ„Ÿå™¨å¸ƒç½®éœ€è¦åˆ†åŒºæ–‡ä»¶ï¼Œä½†æœªæ‰¾åˆ°"
                    return result

                sensor_result = sensor_agent.process(
                    inp_file_path=inp_file_path,
                    partition_csv_path=partition_file,
                    user_message="è‡ªåŠ¨å¸ƒç½®ä¼ æ„Ÿå™¨ï¼Œä½¿ç”¨é—ä¼ ç®—æ³•ä¼˜åŒ–",
                    conversation_id=conversation_id
                )

                if sensor_result.get('success'):
                    # æŸ¥æ‰¾ç”Ÿæˆçš„ä¼ æ„Ÿå™¨æ–‡ä»¶
                    sensor_file = None
                    if os.path.exists(self.downloads_folder):
                        for filename in os.listdir(self.downloads_folder):
                            if (conversation_id[:8] in filename and
                                'sensor_placement' in filename and
                                filename.endswith('.csv')):
                                sensor_file = os.path.join(self.downloads_folder, filename)
                                break

                    if sensor_file:
                        result['sensor_file'] = sensor_file
                        self.log_info("âœ… ä¼ æ„Ÿå™¨é…ç½®ç”ŸæˆæˆåŠŸ")
                    else:
                        self.log_error("ä¼ æ„Ÿå™¨é…ç½®ç”Ÿæˆåæœªæ‰¾åˆ°æ–‡ä»¶")
                        result['success'] = False
                        result['error'] = "ä¼ æ„Ÿå™¨é…ç½®ç”Ÿæˆåæœªæ‰¾åˆ°æ–‡ä»¶"
                        return result
                else:
                    self.log_error(f"ä¼ æ„Ÿå™¨é…ç½®ç”Ÿæˆå¤±è´¥: {sensor_result.get('response', 'æœªçŸ¥é”™è¯¯')}")
                    result['success'] = False
                    result['error'] = f"ä¼ æ„Ÿå™¨é…ç½®ç”Ÿæˆå¤±è´¥: {sensor_result.get('response', 'æœªçŸ¥é”™è¯¯')}"
                    return result

            return result

        except Exception as e:
            self.log_error(f"æ™ºèƒ½ç”Ÿæˆç¼ºå¤±æ–‡ä»¶å¤±è´¥: {str(e)}")
            return {'success': False, 'error': str(e)}

    def _generate_missing_files(self, missing_files: List[str], inp_file_path: str, conversation_id: str) -> Dict[str, Any]:
        """è°ƒç”¨å…¶ä»–æ™ºèƒ½ä½“ç”Ÿæˆç¼ºå¤±çš„é…ç½®æ–‡ä»¶ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        self.log_info("âš ï¸ ä½¿ç”¨å…¼å®¹æ€§æ–¹æ³•ç”Ÿæˆç¼ºå¤±æ–‡ä»¶ï¼Œå»ºè®®ä½¿ç”¨æ™ºèƒ½ç”Ÿæˆæ–¹æ³•")
        return self._generate_missing_files_smart(missing_files, inp_file_path, conversation_id)

    def load_network_model(self, inp_file_path: str) -> bool:
        """åŠ è½½æ°´ç½‘ç»œæ¨¡å‹"""
        try:
            self.log_info(f"åŠ è½½æ°´ç½‘ç»œæ¨¡å‹: {inp_file_path}")
            self.network_model = wntr.network.WaterNetworkModel(inp_file_path)
            
            # è·å–ç½‘ç»œåŸºæœ¬ä¿¡æ¯
            num_nodes = len(self.network_model.node_name_list)
            num_junctions = len(self.network_model.junction_name_list)
            num_links = len(self.network_model.link_name_list)
            
            self.log_info(f"ç½‘ç»œåŠ è½½æˆåŠŸ: {num_nodes}ä¸ªèŠ‚ç‚¹, {num_junctions}ä¸ªéœ€æ°´èŠ‚ç‚¹, {num_links}ä¸ªç®¡æ®µ")
            return True
            
        except Exception as e:
            self.log_error(f"åŠ è½½ç½‘ç»œæ¨¡å‹å¤±è´¥: {str(e)}")
            return False
    
    def calculate_centrality(self, demand_nodes: List[str]) -> Dict[str, float]:
        """è®¡ç®—èŠ‚ç‚¹çš„ç½‘ç»œä¸­å¿ƒæ€§"""
        try:
            # è½¬æ¢ä¸ºNetworkXå›¾
            G = self.network_model.to_graph().to_undirected()
            
            # è®¡ç®—å„ç§ä¸­å¿ƒæ€§
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G)
            closeness_centrality = nx.closeness_centrality(G)
            
            # ç»¼åˆä¸­å¿ƒæ€§åˆ†æ•°
            centrality_scores = {}
            for node in demand_nodes:
                if node in G.nodes():
                    score = (
                        degree_centrality.get(node, 0) + 
                        betweenness_centrality.get(node, 0) + 
                        closeness_centrality.get(node, 0)
                    ) / 3
                    centrality_scores[node] = score
                else:
                    centrality_scores[node] = 0
            
            return centrality_scores
            
        except Exception as e:
            self.log_error(f"è®¡ç®—ç½‘ç»œä¸­å¿ƒæ€§å¤±è´¥: {str(e)}")
            return {}
    
    def get_total_demand(self, node_name: str) -> float:
        """è·å–èŠ‚ç‚¹çš„æ€»éœ€æ°´é‡"""
        try:
            node = self.network_model.get_node(node_name)
            total_demand = 0
            
            for demand_ts in node.demand_timeseries_list:
                total_demand += abs(demand_ts.base_value)
            
            return total_demand
            
        except Exception as e:
            self.log_error(f"è·å–èŠ‚ç‚¹éœ€æ°´é‡å¤±è´¥: {str(e)}")
            return 0
    
    def select_critical_nodes(self, num_scenarios: int) -> List[str]:
        """é€‰æ‹©å…³é”®èŠ‚ç‚¹è¿›è¡Œæ¼æŸæ¨¡æ‹Ÿ"""
        try:
            self.log_info(f"é€‰æ‹© {num_scenarios} ä¸ªå…³é”®èŠ‚ç‚¹è¿›è¡Œæ¼æŸæ¨¡æ‹Ÿ...")
            
            # è·å–éœ€æ°´èŠ‚ç‚¹å’Œä¼ æ„Ÿå™¨èŠ‚ç‚¹
            demand_nodes = self.network_model.junction_name_list

            # è·å–ä¼ æ„Ÿå™¨èŠ‚ç‚¹ï¼Œå°è¯•ä¸åŒçš„åˆ—å
            sensor_nodes = []
            if self.sensor_data is not None:
                if 'èŠ‚ç‚¹ID' in self.sensor_data.columns:
                    sensor_nodes = self.sensor_data['èŠ‚ç‚¹ID'].tolist()
                elif 'èŠ‚ç‚¹åç§°' in self.sensor_data.columns:
                    sensor_nodes = self.sensor_data['èŠ‚ç‚¹åç§°'].tolist()
                else:
                    # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ç¬¬ä¸€åˆ—
                    sensor_nodes = self.sensor_data.iloc[:, 0].tolist()
            
            # ç­–ç•¥1: é«˜éœ€æ°´é‡èŠ‚ç‚¹ (æŒ‰éœ€æ°´é‡æ’åºï¼Œå–å‰50%)
            demand_ranking = sorted(demand_nodes, 
                                  key=lambda x: self.get_total_demand(x), 
                                  reverse=True)
            high_demand_nodes = demand_ranking[:len(demand_ranking)//2]
            
            # ç­–ç•¥2: ç½‘ç»œä¸­å¿ƒä½ç½®èŠ‚ç‚¹
            centrality_scores = self.calculate_centrality(demand_nodes)
            central_nodes = sorted(demand_nodes, 
                                 key=lambda x: centrality_scores.get(x, 0), 
                                 reverse=True)[:len(demand_nodes)//2]
            
            # ç­–ç•¥3: éä¼ æ„Ÿå™¨èŠ‚ç‚¹ä¼˜å…ˆ
            non_sensor_nodes = [node for node in demand_nodes if node not in sensor_nodes]
            
            # ç»¼åˆé€‰æ‹©ï¼šä¼˜å…ˆé€‰æ‹©æ—¢æ˜¯é«˜éœ€æ°´é‡åˆæ˜¯ä¸­å¿ƒä½ç½®çš„éä¼ æ„Ÿå™¨èŠ‚ç‚¹
            priority_nodes = list(set(high_demand_nodes) & set(central_nodes) & set(non_sensor_nodes))
            
            # å¦‚æœä¼˜å…ˆèŠ‚ç‚¹ä¸å¤Ÿï¼Œè¡¥å……å…¶ä»–å…³é”®èŠ‚ç‚¹
            if len(priority_nodes) < num_scenarios:
                remaining_critical = list(set(high_demand_nodes + central_nodes) & set(non_sensor_nodes))
                priority_nodes.extend([n for n in remaining_critical if n not in priority_nodes])
            
            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œæ·»åŠ å…¶ä»–éä¼ æ„Ÿå™¨èŠ‚ç‚¹
            if len(priority_nodes) < num_scenarios:
                other_nodes = [n for n in non_sensor_nodes if n not in priority_nodes]
                priority_nodes.extend(other_nodes)
            
            selected_nodes = priority_nodes[:num_scenarios]
            
            self.log_info(f"é€‰æ‹©äº† {len(selected_nodes)} ä¸ªå…³é”®èŠ‚ç‚¹:")
            for i, node in enumerate(selected_nodes[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                demand = self.get_total_demand(node)
                centrality = centrality_scores.get(node, 0)
                self.log_info(f"  {i+1}. {node} (éœ€æ°´é‡: {demand:.3f}, ä¸­å¿ƒæ€§: {centrality:.3f})")
            
            if len(selected_nodes) > 5:
                self.log_info(f"  ... è¿˜æœ‰ {len(selected_nodes)-5} ä¸ªèŠ‚ç‚¹")
            
            return selected_nodes

        except Exception as e:
            error_msg = f"é€‰æ‹©å…³é”®èŠ‚ç‚¹å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return []

    def run_hydraulic_simulation(self) -> Optional[wntr.sim.results.SimulationResults]:
        """è¿è¡Œæ°´åŠ›ä»¿çœŸ"""
        try:
            sim = wntr.sim.EpanetSimulator(self.network_model)
            results = sim.run_sim()
            return results
        except Exception as e:
            self.log_error(f"æ°´åŠ›ä»¿çœŸå¤±è´¥: {str(e)}")
            return None

    def get_sensor_pressures(self, results: wntr.sim.results.SimulationResults) -> np.ndarray:
        """æå–ä¼ æ„Ÿå™¨èŠ‚ç‚¹çš„å‹åŠ›æ•°æ®"""
        try:
            # å°è¯•ä¸åŒçš„åˆ—å
            if 'èŠ‚ç‚¹ID' in self.sensor_data.columns:
                sensor_nodes = self.sensor_data['èŠ‚ç‚¹ID'].tolist()
            elif 'èŠ‚ç‚¹åç§°' in self.sensor_data.columns:
                sensor_nodes = self.sensor_data['èŠ‚ç‚¹åç§°'].tolist()
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ç¬¬ä¸€åˆ—
                sensor_nodes = self.sensor_data.iloc[:, 0].tolist()

            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºä¼ æ„Ÿå™¨èŠ‚ç‚¹å’Œå¯ç”¨åˆ—
            available_columns = list(results.node['pressure'].columns)
            self.log_info(f"ä¼ æ„Ÿå™¨èŠ‚ç‚¹: {sensor_nodes[:5]}... (å…±{len(sensor_nodes)}ä¸ª)")
            self.log_info(f"å¯ç”¨å‹åŠ›åˆ—: {available_columns[:10]}... (å…±{len(available_columns)}ä¸ª)")

            # å°è¯•å°†ä¼ æ„Ÿå™¨èŠ‚ç‚¹åç§°è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            sensor_nodes_str = [str(node) for node in sensor_nodes]

            # æ£€æŸ¥å“ªäº›ä¼ æ„Ÿå™¨èŠ‚ç‚¹åœ¨ä»¿çœŸç»“æœä¸­å­˜åœ¨
            valid_sensors = []
            for sensor in sensor_nodes_str:
                if sensor in available_columns:
                    valid_sensors.append(sensor)
                else:
                    # å°è¯•ä¸åŒçš„æ ¼å¼
                    for col in available_columns:
                        if str(col) == sensor or str(col).strip() == sensor.strip():
                            valid_sensors.append(col)
                            break

            if not valid_sensors:
                self.log_error(f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ä¼ æ„Ÿå™¨èŠ‚ç‚¹")
                self.log_error(f"ä¼ æ„Ÿå™¨èŠ‚ç‚¹: {sensor_nodes_str}")
                self.log_error(f"å¯ç”¨åˆ—æ ·ä¾‹: {available_columns[:20]}")
                return np.array([])

            self.log_info(f"æ‰¾åˆ° {len(valid_sensors)} ä¸ªæœ‰æ•ˆä¼ æ„Ÿå™¨: {valid_sensors}")

            pressure_data = results.node['pressure'].loc[:, valid_sensors].values
            return pressure_data
        except Exception as e:
            self.log_error(f"æå–ä¼ æ„Ÿå™¨å‹åŠ›å¤±è´¥: {str(e)}")
            return np.array([])

    def simulate_leak(self, leak_node: str, leak_ratio: float) -> Tuple[np.ndarray, int]:
        """æ¨¡æ‹Ÿå•ä¸ªèŠ‚ç‚¹çš„æ¼æŸåœºæ™¯"""
        try:
            # ä¿å­˜åŸå§‹éœ€æ°´é‡
            node = self.network_model.get_node(leak_node)
            original_demands = []
            for demand_ts in node.demand_timeseries_list:
                original_demands.append(demand_ts.base_value)
                # å¢åŠ éœ€æ°´é‡æ¨¡æ‹Ÿæ¼æŸ
                demand_ts.base_value = demand_ts.base_value * (1 + leak_ratio)

            # è¿è¡Œæ¼æŸä»¿çœŸ
            leak_results = self.run_hydraulic_simulation()
            if leak_results is None:
                return np.array([]), 0

            # è·å–ä¼ æ„Ÿå™¨å‹åŠ›
            leak_pressures = self.get_sensor_pressures(leak_results)

            # æ¢å¤åŸå§‹éœ€æ°´é‡
            for i, demand_ts in enumerate(node.demand_timeseries_list):
                demand_ts.base_value = original_demands[i]

            # ç¡®å®šæ¼æŸèŠ‚ç‚¹æ‰€å±åˆ†åŒº
            partition_label = self.get_node_partition(leak_node)

            return leak_pressures, partition_label

        except Exception as e:
            self.log_error(f"æ¨¡æ‹Ÿæ¼æŸå¤±è´¥: {str(e)}")
            return np.array([]), 0

    def get_node_partition(self, node_name: str) -> int:
        """è·å–èŠ‚ç‚¹æ‰€å±åˆ†åŒº"""
        try:
            if self.partition_data is not None:
                node_row = self.partition_data[self.partition_data['èŠ‚ç‚¹ID'] == node_name]
                if not node_row.empty:
                    return int(node_row.iloc[0]['åˆ†åŒºç¼–å·'])
            return 1  # é»˜è®¤åˆ†åŒº
        except Exception as e:
            self.log_error(f"è·å–èŠ‚ç‚¹åˆ†åŒºå¤±è´¥: {str(e)}")
            return 1

    def calculate_sensitivity_matrix(self, normal_pressures: np.ndarray,
                                   leak_pressures: np.ndarray) -> np.ndarray:
        """è®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µ"""
        try:
            # è®¡ç®—å‹åŠ›å·®çš„ç»å¯¹å€¼
            pressure_diff = np.abs(leak_pressures - normal_pressures)

            # å¯¹æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé¿å…é™¤é›¶ï¼‰
            normalized_diff = np.zeros_like(pressure_diff)

            for t in range(pressure_diff.shape[0]):
                for s in range(pressure_diff.shape[1]):
                    if normal_pressures[t, s] > 1e-6:  # é¿å…é™¤é›¶
                        normalized_diff[t, s] = pressure_diff[t, s] / normal_pressures[t, s]
                    else:
                        normalized_diff[t, s] = 0

            # è®¡ç®—æ—¶é—´å¹³å‡å€¼
            sensitivity_vector = np.mean(normalized_diff, axis=0)

            return sensitivity_vector

        except Exception as e:
            self.log_error(f"è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µå¤±è´¥: {str(e)}")
            return np.array([])

    def add_sensor_noise(self, pressure_data: np.ndarray, noise_level: float = 0.02) -> np.ndarray:
        """æ·»åŠ ä¼ æ„Ÿå™¨å™ªå£°"""
        try:
            # é«˜æ–¯å™ªå£°ï¼šå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸ºå‹åŠ›å€¼çš„ç™¾åˆ†æ¯”
            noise = np.random.normal(0, pressure_data * noise_level)

            # ç¡®ä¿å‹åŠ›å€¼ä¸ä¸ºè´Ÿ
            noisy_pressure = np.maximum(pressure_data + noise, 0.1)

            return noisy_pressure

        except Exception as e:
            self.log_error(f"æ·»åŠ ä¼ æ„Ÿå™¨å™ªå£°å¤±è´¥: {str(e)}")
            return pressure_data

    def generate_training_data(self, num_scenarios: int) -> Tuple[np.ndarray, np.ndarray]:
        """ç”Ÿæˆå¹³è¡¡çš„è®­ç»ƒæ•°æ®é›†"""
        try:
            self.log_info(f"å¼€å§‹ç”Ÿæˆ {num_scenarios*2} ä¸ªè®­ç»ƒæ ·æœ¬...")
            self.log_info(f"  - {num_scenarios} ä¸ªå¼‚å¸¸æ ·æœ¬ (æ¼æŸåœºæ™¯)")
            self.log_info(f"  - {num_scenarios} ä¸ªæ­£å¸¸æ ·æœ¬ (å«å™ªå£°)")

            # è¿è¡ŒåŸºå‡†ä»¿çœŸ
            self.log_info("è¿è¡ŒåŸºå‡†æ°´åŠ›ä»¿çœŸ...")
            normal_results = self.run_hydraulic_simulation()
            if normal_results is None:
                raise Exception("åŸºå‡†ä»¿çœŸå¤±è´¥")

            normal_pressures = self.get_sensor_pressures(normal_results)
            self.log_info(f"åŸºå‡†å‹åŠ›æ•°æ®å½¢çŠ¶: {normal_pressures.shape}")

            # é€‰æ‹©å…³é”®èŠ‚ç‚¹
            critical_nodes = self.select_critical_nodes(num_scenarios)
            if len(critical_nodes) < num_scenarios:
                self.log_warning(f"åªæ‰¾åˆ° {len(critical_nodes)} ä¸ªå…³é”®èŠ‚ç‚¹ï¼Œå°‘äºè¯·æ±‚çš„ {num_scenarios} ä¸ª")
                num_scenarios = len(critical_nodes)

            # ç”Ÿæˆå¼‚å¸¸æ•°æ®
            self.log_info("ç”Ÿæˆå¼‚å¸¸æ•°æ®...")
            anomaly_data = []
            anomaly_labels = []

            # ä¸ºäº†ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œå¯¹æ¯ä¸ªå…³é”®èŠ‚ç‚¹ç”Ÿæˆå¤šä¸ªæ¼æŸåœºæ™¯
            scenarios_per_node = max(1, num_scenarios // len(critical_nodes))
            if scenarios_per_node * len(critical_nodes) < num_scenarios:
                scenarios_per_node += 1

            scenario_count = 0
            for node_idx, leak_node in enumerate(critical_nodes):
                for scenario_idx in range(scenarios_per_node):
                    if scenario_count >= num_scenarios:
                        break

                    scenario_count += 1
                    self.log_info(f"  æ¨¡æ‹Ÿæ¼æŸ {scenario_count}/{num_scenarios}: {leak_node} (åœºæ™¯{scenario_idx+1})")

                    # éšæœºæ¼æŸæ¯”ä¾‹ (10%-30%)
                    leak_ratio = random.uniform(0.1, 0.3)

                    # æ¨¡æ‹Ÿæ¼æŸ
                    leak_pressures, partition_label = self.simulate_leak(leak_node, leak_ratio)

                    if leak_pressures.size > 0:
                        # è®¡ç®—æ•æ„Ÿåº¦å‘é‡
                        sensitivity_vector = self.calculate_sensitivity_matrix(normal_pressures, leak_pressures)

                        if sensitivity_vector.size > 0:
                            anomaly_data.append(sensitivity_vector)
                            anomaly_labels.append(partition_label)
                            self.log_info(f"    æ¼æŸæ¯”ä¾‹: {leak_ratio:.1%}, åˆ†åŒº: {partition_label}")

                if scenario_count >= num_scenarios:
                    break

            # ç”Ÿæˆæ­£å¸¸æ•°æ®
            self.log_info("ç”Ÿæˆæ­£å¸¸æ•°æ®...")
            normal_data = []
            normal_labels = []

            for i in range(len(anomaly_data)):  # ç”ŸæˆåŒç­‰æ•°é‡çš„æ­£å¸¸æ•°æ®
                # æ·»åŠ ä¸åŒæ°´å¹³çš„å™ªå£°
                noise_level = random.uniform(0.01, 0.03)
                noisy_pressures = self.add_sensor_noise(normal_pressures, noise_level)

                # è®¡ç®—"æ•æ„Ÿåº¦"ï¼ˆå®é™…ä¸Šæ˜¯å™ªå£°å‘é‡ï¼‰
                noise_vector = self.calculate_sensitivity_matrix(normal_pressures, noisy_pressures)

                if noise_vector.size > 0:
                    normal_data.append(noise_vector)
                    normal_labels.append(0)  # æ­£å¸¸æ ‡ç­¾ä¸º0

            # åˆå¹¶æ•°æ®
            all_data = np.array(anomaly_data + normal_data)
            all_labels = np.array(anomaly_labels + normal_labels)

            self.log_info(f"æ•°æ®ç”Ÿæˆå®Œæˆ:")
            self.log_info(f"  æ€»æ ·æœ¬æ•°: {len(all_data)}")
            self.log_info(f"  ç‰¹å¾ç»´åº¦: {all_data.shape[1] if len(all_data) > 0 else 0}")
            self.log_info(f"  æ­£å¸¸æ ·æœ¬: {np.sum(all_labels == 0)}")
            self.log_info(f"  å¼‚å¸¸æ ·æœ¬: {np.sum(all_labels > 0)}")

            # è¯¦ç»†çš„æ ‡ç­¾ç»Ÿè®¡
            unique_labels, counts = np.unique(all_labels, return_counts=True)
            self.log_info(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}")
            self.log_info(f"  æ ‡ç­¾èŒƒå›´: [{np.min(all_labels)}, {np.max(all_labels)}]")

            # ä¿®å¤ï¼šä¸è¿›è¡Œæ ‡ç­¾é‡æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åˆ†åŒºç¼–å·
            # æ ‡ç­¾0=æ­£å¸¸ï¼Œæ ‡ç­¾N=åˆ†åŒºNæ¼æŸï¼Œä¿æŒåˆ†åŒºç¼–å·ä¸æ ‡ç­¾çš„ç›´æ¥å¯¹åº”å…³ç³»
            self.log_info(f"  ä¿æŒåŸå§‹æ ‡ç­¾: 0=æ­£å¸¸ï¼Œ1-{np.max(unique_labels[unique_labels > 0]) if len(unique_labels[unique_labels > 0]) > 0 else 0}=å¯¹åº”åˆ†åŒºæ¼æŸ")
            self.log_info(f"  æœ€ç»ˆæ ‡ç­¾èŒƒå›´: [{np.min(all_labels)}, {np.max(all_labels)}]")

            return all_data, all_labels

        except Exception as e:
            error_msg = f"ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return np.array([]), np.array([])

    def prepare_datasets(self, data: np.ndarray, labels: np.ndarray) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """å‡†å¤‡è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†"""
        try:
            # æ•°æ®æ ‡å‡†åŒ–
            data_scaled = self.scaler.fit_transform(data)

            # æ£€æŸ¥æ•°æ®é›†å¤§å°å’Œç±»åˆ«åˆ†å¸ƒ
            unique_labels, label_counts = np.unique(labels, return_counts=True)
            min_samples_per_class = np.min(label_counts)
            total_samples = len(data)

            self.log_info(f"æ•°æ®é›†åˆ†æ:")
            self.log_info(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
            self.log_info(f"  ç±»åˆ«æ•°: {len(unique_labels)}")
            self.log_info(f"  æœ€å°‘æ ·æœ¬ç±»åˆ«: {min_samples_per_class} ä¸ªæ ·æœ¬")

            # å¦‚æœæ•°æ®é›†å¤ªå°æˆ–æŸäº›ç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²
            if total_samples < 10 or min_samples_per_class < 2:
                self.log_warning("æ•°æ®é›†è¾ƒå°ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²ç­–ç•¥")

                # ç®€å•åˆ†å‰²ï¼š80% è®­ç»ƒï¼Œ20% éªŒè¯ï¼Œä¸è®¾ç½®æµ‹è¯•é›†
                if total_samples >= 5:
                    split_idx = int(0.8 * total_samples)
                    X_train = data_scaled[:split_idx]
                    y_train = labels[:split_idx]
                    X_val = data_scaled[split_idx:]
                    y_val = labels[split_idx:]
                    X_test = X_val  # éªŒè¯é›†åŒæ—¶ä½œä¸ºæµ‹è¯•é›†
                    y_test = y_val
                else:
                    # æ•°æ®å¤ªå°‘ï¼Œå…¨éƒ¨ç”¨äºè®­ç»ƒ
                    X_train = data_scaled
                    y_train = labels
                    X_val = data_scaled
                    y_val = labels
                    X_test = data_scaled
                    y_test = labels
            else:
                # æ­£å¸¸åˆ†å±‚åˆ†å‰²
                X_temp, X_test, y_temp, y_test = train_test_split(
                    data_scaled, labels, test_size=0.1, random_state=42, stratify=labels
                )

                X_train, X_val, y_train, y_val = train_test_split(
                    X_temp, y_temp, test_size=0.22, random_state=42, stratify=y_temp
                )

            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            X_train_tensor = torch.FloatTensor(X_train)
            y_train_tensor = torch.LongTensor(y_train)
            X_val_tensor = torch.FloatTensor(X_val)
            y_val_tensor = torch.LongTensor(y_val)
            X_test_tensor = torch.FloatTensor(X_test)
            y_test_tensor = torch.LongTensor(y_test)

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œè°ƒæ•´batch_size
            batch_size = min(8, len(X_train))  # å°æ•°æ®é›†ä½¿ç”¨å°batch_size

            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

            self.log_info(f"æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
            self.log_info(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
            self.log_info(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
            self.log_info(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
            self.log_info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")

            return train_loader, val_loader, test_loader

        except Exception as e:
            error_msg = f"å‡†å¤‡æ•°æ®é›†å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None, None, None

    def train_model(self, train_loader: DataLoader, val_loader: DataLoader,
                   input_size: int, num_partitions: int, epochs: int = 100, num_classes: int = None) -> Dict[str, Any]:
        """è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹"""
        try:
            self.log_info(f"å¼€å§‹è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹...")
            self.log_info(f"  è¾“å…¥ç»´åº¦: {input_size}")
            self.log_info(f"  åˆ†åŒºæ•°é‡: {num_partitions}")
            self.log_info(f"  è®­ç»ƒè½®æ•°: {epochs}")

            # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„ç±»åˆ«æ•°
            if num_classes is None:
                num_classes = num_partitions + 1  # é»˜è®¤ï¼šåˆ†åŒºæ•°+1ï¼ˆæ­£å¸¸ç±»ï¼‰

            self.log_info(f"  æ¨¡å‹ç±»åˆ«æ•°: {num_classes}")
            self.model = LeakDetectionMLP(input_size, num_partitions, num_classes=num_classes).to(self.device)

            # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(self.model.parameters(), lr=0.001)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)

            # è®­ç»ƒå†å²
            train_losses = []
            val_losses = []
            train_accuracies = []
            val_accuracies = []

            best_val_acc = 0
            best_model_state = None

            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                self.model.train()
                train_loss = 0
                train_correct = 0
                train_total = 0

                for batch_data, batch_labels in train_loader:
                    batch_data, batch_labels = batch_data.to(self.device), batch_labels.to(self.device)

                    optimizer.zero_grad()
                    outputs = self.model(batch_data)
                    loss = criterion(outputs, batch_labels)
                    loss.backward()
                    optimizer.step()

                    train_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    train_total += batch_labels.size(0)
                    train_correct += (predicted == batch_labels).sum().item()

                # éªŒè¯é˜¶æ®µ
                self.model.eval()
                val_loss = 0
                val_correct = 0
                val_total = 0

                with torch.no_grad():
                    for batch_data, batch_labels in val_loader:
                        batch_data, batch_labels = batch_data.to(self.device), batch_labels.to(self.device)

                        outputs = self.model(batch_data)
                        loss = criterion(outputs, batch_labels)

                        val_loss += loss.item()
                        _, predicted = torch.max(outputs.data, 1)
                        val_total += batch_labels.size(0)
                        val_correct += (predicted == batch_labels).sum().item()

                # è®¡ç®—å‡†ç¡®ç‡
                train_acc = 100 * train_correct / train_total
                val_acc = 100 * val_correct / val_total

                # è®°å½•å†å²
                train_losses.append(train_loss / len(train_loader))
                val_losses.append(val_loss / len(val_loader))
                train_accuracies.append(train_acc)
                val_accuracies.append(val_acc)

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_model_state = self.model.state_dict().copy()

                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step()

                # æ¯10è½®æ‰“å°ä¸€æ¬¡
                if (epoch + 1) % 10 == 0:
                    self.log_info(f"  Epoch {epoch+1}/{epochs}: "
                                f"Train Loss: {train_losses[-1]:.4f}, "
                                f"Train Acc: {train_acc:.2f}%, "
                                f"Val Loss: {val_losses[-1]:.4f}, "
                                f"Val Acc: {val_acc:.2f}%")

            # åŠ è½½æœ€ä½³æ¨¡å‹
            if best_model_state is not None:
                self.model.load_state_dict(best_model_state)

            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„PythonåŸç”Ÿç±»å‹
            training_history = {
                'train_losses': [float(x) for x in train_losses],
                'val_losses': [float(x) for x in val_losses],
                'train_accuracies': [float(x) for x in train_accuracies],
                'val_accuracies': [float(x) for x in val_accuracies],
                'best_val_accuracy': float(best_val_acc),
                'final_train_loss': float(train_losses[-1]) if train_losses else 0.0,
                'final_val_loss': float(val_losses[-1]) if val_losses else 0.0
            }

            self.log_info(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

            return training_history

        except Exception as e:
            error_msg = f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {}

    def evaluate_model(self, test_loader: DataLoader) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            self.log_info("å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

            if self.model is None:
                raise Exception("æ¨¡å‹æœªè®­ç»ƒ")

            self.model.eval()
            all_predictions = []
            all_labels = []

            with torch.no_grad():
                for batch_data, batch_labels in test_loader:
                    batch_data, batch_labels = batch_data.to(self.device), batch_labels.to(self.device)

                    outputs = self.model(batch_data)
                    _, predicted = torch.max(outputs.data, 1)

                    all_predictions.extend(predicted.cpu().numpy())
                    all_labels.extend(batch_labels.cpu().numpy())

            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            accuracy = accuracy_score(all_labels, all_predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(
                all_labels, all_predictions, average='weighted', zero_division=0
            )

            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(all_labels, all_predictions)

            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„PythonåŸç”Ÿç±»å‹
            evaluation_results = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'confusion_matrix': cm.tolist(),
                'predictions': [int(x) for x in all_predictions],
                'true_labels': [int(x) for x in all_labels]
            }

            self.log_info(f"æ¨¡å‹è¯„ä¼°å®Œæˆ:")
            self.log_info(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
            self.log_info(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
            self.log_info(f"  å¬å›ç‡ (Recall): {recall:.4f}")
            self.log_info(f"  F1åˆ†æ•° (F1-Score): {f1:.4f}")

            return evaluation_results

        except Exception as e:
            error_msg = f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {}

    def save_model(self, conversation_id: str, model_info: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"leak_detection_model_{conversation_id[:8]}_{timestamp}.pth"
            model_path = os.path.join(self.downloads_folder, model_filename)

            # ä¿å­˜æ¨¡å‹çŠ¶æ€ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
            model_state = {
                'model_state_dict': self.model.state_dict(),
                'scaler_mean': [float(x) for x in self.scaler.mean_],
                'scaler_scale': [float(x) for x in self.scaler.scale_],
                'input_size': int(model_info['input_size']),
                'num_partitions': int(model_info['num_partitions']),
                'num_classes': int(model_info.get('num_classes', model_info['num_partitions'] + 1)),  # ä¿å­˜å®é™…ç±»åˆ«æ•°
                'max_partition': int(model_info.get('max_partition', model_info['num_partitions'])),  # ä¿å­˜æœ€å¤§åˆ†åŒºç¼–å·
                'model_info': model_info,
                'timestamp': timestamp
            }

            torch.save(model_state, model_path)

            file_size = os.path.getsize(model_path)

            self.log_info(f"æ¨¡å‹å·²ä¿å­˜: {model_filename} ({file_size} å­—èŠ‚)")

            return {
                'success': True,
                'filename': model_filename,
                'file_path': model_path,
                'file_size': file_size,
                'download_url': f'/download/{model_filename}'
            }

        except Exception as e:
            error_msg = f"ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def load_model(self, model_path: str) -> bool:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            self.log_info(f"åŠ è½½æ¨¡å‹: {model_path}")

            # åŠ è½½æ¨¡å‹çŠ¶æ€ï¼Œè®¾ç½®weights_only=Falseä»¥å…¼å®¹æ—§ç‰ˆæœ¬
            try:
                model_state = torch.load(model_path, map_location=self.device, weights_only=False)
            except TypeError:
                # å…¼å®¹æ—§ç‰ˆæœ¬PyTorch
                model_state = torch.load(model_path, map_location=self.device)

            # é‡å»ºæ¨¡å‹ - ä½¿ç”¨ä¿å­˜çš„å®é™…ç±»åˆ«æ•°
            input_size = model_state['input_size']
            num_partitions = model_state['num_partitions']

            # ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ç±»åˆ«æ•°ï¼Œå¦åˆ™ä½¿ç”¨ä¼ ç»Ÿè®¡ç®—æ–¹å¼
            num_classes = model_state.get('num_classes', num_partitions + 1)
            max_partition = model_state.get('max_partition', num_partitions)

            self.model = LeakDetectionMLP(input_size, num_partitions, num_classes=num_classes).to(self.device)
            self.model.load_state_dict(model_state['model_state_dict'])

            # é‡å»ºæ ‡å‡†åŒ–å™¨
            self.scaler.mean_ = np.array(model_state['scaler_mean'])
            self.scaler.scale_ = np.array(model_state['scaler_scale'])

            self.log_info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: è¾“å…¥ç»´åº¦={input_size}, æœ€å¤§åˆ†åŒºç¼–å·={max_partition}, ç±»åˆ«æ•°={num_classes}")
            self.log_info("æ³¨æ„: å®é™…æ¨ç†æ—¶å°†ä½¿ç”¨å½“å‰å¯¹è¯çš„åˆ†åŒºé…ç½®")

            return True

        except Exception as e:
            error_msg = f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return False

    def predict_leak(self, sensor_data: np.ndarray) -> Dict[str, Any]:
        """é¢„æµ‹æ¼æŸæƒ…å†µ"""
        try:
            if self.model is None:
                raise Exception("æ¨¡å‹æœªåŠ è½½")

            self.log_info(f"å¼€å§‹æ¼æŸæ£€æµ‹ï¼Œè¾“å…¥æ•°æ®å½¢çŠ¶: {sensor_data.shape}")

            # æ•°æ®é¢„å¤„ç†
            if len(sensor_data.shape) == 1:
                sensor_data = sensor_data.reshape(1, -1)

            # æ ‡å‡†åŒ–
            sensor_data_scaled = self.scaler.transform(sensor_data)

            # è½¬æ¢ä¸ºå¼ é‡
            input_tensor = torch.FloatTensor(sensor_data_scaled).to(self.device)

            # é¢„æµ‹
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)

            # è§£æç»“æœ
            predictions = predicted.cpu().numpy()
            probs = probabilities.cpu().numpy()

            results = []
            for i, (pred, prob) in enumerate(zip(predictions, probs)):
                # ç¡®ä¿è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
                pred_int = int(pred)
                confidence = float(prob[pred_int])

                if pred_int == 0:
                    status = "æ­£å¸¸"
                    partition = None
                else:
                    status = "å¼‚å¸¸"
                    partition = pred_int

                results.append({
                    'sample_id': int(i + 1),
                    'status': status,
                    'partition': partition,
                    'confidence': confidence,
                    'probabilities': [float(p) for p in prob]  # ç¡®ä¿æ‰€æœ‰æ¦‚ç‡éƒ½æ˜¯floatç±»å‹
                })

            self.log_info(f"æ¼æŸæ£€æµ‹å®Œæˆï¼Œæ£€æµ‹åˆ° {len(results)} ä¸ªæ ·æœ¬")

            return {
                'success': True,
                'results': results,
                'summary': {
                    'total_samples': int(len(results)),
                    'normal_samples': int(sum(1 for r in results if r['status'] == 'æ­£å¸¸')),
                    'anomaly_samples': int(sum(1 for r in results if r['status'] == 'å¼‚å¸¸'))
                }
            }

        except Exception as e:
            error_msg = f"æ¼æŸé¢„æµ‹å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def save_training_data(self, data: np.ndarray, labels: np.ndarray,
                          conversation_id: str) -> Dict[str, Any]:
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"leak_training_data_{conversation_id[:8]}_{timestamp}.csv"
            filepath = os.path.join(self.downloads_folder, filename)

            # å‡†å¤‡æ•°æ®
            df_data = []
            # ä¿®å¤åˆ—åé—®é¢˜
            if self.sensor_data is not None:
                if 'èŠ‚ç‚¹ID' in self.sensor_data.columns:
                    sensor_nodes = self.sensor_data['èŠ‚ç‚¹ID'].tolist()
                elif 'èŠ‚ç‚¹åç§°' in self.sensor_data.columns:
                    sensor_nodes = self.sensor_data['èŠ‚ç‚¹åç§°'].tolist()
                else:
                    sensor_nodes = self.sensor_data.iloc[:, 0].tolist()
            else:
                sensor_nodes = []

            for i, (sample, label) in enumerate(zip(data, labels)):
                # ç¡®ä¿è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
                row = {'æ ·æœ¬ID': int(i + 1), 'æ ‡ç­¾': int(label)}

                # æ·»åŠ ä¼ æ„Ÿå™¨æ•°æ®
                for j, sensor in enumerate(sensor_nodes):
                    if j < len(sample):
                        row[f'ä¼ æ„Ÿå™¨_{sensor}'] = float(sample[j])

                df_data.append(row)

            # ä¿å­˜ä¸ºCSV
            df = pd.DataFrame(df_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')

            file_size = os.path.getsize(filepath)

            self.log_info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜: {filename} ({file_size} å­—èŠ‚)")

            return {
                'success': True,
                'filename': filename,
                'file_path': filepath,
                'file_size': file_size,
                'download_url': f'/download/{filename}'
            }

        except Exception as e:
            error_msg = f"ä¿å­˜è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def save_evaluation_report(self, evaluation_results: Dict[str, Any],
                              training_history: Dict[str, Any],
                              conversation_id: str) -> Dict[str, Any]:
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"leak_evaluation_{conversation_id[:8]}_{timestamp}.csv"
            filepath = os.path.join(self.downloads_folder, filename)

            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            report_data = []

            # åŸºæœ¬æŒ‡æ ‡
            report_data.append({
                'æŒ‡æ ‡': 'å‡†ç¡®ç‡ (Accuracy)',
                'æ•°å€¼': f"{evaluation_results.get('accuracy', 0):.4f}",
                'è¯´æ˜': 'æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ¯”ä¾‹'
            })

            report_data.append({
                'æŒ‡æ ‡': 'ç²¾ç¡®ç‡ (Precision)',
                'æ•°å€¼': f"{evaluation_results.get('precision', 0):.4f}",
                'è¯´æ˜': 'é¢„æµ‹ä¸ºæ­£ä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹'
            })

            report_data.append({
                'æŒ‡æ ‡': 'å¬å›ç‡ (Recall)',
                'æ•°å€¼': f"{evaluation_results.get('recall', 0):.4f}",
                'è¯´æ˜': 'å®é™…æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹'
            })

            report_data.append({
                'æŒ‡æ ‡': 'F1åˆ†æ•° (F1-Score)',
                'æ•°å€¼': f"{evaluation_results.get('f1_score', 0):.4f}",
                'è¯´æ˜': 'ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡'
            })

            # è®­ç»ƒä¿¡æ¯
            if training_history:
                report_data.append({
                    'æŒ‡æ ‡': 'æœ€ä½³éªŒè¯å‡†ç¡®ç‡',
                    'æ•°å€¼': f"{training_history.get('best_val_accuracy', 0):.2f}%",
                    'è¯´æ˜': 'è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡'
                })

            # ä¿å­˜ä¸ºCSV
            df = pd.DataFrame(report_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')

            file_size = os.path.getsize(filepath)

            self.log_info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {filename} ({file_size} å­—èŠ‚)")

            return {
                'success': True,
                'filename': filename,
                'file_path': filepath,
                'file_size': file_size,
                'download_url': f'/download/{filename}'
            }

        except Exception as e:
            error_msg = f"ä¿å­˜è¯„ä¼°æŠ¥å‘Šå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def train_leak_detection_model(self, inp_file_path: str, conversation_id: str,
                                  num_scenarios: int = 50, epochs: int = 100) -> Dict[str, Any]:
        """è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹çš„ä¸»æ¥å£"""
        try:
            self.log_info("=" * 60)
            self.log_info("å¼€å§‹è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹")
            self.log_info("=" * 60)

            # 1. æ™ºèƒ½æ£€æŸ¥ä¾èµ–æ–‡ä»¶
            dependency_check = self.check_dependencies(conversation_id, inp_file_path)
            if not dependency_check.get('success'):
                return dependency_check

            # æ˜¾ç¤ºæ™ºèƒ½å¤ç”¨ä¿¡æ¯
            if dependency_check.get('reused_files'):
                reused_files = dependency_check.get('reused_files', [])
                self.log_info("ğŸ¯ æ™ºèƒ½å·¥ä½œæµä¼˜åŒ–:")
                for reused_file in reused_files:
                    if reused_file == 'sensor_placement':
                        self.log_info("   âœ… å¤ç”¨å·²æœ‰ä¼ æ„Ÿå™¨å¸ƒç½®ï¼Œè·³è¿‡ä¼ æ„Ÿå™¨å¸ƒç½®æ­¥éª¤")
                    elif reused_file == 'partition_results':
                        self.log_info("   âœ… å¤ç”¨å·²æœ‰åˆ†åŒºç»“æœï¼Œè·³è¿‡åˆ†åŒºåˆ†ææ­¥éª¤")
                    elif reused_file == 'partition_results_inferred':
                        self.log_info("   âœ… ä»ä¼ æ„Ÿå™¨æ–‡ä»¶æ¨æ–­åˆ†åŒºä¿¡æ¯ï¼Œè·³è¿‡åˆ†åŒºåˆ†ææ­¥éª¤")
                self.log_info("   âš¡ å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ï¼Œç›´æ¥è¿›å…¥æ¨¡å‹è®­ç»ƒé˜¶æ®µ")

            # 2. åŠ è½½ç½‘ç»œæ¨¡å‹
            if not self.load_network_model(inp_file_path):
                return {'success': False, 'error': 'åŠ è½½ç½‘ç»œæ¨¡å‹å¤±è´¥'}

            # 3. ç”Ÿæˆè®­ç»ƒæ•°æ®
            data, labels = self.generate_training_data(num_scenarios)
            if len(data) == 0:
                return {'success': False, 'error': 'ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥'}

            # 4. å‡†å¤‡æ•°æ®é›†
            train_loader, val_loader, test_loader = self.prepare_datasets(data, labels)
            if train_loader is None:
                return {'success': False, 'error': 'å‡†å¤‡æ•°æ®é›†å¤±è´¥'}

            # 5. è®­ç»ƒæ¨¡å‹
            # è·å–æ‰€æœ‰å”¯ä¸€æ ‡ç­¾å¹¶ç¡®ä¿æ ‡ç­¾èŒƒå›´æ­£ç¡®
            unique_labels = np.unique(labels)
            max_label = int(np.max(unique_labels))
            min_label = int(np.min(unique_labels))

            self.log_info(f"æ ‡ç­¾ç»Ÿè®¡: æœ€å°å€¼={min_label}, æœ€å¤§å€¼={max_label}, å”¯ä¸€å€¼={unique_labels}")

            # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦è¿ç»­ä¸”ä»0å¼€å§‹
            expected_labels = list(range(min_label, max_label + 1))
            if not all(label in unique_labels for label in expected_labels):
                self.log_warning(f"æ ‡ç­¾ä¸è¿ç»­ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒé—®é¢˜")

            # æ¨¡å‹çš„ç±»åˆ«æ•°åº”è¯¥æ˜¯æœ€å¤§æ ‡ç­¾å€¼+1ï¼ˆå› ä¸ºæ ‡ç­¾ä»0å¼€å§‹ï¼‰
            num_classes = max_label + 1

            # ä¿®å¤ï¼šåˆ†åŒºæ•°åº”è¯¥æ˜¯æœ€å¤§åˆ†åŒºç¼–å·ï¼Œè€Œä¸æ˜¯åˆ†åŒºç§ç±»æ•°
            # å› ä¸ºåˆ†åŒºç¼–å·å¯èƒ½ä¸è¿ç»­ï¼ˆå¦‚1,2,3,4,5,6ï¼‰ï¼Œè€Œä¸æ˜¯ä»1å¼€å§‹çš„è¿ç»­ç¼–å·
            max_partition = np.max(unique_labels[unique_labels > 0]) if len(unique_labels[unique_labels > 0]) > 0 else 0
            num_partitions = max_partition  # ä½¿ç”¨æœ€å¤§åˆ†åŒºç¼–å·ä½œä¸ºåˆ†åŒºæ•°
            input_size = data.shape[1]

            self.log_info(f"æ¨¡å‹é…ç½®: è¾“å…¥ç»´åº¦={input_size}, æœ€å¤§åˆ†åŒºç¼–å·={max_partition}, ç±»åˆ«æ•°={num_classes}")

            # é‡æ–°è®¡ç®—æ ‡ç­¾åˆ†å¸ƒç”¨äºæ—¥å¿—
            unique_labels_with_counts, counts = np.unique(labels, return_counts=True)
            self.log_info(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(unique_labels_with_counts, counts))}")

            # æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰æ ‡ç­¾éƒ½åœ¨[0, num_classes-1]èŒƒå›´å†…
            if np.any(labels < 0) or np.any(labels >= num_classes):
                error_msg = f"æ ‡ç­¾è¶…å‡ºèŒƒå›´ [0, {num_classes-1}]: å®é™…èŒƒå›´ [{np.min(labels)}, {np.max(labels)}]"
                self.log_error(error_msg)
                return {'success': False, 'error': error_msg}

            training_history = self.train_model(train_loader, val_loader, input_size, num_partitions, epochs, num_classes)
            if not training_history:
                return {'success': False, 'error': 'æ¨¡å‹è®­ç»ƒå¤±è´¥'}

            # 6. è¯„ä¼°æ¨¡å‹
            evaluation_results = self.evaluate_model(test_loader)
            if not evaluation_results:
                return {'success': False, 'error': 'æ¨¡å‹è¯„ä¼°å¤±è´¥'}

            # 7. ä¿å­˜æ¨¡å‹å’Œç»“æœ
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„PythonåŸç”Ÿç±»å‹
            model_info = {
                'input_size': int(input_size),
                'num_partitions': int(num_partitions),
                'num_scenarios': int(num_scenarios),
                'epochs': int(epochs),
                'evaluation': evaluation_results,
                'training_history': training_history
            }

            # æ›´æ–°æ¨¡å‹ä¿¡æ¯ï¼ŒåŒ…å«æ­£ç¡®çš„åˆ†åŒºæ•°å’Œç±»åˆ«æ•°
            model_info.update({
                'max_partition': int(max_partition),
                'num_classes': int(num_classes)
            })

            model_save_result = self.save_model(conversation_id, model_info)
            training_data_result = self.save_training_data(data, labels, conversation_id)
            evaluation_report_result = self.save_evaluation_report(evaluation_results, training_history, conversation_id)

            self.log_info("=" * 60)
            self.log_info("æ¼æŸæ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ")
            self.log_info("=" * 60)

            return {
                'success': True,
                'model_info': model_info,
                'evaluation': evaluation_results,
                'training_history': training_history,
                'files': {
                    'model': model_save_result,
                    'training_data': training_data_result,
                    'evaluation_report': evaluation_report_result
                }
            }

        except Exception as e:
            error_msg = f"è®­ç»ƒæ¼æŸæ£€æµ‹æ¨¡å‹å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def detect_leak_from_file(self, sensor_file_path: str, model_file_path: str, conversation_id: str = None) -> Dict[str, Any]:
        """ä»æ–‡ä»¶è¯»å–ä¼ æ„Ÿå™¨æ•°æ®å¹¶è¿›è¡Œæ¼æŸæ£€æµ‹"""
        try:
            self.log_info("=" * 60)
            self.log_info("å¼€å§‹æ¼æŸæ£€æµ‹")
            self.log_info("=" * 60)

            # 1. åŠ è½½æ¨¡å‹
            if not self.load_model(model_file_path):
                return {'success': False, 'error': 'åŠ è½½æ¨¡å‹å¤±è´¥'}

            # 2. è¯»å–åˆ†åŒºæ–‡ä»¶è·å–å®é™…åˆ†åŒºæ•°
            actual_num_partitions = None
            if conversation_id:
                partition_file = self._find_partition_file(conversation_id)
                if partition_file:
                    try:
                        partition_df = pd.read_csv(partition_file)
                        # è·å–å®é™…åˆ†åŒºæ•°
                        if 'åˆ†åŒºç¼–å·' in partition_df.columns:
                            actual_num_partitions = partition_df['åˆ†åŒºç¼–å·'].max()
                        elif 'Partition' in partition_df.columns:
                            actual_num_partitions = partition_df['Partition'].max()

                        if actual_num_partitions:
                            self.log_info(f"ä»åˆ†åŒºæ–‡ä»¶è¯»å–å®é™…åˆ†åŒºæ•°: {actual_num_partitions}")
                            # æ›´æ–°æ¨¡å‹çš„åˆ†åŒºæ•°ä¿¡æ¯ï¼ˆç”¨äºç»“æœè§£é‡Šï¼‰
                            self._actual_num_partitions = actual_num_partitions
                        else:
                            self.log_warning("æ— æ³•ä»åˆ†åŒºæ–‡ä»¶ç¡®å®šåˆ†åŒºæ•°ï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼")
                    except Exception as e:
                        self.log_warning(f"è¯»å–åˆ†åŒºæ–‡ä»¶å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤åˆ†åŒºæ•°")

            # 3. è¯»å–ä¼ æ„Ÿå™¨æ•°æ®
            self.log_info(f"è¯»å–ä¼ æ„Ÿå™¨æ•°æ®: {sensor_file_path}")

            try:
                sensor_df = pd.read_csv(sensor_file_path)
                self.log_info(f"ä¼ æ„Ÿå™¨æ•°æ®å½¢çŠ¶: {sensor_df.shape}")

                # æå–æ•°å€¼æ•°æ®ï¼ˆæ’é™¤IDåˆ—ç­‰ï¼‰
                numeric_columns = sensor_df.select_dtypes(include=[np.number]).columns
                sensor_data = sensor_df[numeric_columns].values

                if sensor_data.size == 0:
                    return {'success': False, 'error': 'ä¼ æ„Ÿå™¨æ–‡ä»¶ä¸­æ²¡æœ‰æ•°å€¼æ•°æ®'}

            except Exception as e:
                return {'success': False, 'error': f'è¯»å–ä¼ æ„Ÿå™¨æ–‡ä»¶å¤±è´¥: {str(e)}'}

            # 4. è¿›è¡Œé¢„æµ‹
            prediction_results = self.predict_leak(sensor_data)
            if not prediction_results['success']:
                return prediction_results

            self.log_info("=" * 60)
            self.log_info("æ¼æŸæ£€æµ‹å®Œæˆ")
            self.log_info("=" * 60)

            return prediction_results

        except Exception as e:
            error_msg = f"æ¼æŸæ£€æµ‹å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {'success': False, 'error': error_msg}

    def _find_partition_file(self, conversation_id: str) -> str:
        """æŸ¥æ‰¾å¯¹è¯å¯¹åº”çš„åˆ†åŒºæ–‡ä»¶"""
        try:
            if not os.path.exists(self.downloads_folder):
                return None

            # æŸ¥æ‰¾åˆ†åŒºæ–‡ä»¶
            for filename in os.listdir(self.downloads_folder):
                if (conversation_id[:8] in filename and
                    'partition_results' in filename and
                    filename.endswith('.csv')):
                    partition_file = os.path.join(self.downloads_folder, filename)
                    self.log_info(f"æ‰¾åˆ°åˆ†åŒºæ–‡ä»¶: {os.path.basename(partition_file)}")
                    return partition_file

            self.log_warning(f"æœªæ‰¾åˆ°å¯¹è¯ {conversation_id[:8]} çš„åˆ†åŒºæ–‡ä»¶")
            return None

        except Exception as e:
            self.log_error(f"æŸ¥æ‰¾åˆ†åŒºæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None

    def build_response_prompt(self, result: Dict[str, Any], user_message: str,
                             operation_type: str) -> str:
        """æ„å»ºå“åº”prompt"""
        try:
            if operation_type == "training":
                return self._build_training_prompt(result, user_message)
            elif operation_type == "detection":
                return self._build_detection_prompt(result, user_message)
            else:
                return "æ“ä½œå®Œæˆã€‚"

        except Exception as e:
            self.log_error(f"æ„å»ºå“åº”promptå¤±è´¥: {str(e)}")
            return "æ“ä½œå®Œæˆï¼Œä½†ç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯ã€‚"

    def _build_training_prompt(self, result: Dict[str, Any], user_message: str) -> str:
        """æ„å»ºè®­ç»ƒå“åº”prompt"""
        if not result.get('success', False):
            return f"""
æ¼æŸæ£€æµ‹æ¨¡å‹è®­ç»ƒå¤±è´¥ã€‚

é”™è¯¯ä¿¡æ¯ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}

è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜ï¼š
1. æ˜¯å¦å·²å®Œæˆç®¡ç½‘åˆ†åŒºåˆ†æ
2. æ˜¯å¦å·²å®Œæˆä¼ æ„Ÿå™¨å¸ƒç½®
3. ç½‘ç»œæ–‡ä»¶æ˜¯å¦æ­£ç¡®
4. ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³

ç”¨æˆ·è¯·æ±‚ï¼š{user_message}
"""

        model_info = result.get('model_info', {})
        evaluation = result.get('evaluation', {})
        training_history = result.get('training_history', {})
        files = result.get('files', {})

        # è®¡ç®—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        total_samples = model_info.get('num_scenarios', 0) * 2  # æ­£å¸¸+å¼‚å¸¸æ ·æœ¬
        normal_samples = model_info.get('num_scenarios', 0)
        anomaly_samples = model_info.get('num_scenarios', 0)

        # è·å–åˆ†åŒºç»Ÿè®¡
        num_partitions = model_info.get('num_partitions', 0)
        samples_per_partition = anomaly_samples // max(num_partitions, 1) if num_partitions > 0 else 0

        # æ„å»ºæ€§èƒ½æŒ‡æ ‡è¯´æ˜
        accuracy = evaluation.get('accuracy', 0)
        precision = evaluation.get('precision', 0)
        recall = evaluation.get('recall', 0)
        f1_score = evaluation.get('f1_score', 0)

        # å®‰å…¨è·å–è®­ç»ƒå†å²æ•°æ®
        final_train_loss = training_history.get('final_train_loss', 0)
        final_val_loss = training_history.get('final_val_loss', 0)
        best_val_accuracy = training_history.get('best_val_accuracy', 0)

        # æ€§èƒ½è¯„çº§
        def get_performance_grade(score):
            if score >= 0.9: return "ä¼˜ç§€ ğŸŒŸ"
            elif score >= 0.8: return "è‰¯å¥½ âœ…"
            elif score >= 0.7: return "ä¸€èˆ¬ âš ï¸"
            else: return "éœ€æ”¹è¿› âŒ"

        return f"""
ğŸ‰ æ¼æŸæ£€æµ‹æ¨¡å‹è®­ç»ƒæˆåŠŸå®Œæˆï¼

## ğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡
- **æ€»æ ·æœ¬æ•°**: {total_samples} ä¸ª (å¹³è¡¡æ•°æ®é›†)
- **æ­£å¸¸æ ·æœ¬**: {normal_samples} ä¸ª (åŒ…å«ä¼ æ„Ÿå™¨å™ªå£°)
- **å¼‚å¸¸æ ·æœ¬**: {anomaly_samples} ä¸ª (åˆ†å¸ƒåœ¨ {num_partitions} ä¸ªåˆ†åŒº)
- **æ¯åˆ†åŒºæ ·æœ¬**: çº¦ {samples_per_partition} ä¸ªæ¼æŸåœºæ™¯
- **ä¼ æ„Ÿå™¨æ•°é‡**: {model_info.get('input_size', 'N/A')} ä¸ª
- **è®­ç»ƒè½®æ•°**: {model_info.get('epochs', 'N/A')} è½®

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½è¯„ä¼°
- **å‡†ç¡®ç‡ (Accuracy)**: {accuracy:.4f} ({accuracy*100:.2f}%) - {get_performance_grade(accuracy)}
- **ç²¾ç¡®ç‡ (Precision)**: {precision:.4f} ({precision*100:.2f}%) - {get_performance_grade(precision)}
- **å¬å›ç‡ (Recall)**: {recall:.4f} ({recall*100:.2f}%) - {get_performance_grade(recall)}
- **F1åˆ†æ•° (F1-Score)**: {f1_score:.4f} ({f1_score*100:.2f}%) - {get_performance_grade(f1_score)}

### ğŸ“‹ æ€§èƒ½æŒ‡æ ‡è¯´æ˜
- **å‡†ç¡®ç‡**: æ‰€æœ‰é¢„æµ‹ä¸­æ­£ç¡®çš„æ¯”ä¾‹ (åŒ…æ‹¬æ­£å¸¸å’Œå¼‚å¸¸)
- **ç²¾ç¡®ç‡**: é¢„æµ‹ä¸ºå¼‚å¸¸çš„æ ·æœ¬ä¸­çœŸæ­£å¼‚å¸¸çš„æ¯”ä¾‹
- **å¬å›ç‡**: çœŸæ­£å¼‚å¸¸çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹
- **F1åˆ†æ•°**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°

## ğŸ¯ è®­ç»ƒè¿‡ç¨‹
- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {final_train_loss:.6f}
- **æœ€ç»ˆéªŒè¯æŸå¤±**: {final_val_loss:.6f}
- **æœ€ä½³éªŒè¯å‡†ç¡®ç‡**: {best_val_accuracy:.4f}

## ğŸ“ ç”Ÿæˆæ–‡ä»¶
ä»¥ä¸‹æ–‡ä»¶å·²ç”Ÿæˆå¹¶å¯ä¾›ä¸‹è½½ï¼š

### ğŸ¤– æ¨¡å‹æ–‡ä»¶
- **æ–‡ä»¶å**: `{files.get('model', {}).get('filename', 'N/A')}`
- **æ ¼å¼**: PyTorch PTHæ ¼å¼
- **ç”¨é€”**: ç”¨äºæ¼æŸæ£€æµ‹æ¨ç†

### ğŸ“Š è®­ç»ƒæ•°æ®æ–‡ä»¶
- **æ–‡ä»¶å**: `{files.get('training_data', {}).get('filename', 'N/A')}`
- **æ ¼å¼**: CSVæ ¼å¼
- **å†…å®¹**: åŒ…å«è®­ç»ƒç”¨çš„ä¼ æ„Ÿå™¨å‹åŠ›æ•°æ®å’Œæ ‡ç­¾

### ğŸ“ˆ è¯„ä¼°æŠ¥å‘Šæ–‡ä»¶
- **æ–‡ä»¶å**: `{files.get('evaluation_report', {}).get('filename', 'N/A')}`
- **æ ¼å¼**: CSVæ ¼å¼
- **å†…å®¹**: è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°æŒ‡æ ‡å’Œæ··æ·†çŸ©é˜µ

## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œ
æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼æ‚¨ç°åœ¨å¯ä»¥ï¼š
1. **ä¸‹è½½æ¨¡å‹æ–‡ä»¶**ï¼šç‚¹å‡»ä¸‹æ–¹çš„PTHæ–‡ä»¶ä¸‹è½½æŒ‰é’®
2. **è¿›è¡Œæ¼æŸæ£€æµ‹**ï¼šä¸Šä¼ ä¼ æ„Ÿå™¨å‹åŠ›æ•°æ®CSVæ–‡ä»¶
3. **æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š**ï¼šä¸‹è½½è¯„ä¼°æŠ¥å‘Šäº†è§£æ›´å¤šæ€§èƒ½ç»†èŠ‚

ç”¨æˆ·è¯·æ±‚ï¼š{user_message}

è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š

ç¥å¥½ï¼Œ

Tianwei Mu
Guangzhou Institute of Industrial Intelligence
"""

    def _build_detection_prompt(self, result: Dict[str, Any], user_message: str) -> str:
        """æ„å»ºæ£€æµ‹å“åº”prompt"""
        if not result.get('success', False):
            return f"""
æ¼æŸæ£€æµ‹å¤±è´¥ã€‚

é”™è¯¯ä¿¡æ¯ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}

è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜ï¼š
1. ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
3. æ•°æ®ç»´åº¦æ˜¯å¦åŒ¹é…

ç”¨æˆ·è¯·æ±‚ï¼š{user_message}
"""

        results = result.get('results', [])
        summary = result.get('summary', {})

        # ç»Ÿè®¡å¼‚å¸¸æƒ…å†µå’Œç½®ä¿¡åº¦
        anomaly_partitions = {}
        normal_confidences = []
        all_confidences = []

        for r in results:
            all_confidences.append(r['confidence'])
            if r['status'] == 'å¼‚å¸¸':
                partition = r['partition']
                if partition not in anomaly_partitions:
                    anomaly_partitions[partition] = []
                anomaly_partitions[partition].append(r)
            else:
                normal_confidences.append(r['confidence'])

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_samples = summary.get('total_samples', 0)
        normal_samples = summary.get('normal_samples', 0)
        anomaly_samples = summary.get('anomaly_samples', 0)

        normal_percentage = (normal_samples / total_samples * 100) if total_samples > 0 else 0
        anomaly_percentage = (anomaly_samples / total_samples * 100) if total_samples > 0 else 0

        avg_confidence = np.mean(all_confidences) if all_confidences else 0
        avg_normal_confidence = np.mean(normal_confidences) if normal_confidences else 0

        prompt = f"""
ğŸ¯ **æ™ºèƒ½æ¼æŸæ£€æµ‹æ¨ç†å®Œæˆ**

âœ… **æ¨ç†æ¨¡å¼è¯´æ˜**ï¼šç³»ç»Ÿæ£€æµ‹åˆ°å·²æœ‰è®­ç»ƒå¥½çš„æ¼æŸæ£€æµ‹æ¨¡å‹ï¼Œç›´æ¥è¿›è¡Œæ¨ç†åˆ†æï¼Œæ— éœ€é‡å¤æ‰§è¡Œåˆ†åŒºã€ä¼ æ„Ÿå™¨å¸ƒç½®ã€æ¨¡å‹è®­ç»ƒç­‰æ­¥éª¤ã€‚

## ğŸ“Š æ£€æµ‹æ¦‚å†µ
- **åˆ†ææ ·æœ¬æ•°**: {total_samples} ä¸ªæ—¶é—´ç‚¹
- **æ­£å¸¸çŠ¶æ€**: {normal_samples} ä¸ªæ ·æœ¬ ({normal_percentage:.1f}%)
- **å¼‚å¸¸çŠ¶æ€**: {anomaly_samples} ä¸ªæ ·æœ¬ ({anomaly_percentage:.1f}%)
- **å¹³å‡æ£€æµ‹ç½®ä¿¡åº¦**: {avg_confidence:.3f}

## ğŸ“ˆ ç½®ä¿¡åº¦åˆ†æ
- **æ­£å¸¸çŠ¶æ€å¹³å‡ç½®ä¿¡åº¦**: {avg_normal_confidence:.3f}
- **æ•´ä½“æ£€æµ‹å¯é æ€§**: {'é«˜' if avg_confidence > 0.8 else 'ä¸­ç­‰' if avg_confidence > 0.6 else 'è¾ƒä½'}

"""

        if anomaly_partitions:
            prompt += "## âš ï¸ æ£€æµ‹åˆ°æ¼æŸå¼‚å¸¸\n"
            for partition, samples in anomaly_partitions.items():
                avg_confidence = np.mean([s['confidence'] for s in samples])
                max_confidence = max([s['confidence'] for s in samples])
                min_confidence = min([s['confidence'] for s in samples])

                prompt += f"""
### ğŸš¨ åˆ†åŒº {partition} æ¼æŸè­¦æŠ¥
- **å¼‚å¸¸æ ·æœ¬æ•°**: {len(samples)} ä¸ª
- **å¹³å‡ç½®ä¿¡åº¦**: {avg_confidence:.3f}
- **æœ€é«˜ç½®ä¿¡åº¦**: {max_confidence:.3f}
- **æœ€ä½ç½®ä¿¡åº¦**: {min_confidence:.3f}
- **ä¸¥é‡ç¨‹åº¦**: {'é«˜' if avg_confidence > 0.8 else 'ä¸­ç­‰' if avg_confidence > 0.6 else 'ä½'}
"""

            prompt += "\n## ğŸ”§ å»ºè®®æªæ–½\n"
            prompt += "1. **ç«‹å³æ£€æŸ¥**ï¼šå¯¹æ£€æµ‹åˆ°å¼‚å¸¸çš„åˆ†åŒºè¿›è¡Œç°åœºæ£€æŸ¥\n"
            prompt += "2. **ç¡®è®¤æ¼æŸ**ï¼šä½¿ç”¨å…¶ä»–æ£€æµ‹æ‰‹æ®µéªŒè¯æ¼æŸä½ç½®\n"
            prompt += "3. **åˆ¶å®šä¿®å¤è®¡åˆ’**ï¼šæ ¹æ®æ¼æŸä¸¥é‡ç¨‹åº¦å®‰æ’ç»´ä¿®\n"
            prompt += "4. **æŒç»­ç›‘æ§**ï¼šåŠ å¼ºå¯¹å¼‚å¸¸åˆ†åŒºçš„ç›‘æ§é¢‘ç‡\n"
        else:
            prompt += "## âœ… æœªæ£€æµ‹åˆ°æ¼æŸå¼‚å¸¸\n"
            prompt += f"æ‰€æœ‰ {total_samples} ä¸ªæ—¶é—´ç‚¹çš„ä¼ æ„Ÿå™¨æ•°æ®å‡æ˜¾ç¤ºç®¡ç½‘è¿è¡Œæ­£å¸¸ã€‚\n"
            prompt += f"å¹³å‡æ£€æµ‹ç½®ä¿¡åº¦ä¸º {avg_normal_confidence:.3f}ï¼Œç³»ç»Ÿè¿è¡Œç¨³å®šã€‚\n"

        prompt += f"""

## ğŸ“‹ è¯¦ç»†ç»“æœ
"""

        for i, r in enumerate(results[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç»“æœ
            status_icon = "âœ…" if r['status'] == 'æ­£å¸¸' else "âš ï¸"
            prompt += f"- æ ·æœ¬ {r['sample_id']}: {status_icon} {r['status']}"
            if r['partition']:
                prompt += f" (åˆ†åŒº {r['partition']})"
            prompt += f" - ç½®ä¿¡åº¦: {r['confidence']:.3f}\n"

        if len(results) > 5:
            prompt += f"... è¿˜æœ‰ {len(results)-5} ä¸ªæ ·æœ¬\n"

        prompt += f"""

ç”¨æˆ·è¯·æ±‚ï¼š{user_message}

è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š

ç¥å¥½ï¼Œ

Tianwei Mu
Guangzhou Institute of Industrial Intelligence
"""

        return prompt

    def process(self, *args, **kwargs) -> Dict[str, Any]:
        """å®ç°BaseAgentçš„æŠ½è±¡æ–¹æ³•process"""
        # è¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºå…¼å®¹BaseAgentæ¥å£
        # å®é™…çš„å¤„ç†é€»è¾‘åœ¨train_leak_detection_modelå’Œdetect_leak_from_fileä¸­
        return {
            'success': True,
            'message': 'æ¼æŸæ£€æµ‹æ™ºèƒ½ä½“å·²å°±ç»ªã€‚è¯·ä½¿ç”¨train_leak_detection_modelè¿›è¡Œè®­ç»ƒæˆ–detect_leak_from_fileè¿›è¡Œæ£€æµ‹ã€‚'
        }
