"""
PartitionSim ç®¡ç½‘åˆ†åŒºæ™ºèƒ½ä½“
è´Ÿè´£å¤„ç†.inpæ–‡ä»¶ï¼Œè¿›è¡Œç®¡ç½‘FCMèšç±»åˆ†åŒºå’Œç¦»ç¾¤ç‚¹æ£€æµ‹
"""
import os
import re
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from .base_agent import BaseAgent
from .intent_classifier_fast import FastIntentClassifier as IntentClassifier

try:
    import wntr
    import skfuzzy as fuzz
    import networkx as nx
    WNTR_AVAILABLE = True
    SKFUZZY_AVAILABLE = True
except ImportError as e:
    WNTR_AVAILABLE = False
    SKFUZZY_AVAILABLE = False

class PartitionSim(BaseAgent):
    """ç®¡ç½‘åˆ†åŒºæ™ºèƒ½ä½“"""
    
    def __init__(self):
        super().__init__("PartitionSim")

        if not WNTR_AVAILABLE:
            self.log_error("WNTRåº“æœªå®‰è£…ï¼Œç®¡ç½‘åˆ†æåŠŸèƒ½ä¸å¯ç”¨")
        if not SKFUZZY_AVAILABLE:
            self.log_error("scikit-fuzzyåº“æœªå®‰è£…ï¼ŒFCMèšç±»åŠŸèƒ½ä¸å¯ç”¨")

        self.intent_classifier = IntentClassifier()
        self.downloads_folder = 'downloads'
        os.makedirs(self.downloads_folder, exist_ok=True)

        # ç¼“å­˜æœºåˆ¶ï¼šé¿å…é‡å¤è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µ
        self._sensitivity_cache = {}  # {file_path: {matrix, last_modified}}
        
        # é»˜è®¤å‚æ•°
        self.default_params = {
            'k': 5,  # é»˜è®¤åˆ†åŒºæ•°
            'm': 1.5,  # FCMæ¨¡ç³Šåº¦å‚æ•°
            'error': 1e-6,  # æ”¶æ•›é˜ˆå€¼
            'maxiter': 1000,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
            'perturb_rate': 0.1,  # æ‰°åŠ¨ç‡
            'k_nearest': 10,  # KNNå‚æ•°
            'outliers_detection': True,  # æ˜¯å¦è¿›è¡Œç¦»ç¾¤ç‚¹æ£€æµ‹
            'seed': 42  # éšæœºç§å­
        }
    
    def parse_user_intent(self, user_message: str):
        """è§£æç”¨æˆ·æ„å›¾å’Œå‚æ•°"""
        intent_result = self.intent_classifier.classify_intent(user_message)

        # æå–åˆ†åŒºç›¸å…³å‚æ•°
        params = self.default_params.copy()

        # æå–åˆ†åŒºæ•°é‡ - æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡
        k_patterns = [
            # è‹±æ–‡æ ¼å¼
            r'partition\s+into\s+(\d+)\s+regions?',
            r'partition\s+into\s+(\d+)\s+areas?',
            r'(\d+)\s+regions?',
            r'(\d+)\s+partitions?',
            # ä¸­æ–‡æ ¼å¼
            r'åˆ†æˆ?(\d+)ä¸ª?åˆ†åŒº',
            r'åˆ†æˆ?(\d+)ä¸ª?åŒºåŸŸ?',
            r'åˆ†æˆ?(\d+)ä¸ª?åŒº',
            r'(\d+)ä¸ª?åˆ†åŒº',
            r'(\d+)ä¸ª?åŒºåŸŸ?',
            r'(\d+)ä¸ª?åŒº',
            r'k\s*=\s*(\d+)',
            r'èšç±»æ•°\s*[ï¼š:]\s*(\d+)',
            r'åˆ†åŒºæ•°\s*[ï¼š:]\s*(\d+)',
            r'åˆ†åŒºæ•°ç›®\s*[ï¼š:ä¸º]\s*(\d+)',
            r'åˆ†åŒºæ•°ç›®ä¸º(\d+)'
        ]

        for pattern in k_patterns:
            match = re.search(pattern, user_message, re.IGNORECASE)
            if match:
                params['k'] = int(match.group(1))
                self.logger.info(f"[PartitionSim] è§£æåˆ°åˆ†åŒºæ•°é‡: {params['k']} (åŒ¹é…æ¨¡å¼: {pattern})")
                break
        
        # æå–FCMå‚æ•°
        m_patterns = [
            r'm\s*=\s*([\d.]+)',
            r'æ¨¡ç³Šåº¦\s*[ï¼š:=]\s*([\d.]+)',
            r'æ¨¡ç³Šå‚æ•°\s*[ï¼š:=]\s*([\d.]+)',
            r'æ¨¡ç³Šåº¦=([\d.]+)'
        ]
        
        for pattern in m_patterns:
            match = re.search(pattern, user_message, re.IGNORECASE)
            if match:
                params['m'] = float(match.group(1))
                self.logger.info(f"[PartitionSim] è§£æåˆ°æ¨¡ç³Šåº¦å‚æ•°: {params['m']} (åŒ¹é…æ¨¡å¼: {pattern})")
                break
        
        # æå–æ‰°åŠ¨ç‡
        perturb_patterns = [
            r'æ‰°åŠ¨ç‡\s*[ï¼š:]\s*([\d.]+)',
            r'æ‰°åŠ¨ç‡([\d.]+)',
            r'perturb[_\s]*rate\s*[ï¼š:=]\s*([\d.]+)'
        ]
        
        for pattern in perturb_patterns:
            match = re.search(pattern, user_message, re.IGNORECASE)
            if match:
                params['perturb_rate'] = float(match.group(1))
                self.logger.info(f"[PartitionSim] è§£æåˆ°æ‰°åŠ¨ç‡: {params['perturb_rate']} (åŒ¹é…æ¨¡å¼: {pattern})")
                break
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦ç¦»ç¾¤ç‚¹å¤„ç†
        outlier_disable_keywords = [
            'ä¸æ£€æµ‹ç¦»ç¾¤ç‚¹', 'ä¸å¤„ç†ç¦»ç¾¤ç‚¹', 'è·³è¿‡ç¦»ç¾¤ç‚¹', 'ä¸è¦ç¦»ç¾¤ç‚¹æ£€æµ‹',
            'ä¸è¿›è¡Œç¦»ç¾¤ç‚¹æ£€æµ‹', 'ç¦ç”¨ç¦»ç¾¤ç‚¹æ£€æµ‹', 'å…³é—­ç¦»ç¾¤ç‚¹æ£€æµ‹',
            'ä¸å‰”é™¤å¼‚å¸¸ç‚¹', 'ä¸å¤„ç†å¼‚å¸¸ç‚¹', 'è·³è¿‡å¼‚å¸¸ç‚¹', 'ä¸è¦å¼‚å¸¸ç‚¹æ£€æµ‹',
            'no outlier', 'skip outlier', 'disable outlier'
        ]

        outlier_enable_keywords = [
            'æ£€æµ‹ç¦»ç¾¤ç‚¹', 'å¤„ç†ç¦»ç¾¤ç‚¹', 'ç¦»ç¾¤ç‚¹æ£€æµ‹', 'è¿›è¡Œç¦»ç¾¤ç‚¹æ£€æµ‹',
            'å¯ç”¨ç¦»ç¾¤ç‚¹æ£€æµ‹', 'å¼€å¯ç¦»ç¾¤ç‚¹æ£€æµ‹', 'å‰”é™¤å¼‚å¸¸ç‚¹', 'å¤„ç†å¼‚å¸¸ç‚¹',
            'å¼‚å¸¸ç‚¹æ£€æµ‹', 'å¼‚å¸¸ç‚¹å‰”é™¤', 'outlier detection', 'remove outlier'
        ]

        if any(keyword in user_message for keyword in outlier_disable_keywords):
            params['outliers_detection'] = False
            self.logger.info(f"[PartitionSim] è§£æåˆ°ç¦ç”¨ç¦»ç¾¤ç‚¹æ£€æµ‹")
        elif any(keyword in user_message for keyword in outlier_enable_keywords):
            params['outliers_detection'] = True
            self.logger.info(f"[PartitionSim] è§£æåˆ°å¯ç”¨ç¦»ç¾¤ç‚¹æ£€æµ‹")
        
        return {
            'intent': intent_result['intent'],
            'confidence': intent_result['confidence'],
            'params': params
        }
    
    def parse_network(self, inp_file_path: str):
        """è§£æç®¡ç½‘æ–‡ä»¶ï¼Œæå–åŸºæœ¬ä¿¡æ¯"""
        if not WNTR_AVAILABLE:
            return {'error': 'WNTRåº“æœªå®‰è£…'}

        try:
            # æ£€æŸ¥ç¼“å­˜
            if inp_file_path in getattr(self, '_network_cache', {}):
                file_mtime = os.path.getmtime(inp_file_path)
                cached_data = self._network_cache[inp_file_path]
                if cached_data['last_modified'] == file_mtime:
                    self.log_info(f"ä½¿ç”¨ç¼“å­˜çš„ç®¡ç½‘ä¿¡æ¯: {inp_file_path}")
                    return cached_data['network_info']

            self.log_info(f"å¼€å§‹è§£æç®¡ç½‘æ–‡ä»¶: {inp_file_path}")

            # è¯»å–ç®¡ç½‘æ–‡ä»¶
            wn = wntr.network.WaterNetworkModel(inp_file_path)

            # æå–å…³é”®ä¿¡æ¯
            network_info = {
                'nodes': {
                    'junctions': len(wn.junction_name_list),
                    'reservoirs': len(wn.reservoir_name_list),
                    'tanks': len(wn.tank_name_list),
                    'total': len(wn.node_name_list)
                },
                'links': {
                    'pipes': len(wn.pipe_name_list),
                    'pumps': len(wn.pump_name_list),
                    'valves': len(wn.valve_name_list),
                    'total': len(wn.link_name_list)
                },
                'network_stats': {
                    'total_length': float(sum([wn.get_link(pipe).length for pipe in wn.pipe_name_list])) if len(wn.pipe_name_list) > 0 else 0,
                    'simulation_duration': wn.options.time.duration,
                    'hydraulic_timestep': wn.options.time.hydraulic_timestep,
                    'pattern_timestep': wn.options.time.pattern_timestep
                }
            }

            self.log_info(f"ç®¡ç½‘è§£æå®Œæˆ: {network_info['nodes']['total']}ä¸ªèŠ‚ç‚¹, {network_info['links']['total']}ä¸ªç®¡æ®µ")

            # åˆå§‹åŒ–ç¼“å­˜
            if not hasattr(self, '_network_cache'):
                self._network_cache = {}

            # æ›´æ–°ç¼“å­˜
            file_mtime = os.path.getmtime(inp_file_path)
            self._network_cache[inp_file_path] = {
                'network_info': network_info,
                'last_modified': file_mtime
            }

            return network_info

        except Exception as e:
            error_msg = f"è§£æç®¡ç½‘æ–‡ä»¶å¤±è´¥: {e}"
            self.log_error(error_msg)
            return {'error': error_msg}

    def load_network(self, inp_file_path: str):
        """åŠ è½½æ°´ç½‘ç»œæ¨¡å‹"""
        if not WNTR_AVAILABLE:
            return None, {'error': 'WNTRåº“æœªå®‰è£…'}

        try:
            wn = wntr.network.WaterNetworkModel(inp_file_path)
            self.log_info(f"åŠ è½½ç½‘ç»œ: èŠ‚ç‚¹={len(wn.node_name_list)}, "
                         f"éœ€æ°´èŠ‚ç‚¹={len(wn.junction_name_list)}, "
                         f"ç®¡æ®µ={len(wn.link_name_list)}")
            return wn, None
        except Exception as e:
            error_msg = f"åŠ è½½ç½‘ç»œæ–‡ä»¶å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None, {'error': error_msg}
    
    def normalize_matrix(self, S):
        """å¯¹æ•æ„Ÿåº¦çŸ©é˜µè¿›è¡Œæ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–å¤„ç†"""
        # æ ‡å‡†åŒ–ï¼šå‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®
        S_mean = np.mean(S, axis=0)
        S_std = np.std(S, axis=0)
        # æ·»åŠ ä¸€ä¸ªå°çš„é˜ˆå€¼ï¼Œé¿å…é™¤ä»¥0
        epsilon = 1e-10
        S_std = np.where(S_std == 0, epsilon, S_std)
        S_std = (S - S_mean) / S_std
        
        # å½’ä¸€åŒ–ï¼šå°†å€¼æ˜ å°„åˆ°[0,1]åŒºé—´
        S_min = np.min(S_std)
        S_max = np.max(S_std)
        S_n = (S_std - S_min) / (S_max - S_min)
        
        return S_n
    
    def compute_sensitivity_matrix(self, inp_file_path: str, perturb_rate: float):
        """è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µ"""
        if not WNTR_AVAILABLE:
            return None, None, {'error': 'WNTRåº“æœªå®‰è£…'}
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{inp_file_path}_{perturb_rate}"
            if cache_key in self._sensitivity_cache:
                file_mtime = os.path.getmtime(inp_file_path)
                cached_data = self._sensitivity_cache[cache_key]
                if cached_data['last_modified'] == file_mtime:
                    self.log_info(f"ä½¿ç”¨ç¼“å­˜çš„æ•æ„Ÿåº¦çŸ©é˜µ")
                    return cached_data['nodes'], cached_data['demands'], cached_data['matrix']
            
            self.log_info(f"å¼€å§‹è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µï¼Œæ‰°åŠ¨ç‡: {perturb_rate}")
            
            # åŠ è½½åŸºçº¿ç½‘ç»œæ¨¡å‹
            wn0, error = self.load_network(inp_file_path)
            if error:
                return None, None, error
            
            # è¿è¡ŒåŸºçº¿ä»¿çœŸè·å–å‹åŠ›
            sim = wntr.sim.EpanetSimulator(wn0)
            res = sim.run_sim()
            
            # è·å–æ‰€æœ‰èŠ‚ç‚¹å’Œéœ€æ°´èŠ‚ç‚¹åˆ—è¡¨
            node_list = wn0.node_name_list
            demand_nodes = wn0.junction_name_list

            # è®¡ç®—æ€»å®é™…éœ€æ°´é‡
            total_demand = 0
            for name in demand_nodes:
                node_demands = res.node['demand'].loc[:, name]
                total_demand += node_demands.sum()

            # åˆå§‹åŒ–æ•æ„Ÿåº¦çŸ©é˜µ
            S = np.zeros((len(demand_nodes), len(demand_nodes)))
            # é‡æ–°åŠ è½½ç½‘ç»œç”¨äºæ‰°åŠ¨ä»¿çœŸ
            wn, _ = self.load_network(inp_file_path)
            
            # è·å–åŸºçº¿å‹åŠ›
            base_p = res.node['pressure'].loc[:, demand_nodes].values
            # è®¡ç®—å¹³å‡æ‰°åŠ¨é‡
            delta = total_demand * perturb_rate / len(res.node['demand'])

            # å¯¹æ¯ä¸ªéœ€æ°´èŠ‚ç‚¹è¿›è¡Œæ‰°åŠ¨
            for j, name in enumerate(demand_nodes):
                self.log_info(f"å¤„ç†èŠ‚ç‚¹ {j+1}/{len(demand_nodes)}: {name}")
                
                # è·å–è¯¥èŠ‚ç‚¹çš„éœ€æ°´æ—¶é—´åºåˆ—
                ts_list = wn.get_node(name).demand_timeseries_list
                # ä¿å­˜åŸå§‹éœ€æ°´é‡
                orig_values = [d.base_value for d in ts_list]
                
                # å¯¹æ¯ä¸ªæ—¶é—´åºåˆ—è¿›è¡Œæ‰°åŠ¨
                for d in ts_list:
                    if d.base_value > 0:
                        d.base_value = d.base_value + d.base_value * perturb_rate
                    else:
                        d.base_value = d.base_value + delta
                
                # è¿è¡Œæ‰°åŠ¨åä»¿çœŸ
                sim = wntr.sim.EpanetSimulator(wn)
                res_pert = sim.run_sim()
                
                # è·å–æ‰°åŠ¨åçš„å‹åŠ›
                pert_p = res_pert.node['pressure'].loc[:, demand_nodes].values
                
                # è®¡ç®—å½“å‰æ‰°åŠ¨èŠ‚ç‚¹çš„å‹åŠ›å·®
                current_node_p_diff = np.abs(pert_p[:, j] - base_p[:, j])
                
                # è®¡ç®—æ•æ„Ÿåº¦
                with np.errstate(divide='ignore', invalid='ignore'):
                    S[:, j] = np.mean(np.where(current_node_p_diff[:, np.newaxis] != 0,
                                              np.abs(pert_p - base_p) / current_node_p_diff[:, np.newaxis],
                                              0), axis=0)
                
                # æ¢å¤åŸå§‹éœ€æ°´é‡
                for d, orig in zip(ts_list, orig_values):
                    d.base_value = orig
            
            # ç¼“å­˜ç»“æœ
            self._sensitivity_cache[cache_key] = {
                'nodes': node_list,
                'demands': demand_nodes,
                'matrix': S,
                'last_modified': os.path.getmtime(inp_file_path)
            }
            
            self.log_info(f"æ•æ„Ÿåº¦çŸ©é˜µè®¡ç®—å®Œæˆï¼ŒçŸ©é˜µå¤§å°: {S.shape}")
            return node_list, demand_nodes, S
            
        except Exception as e:
            error_msg = f"è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None, None, {'error': error_msg}

    def perform_fcm_clustering(self, S_normalized, params):
        """æ‰§è¡ŒFCMèšç±»"""
        if not SKFUZZY_AVAILABLE:
            return None, None, {'error': 'scikit-fuzzyåº“æœªå®‰è£…'}

        try:
            self.log_info(f"å¼€å§‹FCMèšç±»ï¼Œå‚æ•°: k={params['k']}, m={params['m']}")

            # è®¾ç½®éšæœºç§å­
            np.random.seed(params['seed'])

            # æ‰§è¡ŒFCMèšç±»
            cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
                data=S_normalized.T,      # è¾“å…¥æ•°æ®çŸ©é˜µï¼Œéœ€è¦è½¬ç½®
                c=params['k'],            # èšç±»æ•°é‡
                m=params['m'],            # æ¨¡ç³Šåº¦å‚æ•°
                error=params['error'],    # æ”¶æ•›é˜ˆå€¼
                maxiter=params['maxiter'], # æœ€å¤§è¿­ä»£æ¬¡æ•°
                init=None,                # åˆå§‹èšç±»ä¸­å¿ƒ
                seed=params['seed']       # éšæœºç§å­
            )

            # è·å–åˆå§‹æ ‡ç­¾ï¼ˆä»1å¼€å§‹ï¼‰
            raw_labels = np.argmax(u, axis=0) + 1

            self.log_info(f"FCMèšç±»å®Œæˆï¼Œæ”¶æ•›è¿­ä»£æ¬¡æ•°: {p}, æ¨¡ç³Šåˆ†å‰²ç³»æ•°: {fpc:.4f}")

            return raw_labels, {
                'centers': cntr,
                'membership': u,
                'iterations': p,
                'fpc': fpc,
                'objective_function': jm
            }, None

        except Exception as e:
            error_msg = f"FCMèšç±»å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None, None, {'error': error_msg}

    def check_connectivity(self, node_connections, cluster_nodes):
        """ä½¿ç”¨Warshallç®—æ³•æ£€æŸ¥èŠ‚ç‚¹è¿é€šæ€§"""
        n = len(cluster_nodes)
        adj_matrix = np.zeros((n, n), dtype=int)

        # å¡«å……é‚»æ¥çŸ©é˜µ
        for i, node1 in enumerate(cluster_nodes):
            for j, node2 in enumerate(cluster_nodes):
                if i == j:
                    adj_matrix[i, j] = 1
                else:
                    # æ£€æŸ¥ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦ç›´æ¥ç›¸è¿
                    mask1 = (node_connections[:, 0] == node1) & (node_connections[:, 1] == node2)
                    mask2 = (node_connections[:, 0] == node2) & (node_connections[:, 1] == node1)
                    if np.any(mask1) or np.any(mask2):
                        adj_matrix[i, j] = 1

        # ä½¿ç”¨Warshallç®—æ³•è®¡ç®—ä¼ é€’é—­åŒ…
        for k in range(n):
            for i in range(n):
                for j in range(n):
                    adj_matrix[i, j] = adj_matrix[i, j] or (adj_matrix[i, k] and adj_matrix[k, j])

        return adj_matrix

    def find_connected_components(self, connect_matrix):
        """æ‰¾å‡ºæ‰€æœ‰è¿é€šåˆ†é‡"""
        n = len(connect_matrix)
        visited = np.zeros(n, dtype=bool)
        components = []

        for i in range(n):
            if not visited[i]:
                component = []
                stack = [i]
                while stack:
                    node = stack.pop()
                    if not visited[node]:
                        visited[node] = True
                        component.append(node)
                        neighbors = np.where(connect_matrix[node, :] == 1)[0]
                        for neighbor in neighbors:
                            if not visited[neighbor]:
                                stack.append(neighbor)
                components.append(component)

        return components

    def assign_unassigned_nodes_by_nearest_neighbor(self, wn, nodes, demands, labels, params):
        """å°†æœªåˆ†é…èŠ‚ç‚¹åˆ†é…åˆ°æœ€è¿‘é‚»åˆ†åŒº"""

        # æ‰¾åˆ°æœªåˆ†é…çš„éœ€æ°´èŠ‚ç‚¹
        unassigned_indices = []
        for i, demand_node in enumerate(demands):
            if labels[i] == 0:
                unassigned_indices.append(i)

        if len(unassigned_indices) == 0:
            return labels

        self.log_info(f"å¼€å§‹ä¸º{len(unassigned_indices)}ä¸ªæœªåˆ†é…éœ€æ°´èŠ‚ç‚¹åˆ†é…æœ€è¿‘é‚»åˆ†åŒº")

        # è·å–èŠ‚ç‚¹åæ ‡
        node_coords = {}
        layout = None  # ç”¨äºæ²¡æœ‰åæ ‡çš„èŠ‚ç‚¹

        for node_name in nodes:
            try:
                coord = wn.get_node(node_name).coordinates
                if coord is None or coord == (0, 0):
                    # å¦‚æœæ²¡æœ‰åæ ‡ï¼Œä½¿ç”¨ç½‘ç»œå¸ƒå±€
                    if layout is None:
                        G = wn.to_graph().to_undirected()
                        layout = nx.spring_layout(G, seed=params['seed'])
                    coord = layout.get(node_name, (0, 0))
            except:
                if layout is None:
                    G = wn.to_graph().to_undirected()
                    layout = nx.spring_layout(G, seed=params['seed'])
                coord = layout.get(node_name, (0, 0))
            node_coords[node_name] = coord

        # åˆ›å»ºå·²åˆ†é…èŠ‚ç‚¹çš„åˆ†åŒºä¿¡æ¯
        assigned_nodes_by_partition = {}
        for i, demand_node in enumerate(demands):
            if labels[i] > 0:
                partition = labels[i]
                if partition not in assigned_nodes_by_partition:
                    assigned_nodes_by_partition[partition] = []
                assigned_nodes_by_partition[partition].append((demand_node, node_coords[demand_node]))

        # ä¸ºæ¯ä¸ªæœªåˆ†é…èŠ‚ç‚¹æ‰¾åˆ°æœ€è¿‘çš„åˆ†åŒº
        labels_copy = labels.copy()

        for unassigned_idx in unassigned_indices:
            unassigned_node = demands[unassigned_idx]
            unassigned_coord = node_coords[unassigned_node]

            min_distance = float('inf')
            nearest_partition = 1  # é»˜è®¤åˆ†åŒº

            # éå†æ‰€æœ‰åˆ†åŒºï¼Œæ‰¾åˆ°æœ€è¿‘çš„èŠ‚ç‚¹
            for partition, nodes_in_partition in assigned_nodes_by_partition.items():
                for assigned_node, assigned_coord in nodes_in_partition:
                    # è®¡ç®—æ¬§æ°è·ç¦»
                    distance = np.sqrt((unassigned_coord[0] - assigned_coord[0])**2 +
                                     (unassigned_coord[1] - assigned_coord[1])**2)

                    if distance < min_distance:
                        min_distance = distance
                        nearest_partition = partition

            # åˆ†é…åˆ°æœ€è¿‘çš„åˆ†åŒº
            labels_copy[unassigned_idx] = nearest_partition

            self.log_info(f"èŠ‚ç‚¹{unassigned_node}åˆ†é…åˆ°åˆ†åŒº{nearest_partition}ï¼Œæœ€è¿‘è·ç¦»: {min_distance:.4f}")

            # æ›´æ–°åˆ†åŒºä¿¡æ¯ï¼Œä»¥ä¾¿åç»­èŠ‚ç‚¹å¯ä»¥è€ƒè™‘è¿™ä¸ªæ–°åˆ†é…çš„èŠ‚ç‚¹
            if nearest_partition not in assigned_nodes_by_partition:
                assigned_nodes_by_partition[nearest_partition] = []
            assigned_nodes_by_partition[nearest_partition].append((unassigned_node, unassigned_coord))

        return labels_copy

    def remove_outliers_iteratively(self, wn, nodes, demands, raw_labels, params):
        """è¿­ä»£å¤„ç†ä¸¤ç±»ç¦»ç¾¤ç‚¹"""
        if not params['outliers_detection']:
            self.log_info("è·³è¿‡ç¦»ç¾¤ç‚¹æ£€æµ‹")
            return raw_labels

        self.log_info("å¼€å§‹è¿­ä»£ç¦»ç¾¤ç‚¹æ£€æµ‹")

        # åˆ›å»ºå®Œæ•´çš„æ ‡ç­¾æ•°ç»„
        all_labels = np.zeros(len(nodes))
        for i, node in enumerate(nodes):
            if node in demands:
                idx = demands.index(node)
                all_labels[i] = raw_labels[idx]
            else:
                all_labels[i] = 0

        # è·å–èŠ‚ç‚¹è¿æ¥å…³ç³»
        node_connections = []
        for link in wn.links():
            node1 = link[1].start_node_name
            node2 = link[1].end_node_name
            node_connections.append([nodes.index(node1), nodes.index(node2)])
        node_connections = np.array(node_connections)

        number_iter = 0
        max_iterations = 10

        while number_iter < max_iterations:
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ ‡ç­¾ä¸º0çš„ç‚¹
            zero_count = np.sum(all_labels == 0)
            if zero_count == 0:
                break

            number_iter += 1
            self.log_info(f"ç¦»ç¾¤ç‚¹æ£€æµ‹è¿­ä»£ {number_iter}, å‰©ä½™æœªåˆ†é…èŠ‚ç‚¹: {zero_count}")

            # å¤„ç†ç¬¬ä¸€ç±»ç¦»ç¾¤ç‚¹ï¼šåŸºäºé‚»å±…èŠ‚ç‚¹æ ‡ç­¾çš„ä¸€è‡´æ€§
            for i, node in enumerate(nodes):
                if all_labels[i] != 99999:  # æ’é™¤ç‰¹æ®Šæ ‡è®°
                    # è·å–å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰è¿æ¥èŠ‚ç‚¹
                    connected_nodes = []
                    for conn in node_connections:
                        if conn[0] == i:
                            connected_nodes.append(conn[1])
                        elif conn[1] == i:
                            connected_nodes.append(conn[0])
                    connected_nodes = np.array(connected_nodes)

                    if len(connected_nodes) > 0:
                        # è·å–é‚»å±…èŠ‚ç‚¹çš„å”¯ä¸€æ ‡ç­¾
                        neighbor_labels = np.unique(all_labels[connected_nodes])
                        # è®¡ç®—æ¯ä¸ªæ ‡ç­¾å‡ºç°çš„æ¬¡æ•°
                        label_counts = np.array([np.sum(all_labels[connected_nodes] == label) for label in neighbor_labels])
                        # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„å€¼
                        max_count = np.max(label_counts)
                        # è·å–æ‰€æœ‰è¾¾åˆ°æœ€å¤§æ¬¡æ•°çš„æ ‡ç­¾
                        max_labels = neighbor_labels[label_counts == max_count]
                        # å¦‚æœ0åœ¨æœ€å¤§æ¬¡æ•°æ ‡ç­¾ä¸­ï¼Œä¸”è¿˜æœ‰å…¶ä»–æ ‡ç­¾ï¼Œåˆ™ç§»é™¤0
                        if 0 in max_labels and len(max_labels) > 1:
                            max_labels = max_labels[max_labels != 0]
                        # é€‰æ‹©ç¬¬ä¸€ä¸ªé0çš„æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if len(max_labels) > 0:
                            all_labels[i] = max_labels[0]
                        else:
                            all_labels[i] = 0

            # å¤„ç†ç¬¬äºŒç±»ç¦»ç¾¤ç‚¹ï¼šåŸºäºç©ºé—´è·ç¦»å’Œè¿é€šæ€§
            for cluster in range(1, int(np.max(all_labels)) + 1):
                cluster_nodes = np.where(all_labels == cluster)[0]
                if len(cluster_nodes) <= 1:
                    continue

                # è·å–èŠ‚ç‚¹çš„åæ ‡å’Œé«˜åº¦
                coordinates = []
                elevations = []
                for node_idx in cluster_nodes:
                    node = wn.get_node(nodes[node_idx])
                    try:
                        coord = node.coordinates
                        elev = node.elevation
                    except:
                        coord = (0, 0)
                        elev = 0
                    coordinates.append(coord)
                    elevations.append(elev)

                # æ„å»ºç‰¹å¾çŸ©é˜µ [x, y, elevation]
                features = np.column_stack([coordinates, elevations])

                # è®¡ç®—æ¬§æ°è·ç¦»çŸ©é˜µ
                dist_matrix = np.zeros((len(cluster_nodes), len(cluster_nodes)))
                for i in range(len(cluster_nodes)):
                    for j in range(len(cluster_nodes)):
                        dist_matrix[i, j] = np.linalg.norm(features[i] - features[j])

                # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„KNNè·ç¦»
                knn_distances = []
                for i in range(len(cluster_nodes)):
                    distances = dist_matrix[i, :]
                    distances = distances[distances > 0]
                    k = min(params['k_nearest'], len(distances))
                    if k > 0:
                        knn_dist = np.mean(np.sort(distances)[:k])
                        knn_distances.append(knn_dist)
                    else:
                        knn_distances.append(0)

                knn_distances = np.array(knn_distances)

                # è®¡ç®—ç»Ÿè®¡é‡å¹¶æ ‡è®°ç¦»ç¾¤ç‚¹
                if len(knn_distances) > 0:
                    mean_dist = np.mean(knn_distances)
                    std_dist = np.std(knn_distances)

                    # æ ‡è®°è·ç¦»ç¦»ç¾¤ç‚¹
                    outliers = (knn_distances <= mean_dist - 3 * std_dist) | (knn_distances >= mean_dist + 3 * std_dist)
                    all_labels[cluster_nodes[outliers]] = 0

                # æ£€æŸ¥è¿é€šæ€§
                connect_matrix = self.check_connectivity(node_connections, cluster_nodes)
                components = self.find_connected_components(connect_matrix)

                if len(components) > 1:
                    # é€‰æ‹©æœ€å¤§çš„è¿é€šåˆ†é‡ä½œä¸ºä¸»åŒº
                    main_component = max(components, key=len)
                    # å°†ä¸åœ¨ä¸»åŒºä¸­çš„èŠ‚ç‚¹æ ‡è®°ä¸ºç¦»ç¾¤ç‚¹
                    outliers = np.setdiff1d(np.arange(len(cluster_nodes)), main_component)
                    all_labels[cluster_nodes[outliers]] = 0

        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†åŒºè¢«å®Œå…¨æ¶ˆé™¤ï¼Œå¦‚æœæœ‰åˆ™æ¢å¤æœ€å¤§çš„è¿é€šåˆ†é‡
        original_partitions = set(raw_labels)
        current_partitions = set(all_labels[all_labels > 0])

        lost_partitions = original_partitions - current_partitions
        if lost_partitions:
            self.log_info(f"æ£€æµ‹åˆ°è¢«å®Œå…¨æ¶ˆé™¤çš„åˆ†åŒº: {sorted(lost_partitions)}")

            # å¯¹äºæ¯ä¸ªè¢«æ¶ˆé™¤çš„åˆ†åŒºï¼Œæ¢å¤å…¶æœ€å¤§è¿é€šåˆ†é‡
            for lost_partition in lost_partitions:
                # æ‰¾åˆ°åŸæœ¬å±äºè¿™ä¸ªåˆ†åŒºçš„èŠ‚ç‚¹
                original_nodes = []
                for i, node in enumerate(nodes):
                    if node in demands:
                        idx = demands.index(node)
                        if raw_labels[idx] == lost_partition:
                            original_nodes.append(i)

                if original_nodes:
                    # æ£€æŸ¥è¿™äº›èŠ‚ç‚¹çš„è¿é€šæ€§
                    if len(original_nodes) > 1:
                        # æ„å»ºè¿é€šæ€§çŸ©é˜µ
                        connect_matrix = self.check_connectivity(node_connections, original_nodes)
                        components = self.find_connected_components(connect_matrix)

                        if components:
                            # æ¢å¤æœ€å¤§çš„è¿é€šåˆ†é‡
                            main_component = max(components, key=len)
                            for local_idx in main_component:
                                global_idx = original_nodes[local_idx]
                                all_labels[global_idx] = lost_partition

                            self.log_info(f"æ¢å¤åˆ†åŒº{lost_partition}çš„æœ€å¤§è¿é€šåˆ†é‡: {len(main_component)}ä¸ªèŠ‚ç‚¹")
                    else:
                        # åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç›´æ¥æ¢å¤
                        all_labels[original_nodes[0]] = lost_partition
                        self.log_info(f"æ¢å¤åˆ†åŒº{lost_partition}çš„å•ä¸ªèŠ‚ç‚¹")

        # æ›´æ–°raw_labels
        for i, node in enumerate(nodes):
            if node in demands:
                idx = demands.index(node)
                raw_labels[idx] = all_labels[i]

        # æœ€ç»ˆéªŒè¯åˆ†åŒºæ•°é‡
        final_partitions = len(set(raw_labels[raw_labels > 0]))
        expected_partitions = params['k']

        if final_partitions != expected_partitions:
            self.log_info(f"âš ï¸ åˆ†åŒºæ•°é‡ä¸åŒ¹é…: æœŸæœ›{expected_partitions}ä¸ªï¼Œå®é™…{final_partitions}ä¸ª")
        else:
            self.log_info(f"âœ… åˆ†åŒºæ•°é‡éªŒè¯é€šè¿‡: {final_partitions}ä¸ªåˆ†åŒº")

        # æ£€æŸ¥æœªåˆ†é…èŠ‚ç‚¹æ•°é‡
        unassigned_count = np.sum(raw_labels == 0)
        if unassigned_count > 0:
            self.log_info(f"æ£€æµ‹åˆ°{unassigned_count}ä¸ªæœªåˆ†é…èŠ‚ç‚¹ï¼Œå¼€å§‹æœ€è¿‘é‚»åˆ†é…")
            # è¿›è¡Œæœ€è¿‘é‚»åˆ†é…
            final_labels = self.assign_unassigned_nodes_by_nearest_neighbor(wn, nodes, demands, raw_labels, params)

            # éªŒè¯æœ€è¿‘é‚»åˆ†é…ç»“æœ
            final_unassigned = np.sum(final_labels == 0)
            if final_unassigned == 0:
                self.log_info("âœ… æ‰€æœ‰èŠ‚ç‚¹å·²é€šè¿‡æœ€è¿‘é‚»åˆ†é…æˆåŠŸåˆ†é…åˆ°åˆ†åŒº")
            else:
                self.log_info(f"âš ï¸ æœ€è¿‘é‚»åˆ†é…åä»æœ‰{final_unassigned}ä¸ªèŠ‚ç‚¹æœªåˆ†é…")

            self.log_info(f"ç¦»ç¾¤ç‚¹æ£€æµ‹å’Œæœ€è¿‘é‚»åˆ†é…å®Œæˆï¼Œè¿­ä»£æ¬¡æ•°: {number_iter}")
            return final_labels
        else:
            self.log_info("âœ… æ‰€æœ‰èŠ‚ç‚¹å·²åˆ†é…ï¼Œæ— éœ€æœ€è¿‘é‚»åˆ†é…")
            self.log_info(f"ç¦»ç¾¤ç‚¹æ£€æµ‹å®Œæˆï¼Œè¿­ä»£æ¬¡æ•°: {number_iter}")
            return raw_labels

    def identify_boundary_pipes(self, wn, nodes, demands, labels):
        """è¯†åˆ«è¾¹ç•Œç®¡é“ - ç®¡é“ä¸¤ç«¯èŠ‚ç‚¹å±äºä¸åŒåˆ†åŒº"""
        try:
            # åˆ›å»ºå®Œæ•´çš„æ ‡ç­¾æ•°ç»„
            all_labels = np.zeros(len(nodes))
            for i, node in enumerate(nodes):
                if node in demands:
                    idx = demands.index(node)
                    all_labels[i] = labels[idx]

            # åˆ›å»ºèŠ‚ç‚¹åˆ°ç´¢å¼•çš„æ˜ å°„
            node_to_idx = {node: i for i, node in enumerate(nodes)}

            boundary_pipes = []
            non_boundary_pipes = []

            # éå†æ‰€æœ‰ç®¡æ®µ
            for link in wn.links():
                link_obj = link[1]
                start_node = link_obj.start_node_name
                end_node = link_obj.end_node_name

                # è·å–ä¸¤ç«¯èŠ‚ç‚¹çš„åˆ†åŒºæ ‡ç­¾
                if start_node in node_to_idx and end_node in node_to_idx:
                    start_idx = node_to_idx[start_node]
                    end_idx = node_to_idx[end_node]
                    start_label = all_labels[start_idx]
                    end_label = all_labels[end_idx]

                    # åˆ¤æ–­æ˜¯å¦ä¸ºè¾¹ç•Œç®¡é“
                    if start_label != end_label and start_label > 0 and end_label > 0:
                        boundary_pipes.append((start_node, end_node))
                    else:
                        non_boundary_pipes.append((start_node, end_node))

            self.log_info(f"è¯†åˆ«åˆ°{len(boundary_pipes)}æ¡è¾¹ç•Œç®¡é“ï¼Œ{len(non_boundary_pipes)}æ¡éè¾¹ç•Œç®¡é“")
            return boundary_pipes, non_boundary_pipes

        except Exception as e:
            error_msg = f"è¯†åˆ«è¾¹ç•Œç®¡é“å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return [], []

    def generate_partition_visualization(self, wn, nodes, demands, labels, params, save_path=None):
        """ç”Ÿæˆåˆ†åŒºå¯è§†åŒ–å›¾"""
        try:
            # è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡ä¹±ç 
            plt.rcParams['font.family'] = 'DejaVu Sans'
            plt.rcParams['axes.unicode_minus'] = False

            # åˆ›å»ºæ— å‘å›¾
            G = wn.to_graph().to_undirected()

            # å‡†å¤‡èŠ‚ç‚¹ä½ç½®
            pos = {}
            layout = None
            for n in G.nodes():
                try:
                    coord = wn.get_node(n).coordinates
                except:
                    if layout is None:
                        layout = nx.spring_layout(G, seed=params['seed'])
                    coord = layout[n]
                pos[n] = coord

            # åˆ›å»ºå®Œæ•´çš„æ ‡ç­¾æ•°ç»„
            all_labels = np.zeros(len(nodes))
            for i, node in enumerate(nodes):
                if node in demands:
                    idx = demands.index(node)
                    all_labels[i] = labels[idx]

            # ç»˜åˆ¶ç½‘ç»œåˆ†åŒº
            plt.figure(figsize=(12, 10))

            # ç»˜åˆ¶è¾¹
            nx.draw_networkx_edges(G, pos=pos, alpha=0.9, width=0.8)

            # ç»˜åˆ¶èŠ‚ç‚¹
            scatter = nx.draw_networkx_nodes(
                G, pos=pos,
                nodelist=nodes,
                node_color=all_labels,
                cmap=plt.get_cmap("tab10", params['k']+1),
                vmin=0, vmax=params['k'],
                node_size=30
            )

            # æ·»åŠ å›¾ä¾‹ï¼ˆä½¿ç”¨è‹±æ–‡ï¼‰
            legend_labels = ['Unassigned'] + [f'Partition {i}' for i in range(1, params['k']+1)]
            plt.legend(scatter.legend_elements()[0], legend_labels,
                      title="Node Type",
                      loc='upper right',
                      bbox_to_anchor=(1, 1))

            plt.title(f"Water Network Partitioning Results (K={params['k']}, Fuzziness={params['m']})")
            plt.axis("off")

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                self.log_info(f"åˆ†åŒºå›¾å·²ä¿å­˜åˆ°: {save_path}")

            return save_path

        except Exception as e:
            error_msg = f"ç”Ÿæˆå¯è§†åŒ–å›¾å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None

    def generate_boundary_pipes_visualization(self, wn, nodes, demands, labels, params, save_path=None):
        """ç”Ÿæˆè¾¹ç•Œç®¡é“å¯è§†åŒ–å›¾ - çªå‡ºæ˜¾ç¤ºè¾¹ç•Œç®¡é“"""
        try:
            # è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡å­—ä½“
            plt.rcParams['font.family'] = 'DejaVu Sans'
            plt.rcParams['axes.unicode_minus'] = False

            # åŠ è½½ç½‘ç»œæ¨¡å‹
            wn = wntr.network.WaterNetworkModel(wn.name) if hasattr(wn, 'name') else wn
            G = wn.to_graph().to_undirected()

            # å‡†å¤‡èŠ‚ç‚¹ä½ç½®
            pos = {}
            layout = None

            for node in G.nodes():
                try:
                    coord = wn.get_node(node).coordinates
                    if coord is None or coord == (0, 0):
                        if layout is None:
                            layout = nx.spring_layout(G, seed=params['seed'])
                        coord = layout.get(node, (0, 0))
                except:
                    if layout is None:
                        layout = nx.spring_layout(G, seed=params['seed'])
                    coord = layout.get(node, (0, 0))
                pos[node] = coord

            # åˆ›å»ºå®Œæ•´çš„æ ‡ç­¾æ•°ç»„
            all_labels = np.zeros(len(nodes))
            for i, node in enumerate(nodes):
                if node in demands:
                    idx = demands.index(node)
                    all_labels[i] = labels[idx]

            # è¯†åˆ«è¾¹ç•Œç®¡é“
            boundary_pipes, non_boundary_pipes = self.identify_boundary_pipes(wn, nodes, demands, labels)
            boundary_count = len(boundary_pipes)

            # åˆ›å»ºå›¾å½¢ - ä½¿ç”¨ä¸sensor_placement.pyç›¸åŒçš„é£æ ¼
            plt.figure(figsize=(15, 12))

            # ç»˜åˆ¶éè¾¹ç•Œç®¡é“ï¼ˆæ·¡åŒ–ä½†æ›´æ·±ï¼‰
            nx.draw_networkx_edges(G, pos=pos, edgelist=non_boundary_pipes,
                                  alpha=0.4, width=0.5, edge_color='gray')

            # ç»˜åˆ¶è¾¹ç•Œç®¡é“ï¼ˆçº¢è‰²ï¼ŒåŠ ç²—ï¼‰
            nx.draw_networkx_edges(G, pos=pos, edgelist=boundary_pipes,
                                  alpha=0.9, width=2.5, edge_color='red')

            # ç»˜åˆ¶æ™®é€šèŠ‚ç‚¹ï¼ˆæ·¡åŒ–ï¼‰
            all_nodes = list(G.nodes())
            nx.draw_networkx_nodes(G, pos=pos, nodelist=all_nodes,
                                 node_color='lightblue', node_size=20, alpha=0.5)

            # ç»˜åˆ¶åˆ†åŒºèŠ‚ç‚¹ï¼ˆæŒ‰åˆ†åŒºç€è‰²ï¼Œè¦†ç›–æ™®é€šèŠ‚ç‚¹ï¼‰
            colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray', 'cyan', 'magenta']
            for partition_id in range(1, params['k'] + 1):
                partition_nodes = [nodes[i] for i in range(len(nodes)) if all_labels[i] == partition_id]
                if partition_nodes:
                    color = colors[partition_id % len(colors)]
                    nx.draw_networkx_nodes(G, pos=pos, nodelist=partition_nodes,
                                         node_color=color, node_size=30, alpha=0.7,
                                         label=f'Partition {partition_id}')

            # æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.title(f'Water Network Boundary Pipes Analysis\n'
                     f'Total Boundary Pipes: {boundary_count}, '
                     f'Partitions: {params["k"]}, '
                     f'Fuzziness: {params["m"]}')
            plt.axis('off')

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                plt.close()
                self.log_info(f"è¾¹ç•Œç®¡é“å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {save_path}")

            return save_path, boundary_count

        except Exception as e:
            error_msg = f"ç”Ÿæˆè¾¹ç•Œç®¡é“å¯è§†åŒ–å›¾å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None, 0

    def save_partition_results(self, nodes, demands, labels, params, clustering_info, conversation_id):
        """ä¿å­˜åˆ†åŒºç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"partition_results_{conversation_id[:8]}_{timestamp}.csv"
            filepath = os.path.join(self.downloads_folder, filename)

            # å‡†å¤‡æ•°æ®
            results_data = []

            # æ·»åŠ éœ€æ°´èŠ‚ç‚¹çš„åˆ†åŒºä¿¡æ¯
            for i, node_id in enumerate(demands):
                results_data.append({
                    'èŠ‚ç‚¹ID': node_id,
                    'èŠ‚ç‚¹ç±»å‹': 'éœ€æ°´èŠ‚ç‚¹',
                    'åˆ†åŒºç¼–å·': int(labels[i]),
                    'åˆ†åŒºåç§°': f'åˆ†åŒº{int(labels[i])}' if labels[i] > 0 else 'æœªåˆ†é…'
                })

            # æ·»åŠ ééœ€æ°´èŠ‚ç‚¹ä¿¡æ¯
            for node_id in nodes:
                if node_id not in demands:
                    results_data.append({
                        'èŠ‚ç‚¹ID': node_id,
                        'èŠ‚ç‚¹ç±»å‹': 'ééœ€æ°´èŠ‚ç‚¹',
                        'åˆ†åŒºç¼–å·': 0,
                        'åˆ†åŒºåç§°': 'ééœ€æ°´èŠ‚ç‚¹'
                    })

            # åˆ›å»ºDataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(results_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            partition_stats = {}
            for i in range(1, params['k'] + 1):
                count = np.sum(labels == i)
                partition_stats[f'åˆ†åŒº{i}'] = count

            unassigned_count = np.sum(labels == 0)
            if unassigned_count > 0:
                partition_stats['æœªåˆ†é…'] = unassigned_count

            file_size = os.path.getsize(filepath)

            self.log_info(f"åˆ†åŒºç»“æœå·²ä¿å­˜åˆ°: {filepath}")

            return {
                'success': True,
                'filename': filename,
                'filepath': filepath,
                'file_size': file_size,
                'records_count': len(results_data),
                'partition_stats': partition_stats,
                'download_url': f'/download/{filename}'
            }

        except Exception as e:
            error_msg = f"ä¿å­˜åˆ†åŒºç»“æœå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {
                'success': False,
                'error': error_msg
            }

    def build_partition_prompt(self, network_info: dict, partition_result: dict, user_message: str, csv_info: dict = None):
        """æ„å»ºåŒ…å«ç½‘ç»œä¿¡æ¯å’Œåˆ†åŒºç»“æœçš„ä¸“ä¸šåˆ†æprompt"""
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»™æ°´ç®¡ç½‘åˆ†åŒºåˆ†æä¸“å®¶ã€‚ç°åœ¨éœ€è¦åˆ†æä»¥ä¸‹ç®¡ç½‘ç³»ç»Ÿçš„åˆ†åŒºç»“æœï¼š

ç®¡ç½‘åŸºæœ¬ä¿¡æ¯ï¼š
- èŠ‚ç‚¹æ€»æ•°ï¼š{network_info['nodes']['total']} (èŠ‚ç‚¹: {network_info['nodes']['junctions']}, æ°´åº“: {network_info['nodes']['reservoirs']}, æ°´å¡”: {network_info['nodes']['tanks']})
- ç®¡æ®µæ€»æ•°ï¼š{network_info['links']['total']} (ç®¡é“: {network_info['links']['pipes']}, æ°´æ³µ: {network_info['links']['pumps']}, é˜€é—¨: {network_info['links']['valves']})
- ç®¡ç½‘æ€»é•¿åº¦ï¼š{network_info['network_stats']['total_length']:.2f} ç±³
- ä»¿çœŸæ—¶é•¿ï¼š{network_info['network_stats']['simulation_duration']} ç§’

âœ… ç®¡ç½‘åˆ†åŒºåˆ†æå·²æˆåŠŸå®Œæˆï¼

åˆ†åŒºåˆ†æç»“æœï¼š
{partition_result['response']}

åˆ†åŒºæŠ€æœ¯å‚æ•°ï¼š
- FCMèšç±»ç®—æ³•ï¼Œæ¨¡ç³Šåº¦å‚æ•° m = {partition_result['parameters']['m']}
- æ•æ„Ÿåº¦çŸ©é˜µæ‰°åŠ¨ç‡ï¼š{partition_result['parameters']['perturb_rate']}
- æ”¶æ•›é˜ˆå€¼ï¼š{partition_result['parameters']['error']}
- æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼š{partition_result['parameters']['maxiter']}
- ç¦»ç¾¤ç‚¹æ£€æµ‹ï¼š{'å·²å¯ç”¨' if partition_result['parameters']['outliers_detection'] else 'æœªå¯ç”¨'}

åˆ†åŒºè´¨é‡æŒ‡æ ‡ï¼š
- æ¨¡ç³Šåˆ†å‰²ç³»æ•° (FPC)ï¼š{partition_result['partition_info']['fpc']:.4f}
- èšç±»æ”¶æ•›è¿­ä»£æ¬¡æ•°ï¼š{partition_result['partition_info']['iterations']}
"""

        if csv_info and csv_info['success']:
            prompt += f"""
ğŸ“Š è¯¦ç»†åˆ†åŒºæ•°æ®å·²ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼š{csv_info['filename']}
æ–‡ä»¶å¤§å°ï¼š{csv_info['file_size']} å­—èŠ‚ï¼Œå…± {csv_info['records_count']} æ¡è®°å½•
"""

        prompt += f"""
ç”¨æˆ·é—®é¢˜ï¼š{user_message}

è¯·åŸºäºç®¡ç½‘åŸºæœ¬ä¿¡æ¯å’Œåˆ†åŒºåˆ†æç»“æœï¼Œæä¾›ä¸“ä¸šçš„åˆ†æå’Œå»ºè®®ï¼ŒåŒ…æ‹¬ï¼š
1. åˆ†åŒºç»“æœçš„åˆç†æ€§è¯„ä¼°
2. åˆ†åŒºè´¨é‡çš„æŠ€æœ¯åˆ†æï¼ˆåŸºäºFPCå€¼å’Œåˆ†åŒºåˆ†å¸ƒï¼‰
3. å¯èƒ½çš„ä¼˜åŒ–å»ºè®®
4. å·¥ç¨‹åº”ç”¨ä»·å€¼å’Œæ„ä¹‰
5. å¦‚æœ‰å¿…è¦ï¼Œå»ºè®®è¿›ä¸€æ­¥çš„åˆ†ææ–¹å‘

åŒæ—¶å‘ŠçŸ¥ç”¨æˆ·å¯ä»¥ä¸‹è½½è¯¦ç»†çš„åˆ†åŒºæ•°æ®è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚

è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š

ç¥å¥½ï¼Œ

Tianwei Mu
Guangzhou Institute of Industrial Intelligence
"""
        return prompt

    def process(self, inp_file_path: str, user_message: str, conversation_id: str):
        """ä¸»å¤„ç†å‡½æ•°"""
        try:
            self.log_info(f"å¼€å§‹å¤„ç†ç®¡ç½‘åˆ†åŒºè¯·æ±‚: {user_message}")

            # Step 1: è§£æç®¡ç½‘æ–‡ä»¶ï¼Œè·å–åŸºæœ¬ä¿¡æ¯
            network_info = self.parse_network(inp_file_path)
            if 'error' in network_info:
                return {
                    'success': False,
                    'response': f"ç®¡ç½‘æ–‡ä»¶è§£æå¤±è´¥: {network_info['error']}",
                    'intent': 'partition_analysis',
                    'confidence': 0.0
                }

            # Step 2: è§£æç”¨æˆ·æ„å›¾å’Œå‚æ•°
            intent_result = self.parse_user_intent(user_message)
            params = intent_result['params']

            self.log_info(f"è§£æå‚æ•°: {params}")

            # Step 3: åŠ è½½ç½‘ç»œæ¨¡å‹
            wn, error = self.load_network(inp_file_path)
            if error:
                return {
                    'success': False,
                    'response': f"åŠ è½½ç½‘ç»œæ–‡ä»¶å¤±è´¥: {error['error']}",
                    'intent': intent_result['intent'],
                    'confidence': intent_result['confidence']
                }

            # è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µ
            nodes, demands, S = self.compute_sensitivity_matrix(inp_file_path, params['perturb_rate'])
            if isinstance(S, dict) and 'error' in S:
                return {
                    'success': False,
                    'response': f"è®¡ç®—æ•æ„Ÿåº¦çŸ©é˜µå¤±è´¥: {S['error']}",
                    'intent': intent_result['intent'],
                    'confidence': intent_result['confidence']
                }

            # æ ‡å‡†åŒ–æ•æ„Ÿåº¦çŸ©é˜µ
            S_normalized = self.normalize_matrix(S)

            # æ‰§è¡ŒFCMèšç±»
            raw_labels, clustering_info, error = self.perform_fcm_clustering(S_normalized, params)
            if error:
                return {
                    'success': False,
                    'response': f"FCMèšç±»å¤±è´¥: {error['error']}",
                    'intent': intent_result['intent'],
                    'confidence': intent_result['confidence']
                }

            # ç¦»ç¾¤ç‚¹æ£€æµ‹å’Œå¤„ç†
            refined_labels = self.remove_outliers_iteratively(wn, nodes, demands, raw_labels, params)

            # ç”Ÿæˆå¯è§†åŒ–å›¾
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            viz_filename = f"partition_viz_{conversation_id[:8]}_{timestamp}.png"
            viz_path = os.path.join(self.downloads_folder, viz_filename)

            viz_result = self.generate_partition_visualization(wn, nodes, demands, refined_labels, params, viz_path)

            # ç”Ÿæˆè¾¹ç•Œç®¡é“å¯è§†åŒ–å›¾
            boundary_viz_filename = f"boundary_pipes_viz_{conversation_id[:8]}_{timestamp}.png"
            boundary_viz_path = os.path.join(self.downloads_folder, boundary_viz_filename)
            boundary_viz_result, boundary_pipe_count = self.generate_boundary_pipes_visualization(
                wn, nodes, demands, refined_labels, params, boundary_viz_path
            )

            # è·å–è¾¹ç•Œç®¡é“ä¿¡æ¯ç”¨äºæŠ¥å‘Š
            boundary_pipes, non_boundary_pipes = self.identify_boundary_pipes(wn, nodes, demands, refined_labels)

            # ä¿å­˜åˆ†åŒºç»“æœ
            save_result = self.save_partition_results(nodes, demands, refined_labels, params, clustering_info, conversation_id)

            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            total_nodes = len(nodes)
            demand_nodes_count = len(demands)
            partition_distribution = {}
            for i in range(1, params['k'] + 1):
                count = int(np.sum(refined_labels == i))  # è½¬æ¢ä¸ºPython int
                partition_distribution[i] = count

            unassigned_count = int(np.sum(refined_labels == 0))  # è½¬æ¢ä¸ºPython int

            response_text = f"""
ç®¡ç½‘åˆ†åŒºåˆ†æå®Œæˆï¼

ğŸ“Š **åˆ†åŒºæ¦‚å†µ**
- æ€»èŠ‚ç‚¹æ•°: {total_nodes}
- éœ€æ°´èŠ‚ç‚¹æ•°: {demand_nodes_count}
- åˆ†åŒºæ•°é‡: {params['k']}
- æ¨¡ç³Šåº¦å‚æ•°: {params['m']}
- æ‰°åŠ¨ç‡: {params['perturb_rate']}

ğŸ“ˆ **èšç±»è´¨é‡**
- æ¨¡ç³Šåˆ†å‰²ç³»æ•° (FPC): {clustering_info['fpc']:.4f}
- æ”¶æ•›è¿­ä»£æ¬¡æ•°: {clustering_info['iterations']}

ğŸ¯ **åˆ†åŒºåˆ†å¸ƒ**
"""
            for i in range(1, params['k'] + 1):
                count = partition_distribution[i]
                percentage = (count / demand_nodes_count) * 100
                response_text += f"- åˆ†åŒº{i}: {count}ä¸ªèŠ‚ç‚¹ ({percentage:.1f}%)\n"

            if unassigned_count > 0:
                percentage = (unassigned_count / demand_nodes_count) * 100
                response_text += f"- æœªåˆ†é…: {unassigned_count}ä¸ªèŠ‚ç‚¹ ({percentage:.1f}%)\n"

            if params['outliers_detection']:
                response_text += f"\nâœ… å·²è¿›è¡Œç¦»ç¾¤ç‚¹æ£€æµ‹å’Œå¤„ç†"
            else:
                response_text += f"\nâš ï¸ æœªè¿›è¡Œç¦»ç¾¤ç‚¹æ£€æµ‹"

            # æ·»åŠ è¾¹ç•Œç®¡é“ä¿¡æ¯
            response_text += f"\n\nğŸ”´ **è¾¹ç•Œç®¡é“åˆ†æ**\n"
            response_text += f"- è¾¹ç•Œç®¡é“æ€»æ•°: {boundary_pipe_count}\n"
            response_text += f"- è¾¹ç•Œç®¡é“å æ¯”: {(boundary_pipe_count / (boundary_pipe_count + len(non_boundary_pipes)) * 100):.1f}% (å…±{boundary_pipe_count + len(non_boundary_pipes)}æ¡ç®¡é“)"

            # æ„å»ºä¸“ä¸šåˆ†æprompt
            prompt = self.build_partition_prompt(
                network_info,
                {
                    'response': response_text,
                    'partition_info': {
                        'total_nodes': total_nodes,
                        'demand_nodes': demand_nodes_count,
                        'k': params['k'],
                        'partition_distribution': partition_distribution,
                        'unassigned_count': unassigned_count,
                        'fpc': float(clustering_info['fpc']),  # è½¬æ¢ä¸ºPython float
                        'iterations': int(clustering_info['iterations'])  # è½¬æ¢ä¸ºPython int
                    },
                    'parameters': params
                },
                user_message,
                save_result if save_result['success'] else None
            )

            result = {
                'success': True,
                'response': response_text,
                'prompt': prompt,  # æ·»åŠ ä¸“ä¸špromptç”¨äºGPTåˆ†æ
                'intent': intent_result['intent'],
                'confidence': intent_result['confidence'],
                'partition_info': {
                    'total_nodes': total_nodes,
                    'demand_nodes': demand_nodes_count,
                    'k': params['k'],
                    'partition_distribution': partition_distribution,
                    'unassigned_count': unassigned_count,
                    'fpc': float(clustering_info['fpc']),  # è½¬æ¢ä¸ºPython float
                    'iterations': int(clustering_info['iterations'])  # è½¬æ¢ä¸ºPython int
                },
                'parameters': params,
                'network_info': network_info  # æ·»åŠ ç½‘ç»œä¿¡æ¯
            }

            # æ·»åŠ æ–‡ä»¶ä¸‹è½½ä¿¡æ¯
            if save_result['success']:
                result['csv_info'] = save_result

            if viz_result:
                result['visualization'] = {
                    'filename': viz_filename,
                    'path': viz_path
                }

            if boundary_viz_result:
                result['boundary_visualization'] = {
                    'filename': boundary_viz_filename,
                    'path': boundary_viz_result,
                    'boundary_pipe_count': boundary_pipe_count
                }

            return result

        except Exception as e:
            error_msg = f"å¤„ç†ç®¡ç½‘åˆ†åŒºè¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
            self.log_error(error_msg)
            return {
                'success': False,
                'response': error_msg,
                'intent': 'partition_analysis',
                'confidence': 0.0
            }
