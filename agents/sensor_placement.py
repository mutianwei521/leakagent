"""
SensorPlacement ä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®æ™ºèƒ½ä½“
è´Ÿè´£åŸºäºç®¡ç½‘åˆ†åŒºç»“æœè¿›è¡Œä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®ï¼Œè€ƒè™‘éŸ§æ€§å’Œæ£€æµ‹æ•ˆæœ
"""
import os
import re
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from itertools import combinations
from .base_agent import BaseAgent
from .partition_sim import PartitionSim

try:
    import wntr
    WNTR_AVAILABLE = True
except ImportError:
    WNTR_AVAILABLE = False

class SensorPlacement(BaseAgent):
    """ä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®æ™ºèƒ½ä½“"""
    
    def __init__(self):
        super().__init__("SensorPlacement")
        
        if not WNTR_AVAILABLE:
            self.log_error("WNTRåº“æœªå®‰è£…ï¼Œä¼ æ„Ÿå™¨å¸ƒç½®åŠŸèƒ½ä¸å¯ç”¨")
        
        self.partition_sim = PartitionSim()
        self.downloads_folder = 'downloads'
        os.makedirs(self.downloads_folder, exist_ok=True)
        
        # é»˜è®¤å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è®¡ç®—é‡ï¼‰
        self.default_params = {
            'demand_ratios': [0.20],  # å‡å°‘æ‰°åŠ¨æ¯”ä¾‹ï¼Œåªä½¿ç”¨ä¸€ä¸ª
            'sensitivity_threshold': 0.5,  # æ•æ„Ÿåº¦é˜ˆå€¼
            'max_failure_rate': 0.8,  # æœ€å¤§æ•…éšœç‡ï¼ˆå…è®¸æ›´å¤šä¼ æ„Ÿå™¨æ•…éšœï¼‰
            'resilience_weight': 0.4,  # éŸ§æ€§æƒé‡
            'coverage_weight': 0.6,  # è¦†ç›–ç‡æƒé‡
            'target_coverage': 0.95,  # ç›®æ ‡è¦†ç›–ç‡
            'min_sensor_ratio': 0.04,  # æœ€å°ä¼ æ„Ÿå™¨æ¯”ä¾‹ï¼ˆèŠ‚ç‚¹æ•°çš„4%ï¼‰
            'max_sensor_ratio': 0.15,  # æœ€å¤§ä¼ æ„Ÿå™¨æ¯”ä¾‹ï¼ˆèŠ‚ç‚¹æ•°çš„15%ï¼‰
            'custom_thresholds': {},  # è‡ªå®šä¹‰ä¼ æ„Ÿå™¨é˜ˆå€¼ {sensor_node: threshold}
            'enable_custom_thresholds': False  # æ˜¯å¦å¯ç”¨è‡ªå®šä¹‰é˜ˆå€¼
        }
    
    def load_partition_results(self, csv_file_path):
        """ä»CSVæ–‡ä»¶åŠ è½½åˆ†åŒºç»“æœ"""
        try:
            self.log_info(f"åŠ è½½åˆ†åŒºç»“æœ: {csv_file_path}")
            
            df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
            
            # æå–éœ€æ°´èŠ‚ç‚¹çš„åˆ†åŒºä¿¡æ¯
            demand_nodes_df = df[df['èŠ‚ç‚¹ç±»å‹'] == 'éœ€æ°´èŠ‚ç‚¹']
            
            partitions = {}
            for _, row in demand_nodes_df.iterrows():
                partition_id = row['åˆ†åŒºç¼–å·']
                node_id = row['èŠ‚ç‚¹ID']
                
                if partition_id > 0:  # æ’é™¤æœªåˆ†é…èŠ‚ç‚¹
                    if partition_id not in partitions:
                        partitions[partition_id] = []
                    partitions[partition_id].append(node_id)
            
            self.log_info(f"æˆåŠŸåŠ è½½åˆ†åŒºç»“æœ: {len(partitions)}ä¸ªåˆ†åŒº")
            for partition_id, nodes in partitions.items():
                self.log_info(f"  åˆ†åŒº{partition_id}: {len(nodes)}ä¸ªèŠ‚ç‚¹")
            
            return partitions
            
        except Exception as e:
            error_msg = f"åŠ è½½åˆ†åŒºç»“æœå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None
    
    def compute_pressure_sensitivity_matrix(self, inp_file_path, partitions, demand_ratios):
        """è®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µ"""
        try:
            self.log_info(f"å¼€å§‹è®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µï¼Œæ‰°åŠ¨æ¯”ä¾‹: {demand_ratios}")

            # åŠ è½½ç½‘ç»œæ¨¡å‹
            wn = wntr.network.WaterNetworkModel(inp_file_path)

            # è·å–æ‰€æœ‰éœ€æ°´èŠ‚ç‚¹
            all_demand_nodes = []
            for nodes in partitions.values():
                all_demand_nodes.extend(nodes)

            self.log_info(f"éœ€æ°´èŠ‚ç‚¹æ€»æ•°: {len(all_demand_nodes)}")

            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨äºç½‘ç»œä¸­
            valid_demand_nodes = []
            for node in all_demand_nodes:
                if node in wn.node_name_list:
                    valid_demand_nodes.append(node)
                else:
                    self.log_info(f"èŠ‚ç‚¹ {node} ä¸å­˜åœ¨äºç½‘ç»œä¸­ï¼Œè·³è¿‡")

            if not valid_demand_nodes:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„éœ€æ°´èŠ‚ç‚¹")

            self.log_info(f"æœ‰æ•ˆéœ€æ°´èŠ‚ç‚¹æ•°: {len(valid_demand_nodes)}")

            # è¿è¡ŒåŸºå‡†ä»¿çœŸ
            self.log_info("è¿è¡ŒåŸºå‡†ä»¿çœŸ...")
            sim = wntr.sim.EpanetSimulator(wn)
            base_results = sim.run_sim()
            base_pressure = base_results.node['pressure'].loc[:, valid_demand_nodes].values

            # è®¡ç®—æ€»å®é™…éœ€æ°´é‡ï¼ˆå‚è€ƒcluster_simple.pyçš„åšæ³•ï¼‰
            self.log_info("è®¡ç®—æ€»éœ€æ°´é‡...")
            total_demand = 0
            for name in valid_demand_nodes:
                # è·å–è¯¥èŠ‚ç‚¹åœ¨æ‰€æœ‰æ—¶é—´æ­¥çš„å®é™…éœ€æ°´é‡
                node_demands = base_results.node['demand'].loc[:, name]
                # ç´¯åŠ è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰æ—¶é—´æ­¥éœ€æ°´é‡
                total_demand += node_demands.sum()

            self.log_info(f"æ€»éœ€æ°´é‡: {total_demand:.4f}")

            # åˆå§‹åŒ–æ•æ„Ÿåº¦çŸ©é˜µ
            n_nodes = len(valid_demand_nodes)
            sensitivity_matrix = np.zeros((n_nodes, n_nodes))

            # å¯¹æ¯ä¸ªéœ€æ°´èŠ‚ç‚¹è¿›è¡Œæ‰°åŠ¨
            for i, perturb_node in enumerate(valid_demand_nodes):
                if i % 50 == 0:  # æ¯50ä¸ªèŠ‚ç‚¹è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    self.log_info(f"å¤„ç†èŠ‚ç‚¹ {i+1}/{n_nodes}: {perturb_node} ({(i+1)/n_nodes*100:.1f}%)")

                # ä¿å­˜åŸå§‹éœ€æ°´é‡
                original_demands = {}
                node = wn.get_node(perturb_node)
                for j, ts in enumerate(node.demand_timeseries_list):
                    original_demands[j] = ts.base_value

                # å¯¹æ¯ä¸ªæ‰°åŠ¨æ¯”ä¾‹è¿›è¡Œè®¡ç®—
                ratio_sensitivities = []

                for ratio in demand_ratios:
                    # è®¡ç®—è¯¥æ¯”ä¾‹ä¸‹çš„å¹³å‡æ‰°åŠ¨é‡ï¼ˆå‚è€ƒcluster_simple.pyï¼‰
                    delta = total_demand * ratio / len(base_results.node['demand'])

                    # è®¾ç½®æ‰°åŠ¨éœ€æ°´é‡
                    node = wn.get_node(perturb_node)
                    for j, ts in enumerate(node.demand_timeseries_list):
                        if original_demands[j] > 0:
                            # æŒ‰æ¯”ä¾‹æ‰°åŠ¨
                            ts.base_value = original_demands[j] + original_demands[j] * ratio
                        else:
                            # ç»å¯¹é‡æ‰°åŠ¨ï¼ˆä½¿ç”¨åŸºäºæ€»éœ€æ°´é‡çš„deltaï¼‰
                            ts.base_value = original_demands[j] + delta

                    # è¿è¡Œæ‰°åŠ¨ä»¿çœŸ
                    sim = wntr.sim.EpanetSimulator(wn)
                    perturb_results = sim.run_sim()
                    perturb_pressure = perturb_results.node['pressure'].loc[:, valid_demand_nodes].values

                    # è®¡ç®—å‹åŠ›å·®
                    pressure_diff = np.abs(perturb_pressure - base_pressure)

                    # è®¡ç®—æ—¶é—´å¹³å‡
                    avg_pressure_diff = np.mean(pressure_diff, axis=0)
                    ratio_sensitivities.append(avg_pressure_diff)

                    # æ¢å¤åŸå§‹éœ€æ°´é‡
                    node = wn.get_node(perturb_node)
                    for j, ts in enumerate(node.demand_timeseries_list):
                        ts.base_value = original_demands[j]

                # è®¡ç®—å¤šä¸ªæ‰°åŠ¨æ¯”ä¾‹çš„å¹³å‡æ•æ„Ÿåº¦
                avg_sensitivity = np.mean(ratio_sensitivities, axis=0)

                # å½’ä¸€åŒ–å¤„ç†ï¼ˆé¿å…åˆ†æ¯ä¸º0ï¼‰
                max_sensitivity = np.max(avg_sensitivity)
                if max_sensitivity > 0:
                    sensitivity_matrix[i, :] = avg_sensitivity / max_sensitivity
                else:
                    sensitivity_matrix[i, :] = 0

            self.log_info("å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µè®¡ç®—å®Œæˆ")

            # è¾“å‡ºæ•æ„Ÿåº¦çŸ©é˜µçš„ç»Ÿè®¡ä¿¡æ¯
            non_zero_count = np.count_nonzero(sensitivity_matrix)
            total_elements = sensitivity_matrix.size
            self.log_info(f"æ•æ„Ÿåº¦çŸ©é˜µç»Ÿè®¡: éé›¶å…ƒç´  {non_zero_count}/{total_elements} ({non_zero_count/total_elements*100:.1f}%)")
            self.log_info(f"æ•æ„Ÿåº¦çŸ©é˜µèŒƒå›´: [{np.min(sensitivity_matrix):.6f}, {np.max(sensitivity_matrix):.6f}]")

            # æ›´æ–°åˆ†åŒºä¿¡æ¯ï¼Œåªä¿ç•™æœ‰æ•ˆèŠ‚ç‚¹
            valid_partitions = {}
            for partition_id, nodes in partitions.items():
                valid_nodes = [node for node in nodes if node in valid_demand_nodes]
                if valid_nodes:
                    valid_partitions[partition_id] = valid_nodes

            return {
                'matrix': sensitivity_matrix,
                'nodes': valid_demand_nodes,
                'partitions': valid_partitions
            }

        except Exception as e:
            error_msg = f"è®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None
    
    def select_sensors_by_partition(self, sensitivity_data, threshold=0.5):
        """åŸºäºåˆ†åŒºé€‰æ‹©ä¼ æ„Ÿå™¨"""
        try:
            self.log_info(f"å¼€å§‹ä¼ æ„Ÿå™¨é€‰æ‹©ï¼Œæ•æ„Ÿåº¦é˜ˆå€¼: {threshold}")
            
            sensitivity_matrix = sensitivity_data['matrix']
            all_nodes = sensitivity_data['nodes']
            partitions = sensitivity_data['partitions']
            
            # åˆ›å»ºèŠ‚ç‚¹ç´¢å¼•æ˜ å°„
            node_to_index = {node: i for i, node in enumerate(all_nodes)}
            
            selected_sensors = {}
            
            for partition_id, partition_nodes in partitions.items():
                self.log_info(f"å¤„ç†åˆ†åŒº{partition_id}: {len(partition_nodes)}ä¸ªèŠ‚ç‚¹")
                
                # è·å–åˆ†åŒºå†…èŠ‚ç‚¹çš„ç´¢å¼•
                partition_indices = [node_to_index[node] for node in partition_nodes if node in node_to_index]
                
                if len(partition_indices) < 2:
                    self.log_info(f"åˆ†åŒº{partition_id}èŠ‚ç‚¹æ•°å°‘äº2ä¸ªï¼Œè·³è¿‡")
                    continue
                
                # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å½±å“åŠ›ï¼ˆèƒ½æ£€æµ‹åˆ°çš„èŠ‚ç‚¹æ•°ï¼‰
                influence_scores = {}
                for i, node_idx in enumerate(partition_indices):
                    node_name = all_nodes[node_idx]
                    
                    # åªè€ƒè™‘åŒåˆ†åŒºå†…çš„æ•æ„Ÿåº¦
                    partition_sensitivities = sensitivity_matrix[node_idx, partition_indices]
                    detectable_count = np.sum(partition_sensitivities > threshold)
                    
                    influence_scores[node_name] = {
                        'index': node_idx,
                        'detectable_count': detectable_count,
                        'avg_sensitivity': np.mean(partition_sensitivities)
                    }
                
                # åŠ¨æ€ç¡®å®šä¼ æ„Ÿå™¨æ•°é‡ä»¥ä¿è¯éŸ§æ€§
                partition_size = len(partition_nodes)
                min_sensors = max(2, int(partition_size * self.default_params['min_sensor_ratio']))
                max_sensors = min(10, max(3, int(partition_size * self.default_params['max_sensor_ratio'])))
                target_coverage = self.default_params['target_coverage']

                self.log_info(f"åˆ†åŒº{partition_id}åŠ¨æ€ä¼ æ„Ÿå™¨èŒƒå›´: {min_sensors}-{max_sensors}ä¸ª")

                # è´ªå¿ƒç®—æ³•é€‰æ‹©ä¼ æ„Ÿå™¨
                uncovered_indices = set(partition_indices)
                selected_sensors[partition_id] = []

                # ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºè¦†ç›–ç‡é€‰æ‹©ä¼ æ„Ÿå™¨
                while (uncovered_indices and
                       len(selected_sensors[partition_id]) < max_sensors and
                       len(uncovered_indices) / len(partition_indices) > (1 - target_coverage)):

                    best_sensor = None
                    best_coverage = 0

                    for node_name, info in influence_scores.items():
                        if node_name in [s['node'] for s in selected_sensors[partition_id]]:
                            continue  # å·²é€‰æ‹©çš„ä¼ æ„Ÿå™¨

                        node_idx = info['index']
                        # è®¡ç®—èƒ½è¦†ç›–å¤šå°‘æœªè¦†ç›–çš„èŠ‚ç‚¹
                        partition_sensitivities = sensitivity_matrix[node_idx, list(uncovered_indices)]
                        coverage = np.sum(partition_sensitivities > threshold)

                        if coverage > best_coverage:
                            best_coverage = coverage
                            best_sensor = {
                                'node': node_name,
                                'index': node_idx,
                                'coverage': coverage,
                                'influence_score': info['detectable_count'],
                                'avg_sensitivity': info['avg_sensitivity']
                            }

                    if best_sensor is None or best_coverage == 0:
                        break

                    selected_sensors[partition_id].append(best_sensor)

                    # æ›´æ–°æœªè¦†ç›–èŠ‚ç‚¹
                    sensor_idx = best_sensor['index']
                    covered_indices = []
                    for idx in uncovered_indices:
                        if sensitivity_matrix[sensor_idx, idx] > threshold:
                            covered_indices.append(idx)

                    for idx in covered_indices:
                        uncovered_indices.discard(idx)

                # ç¬¬äºŒé˜¶æ®µï¼šç¡®ä¿è¾¾åˆ°æœ€å°ä¼ æ„Ÿå™¨æ•°é‡ï¼ˆéŸ§æ€§ä¿è¯ï¼‰
                while len(selected_sensors[partition_id]) < min_sensors:
                    best_sensor = None
                    max_diversity = 0

                    for node_name, info in influence_scores.items():
                        if node_name in [s['node'] for s in selected_sensors[partition_id]]:
                            continue

                        node_idx = info['index']
                        # è®¡ç®—ä¸å·²æœ‰ä¼ æ„Ÿå™¨çš„å¤šæ ·æ€§ï¼ˆè·ç¦»ï¼‰
                        diversity_score = 0
                        for existing_sensor in selected_sensors[partition_id]:
                            existing_idx = existing_sensor['index']
                            # ä½¿ç”¨æ•æ„Ÿåº¦å·®å¼‚ä½œä¸ºè·ç¦»åº¦é‡
                            distance = 1 - sensitivity_matrix[node_idx, existing_idx]
                            diversity_score += distance

                        # å¹³å‡å¤šæ ·æ€§åˆ†æ•°
                        if len(selected_sensors[partition_id]) > 0:
                            diversity_score /= len(selected_sensors[partition_id])

                        if diversity_score > max_diversity:
                            max_diversity = diversity_score
                            best_sensor = {
                                'node': node_name,
                                'index': node_idx,
                                'coverage': 0,
                                'influence_score': info['detectable_count'],
                                'avg_sensitivity': info['avg_sensitivity']
                            }

                    if best_sensor is not None:
                        selected_sensors[partition_id].append(best_sensor)
                    else:
                        # å¦‚æœæ²¡æœ‰æ›´å¤šèŠ‚ç‚¹ï¼Œéšæœºé€‰æ‹©å‰©ä½™èŠ‚ç‚¹
                        remaining_nodes = [node for node in partition_nodes
                                         if node not in [s['node'] for s in selected_sensors[partition_id]]]
                        if remaining_nodes:
                            node_name = remaining_nodes[0]
                            if node_name in influence_scores:
                                info = influence_scores[node_name]
                                selected_sensors[partition_id].append({
                                    'node': node_name,
                                    'index': info['index'],
                                    'coverage': 0,
                                    'influence_score': info['detectable_count'],
                                    'avg_sensitivity': info['avg_sensitivity']
                                })
                        else:
                            break
                
                self.log_info(f"åˆ†åŒº{partition_id}é€‰æ‹©äº†{len(selected_sensors[partition_id])}ä¸ªä¼ æ„Ÿå™¨")
            
            return selected_sensors

        except Exception as e:
            error_msg = f"ä¼ æ„Ÿå™¨é€‰æ‹©å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None

    def evaluate_resilience(self, selected_sensors, sensitivity_data, threshold=0.5):
        """è¯„ä¼°ä¼ æ„Ÿå™¨å¸ƒç½®çš„éŸ§æ€§ - è¯¦ç»†ç‰ˆæœ¬"""
        try:
            self.log_info("å¼€å§‹è¯¦ç»†éŸ§æ€§è¯„ä¼°")

            sensitivity_matrix = sensitivity_data['matrix']
            all_nodes = sensitivity_data['nodes']
            partitions = sensitivity_data['partitions']

            # åˆ›å»ºèŠ‚ç‚¹ç´¢å¼•æ˜ å°„
            node_to_index = {node: i for i, node in enumerate(all_nodes)}

            resilience_results = {}

            for partition_id, sensors in selected_sensors.items():
                partition_nodes = partitions[partition_id]
                partition_indices = [node_to_index[node] for node in partition_nodes if node in node_to_index]

                self.log_info(f"è¯„ä¼°åˆ†åŒº{partition_id}çš„éŸ§æ€§: {len(sensors)}ä¸ªä¼ æ„Ÿå™¨, {len(partition_nodes)}ä¸ªèŠ‚ç‚¹")

                # è¯¦ç»†åœºæ™¯åˆ†æ
                detailed_scenarios = []

                # 1. å…¨éƒ¨ä¼ æ„Ÿå™¨æ­£å¸¸å·¥ä½œ
                all_sensor_indices = [s['index'] for s in sensors]
                all_sensor_nodes = [s['node'] for s in sensors]
                full_detected_count, full_coverage_rate = self._calculate_detailed_detection(
                    all_sensor_indices, partition_indices, sensitivity_matrix, threshold, all_sensor_nodes
                )

                detailed_scenarios.append({
                    'scenario_type': 'å…¨éƒ¨ä¼ æ„Ÿå™¨æ­£å¸¸',
                    'failed_sensors': [],
                    'remaining_sensors': [s['node'] for s in sensors],
                    'detected_nodes': full_detected_count,
                    'total_nodes': len(partition_nodes),
                    'coverage_rate': full_coverage_rate,
                    'coverage_percentage': f"{full_coverage_rate*100:.1f}%",
                    'threshold_used': threshold
                })

                # 2. ä¼ æ„Ÿå™¨æ•…éšœåœºæ™¯ï¼ˆ1åˆ°M-1ä¸ªæ•…éšœï¼‰
                total_failure_coverage = 0.0
                failure_scenario_count = 0

                for failure_count in range(1, len(sensors)):
                    failure_combinations = list(combinations(range(len(sensors)), failure_count))

                    for failed_indices in failure_combinations:
                        # ç¡®å®šå¤±æ•ˆå’Œå‰©ä½™çš„ä¼ æ„Ÿå™¨
                        failed_sensors = [sensors[i]['node'] for i in failed_indices]
                        remaining_sensor_indices = [
                            sensors[i]['index'] for i in range(len(sensors))
                            if i not in failed_indices
                        ]
                        remaining_sensors = [
                            sensors[i]['node'] for i in range(len(sensors))
                            if i not in failed_indices
                        ]

                        # è®¡ç®—å‰©ä½™ä¼ æ„Ÿå™¨çš„æ£€æµ‹èƒ½åŠ›
                        detected_count, coverage_rate = self._calculate_detailed_detection(
                            remaining_sensor_indices, partition_indices, sensitivity_matrix, threshold, remaining_sensors
                        )

                        total_failure_coverage += coverage_rate
                        failure_scenario_count += 1

                        detailed_scenarios.append({
                            'scenario_type': f'{failure_count}ä¸ªä¼ æ„Ÿå™¨å¤±æ•ˆ',
                            'failed_sensors': failed_sensors,
                            'remaining_sensors': remaining_sensors,
                            'detected_nodes': detected_count,
                            'total_nodes': len(partition_nodes),
                            'coverage_rate': coverage_rate,
                            'coverage_percentage': f"{coverage_rate*100:.1f}%",
                            'threshold_used': threshold
                        })

                # è®¡ç®—å¹³å‡éŸ§æ€§åˆ†æ•°ï¼ˆåªè€ƒè™‘æ•…éšœåœºæ™¯ï¼‰
                avg_failure_resilience = total_failure_coverage / failure_scenario_count if failure_scenario_count > 0 else 0.0

                resilience_results[partition_id] = {
                    'detailed_scenarios': detailed_scenarios,
                    'resilience_score': avg_failure_resilience,
                    'sensor_count': len(sensors),
                    'full_coverage_rate': full_coverage_rate,
                    'avg_failure_coverage': avg_failure_resilience,
                    'total_scenarios': len(detailed_scenarios),
                    'failure_scenarios': failure_scenario_count
                }

                self.log_info(f"åˆ†åŒº{partition_id}éŸ§æ€§åˆ†æ•°: {avg_failure_resilience:.4f}")

            return resilience_results

        except Exception as e:
            error_msg = f"éŸ§æ€§è¯„ä¼°å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None

    def _calculate_detection_rate(self, sensor_indices, target_indices, sensitivity_matrix, threshold):
        """è®¡ç®—æ£€æµ‹ç‡"""
        if not sensor_indices:
            return 0.0

        detected_count = 0
        for target_idx in target_indices:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¼ æ„Ÿå™¨èƒ½æ£€æµ‹åˆ°è¯¥èŠ‚ç‚¹
            for sensor_idx in sensor_indices:
                if sensitivity_matrix[sensor_idx, target_idx] > threshold:
                    detected_count += 1
                    break

        return detected_count / len(target_indices) if target_indices else 0.0

    def _calculate_detailed_detection(self, sensor_indices, target_indices, sensitivity_matrix, threshold, sensor_nodes=None):
        """è®¡ç®—è¯¦ç»†æ£€æµ‹ä¿¡æ¯ï¼šè¿”å›æ£€æµ‹åˆ°çš„èŠ‚ç‚¹æ•°å’Œè¦†ç›–ç‡"""
        if not sensor_indices or not target_indices:
            return 0, 0.0

        detected_count = 0
        for target_idx in target_indices:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¼ æ„Ÿå™¨èƒ½æ£€æµ‹åˆ°è¯¥èŠ‚ç‚¹
            for i, sensor_idx in enumerate(sensor_indices):
                # è·å–è¯¥ä¼ æ„Ÿå™¨çš„é˜ˆå€¼ï¼ˆæ”¯æŒè‡ªå®šä¹‰é˜ˆå€¼ï¼‰
                sensor_threshold = threshold
                if (self.default_params.get('enable_custom_thresholds', False) and
                    sensor_nodes and i < len(sensor_nodes)):
                    sensor_node = sensor_nodes[i]
                    sensor_threshold = self.default_params.get('custom_thresholds', {}).get(sensor_node, threshold)

                if sensitivity_matrix[sensor_idx, target_idx] > sensor_threshold:
                    detected_count += 1
                    break

        coverage_rate = detected_count / len(target_indices)
        return detected_count, coverage_rate

    def set_custom_sensor_thresholds(self, custom_thresholds):
        """è®¾ç½®è‡ªå®šä¹‰ä¼ æ„Ÿå™¨é˜ˆå€¼

        Args:
            custom_thresholds (dict): {sensor_node: threshold} æ ¼å¼çš„å­—å…¸
        """
        self.default_params['custom_thresholds'] = custom_thresholds
        self.default_params['enable_custom_thresholds'] = bool(custom_thresholds)
        self.log_info(f"è®¾ç½®è‡ªå®šä¹‰ä¼ æ„Ÿå™¨é˜ˆå€¼: {custom_thresholds}")

    def _calculate_resilience_score(self, scenario_results):
        """è®¡ç®—éŸ§æ€§åˆ†æ•°"""
        # åŠ æƒå¹³å‡ä¸åŒæ•…éšœåœºæ™¯çš„æ£€æµ‹ç‡
        total_score = 0.0
        total_weight = 0.0

        for scenario, result in scenario_results.items():
            if 'no_failure' in scenario:
                weight = 0.5  # æ— æ•…éšœæƒ…å†µæƒé‡
            else:
                failure_count = int(scenario.split('_')[0])
                weight = 1.0 / (failure_count + 1)  # æ•…éšœè¶Šå¤šæƒé‡è¶Šå°

            total_score += result['detection_rate'] * weight
            total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0

    def optimize_sensor_placement(self, sensitivity_data, max_iterations=10):
        """ä¼˜åŒ–ä¼ æ„Ÿå™¨å¸ƒç½®"""
        try:
            self.log_info("å¼€å§‹ä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–")

            best_solution = None
            best_score = 0.0

            # å°è¯•ä¸åŒçš„æ•æ„Ÿåº¦é˜ˆå€¼ï¼ˆå‡å°‘æ•°é‡ä»¥åŠ å¿«è®¡ç®—ï¼‰
            thresholds = [0.4, 0.5, 0.6]

            for threshold in thresholds:
                self.log_info(f"å°è¯•é˜ˆå€¼: {threshold}")

                # é€‰æ‹©ä¼ æ„Ÿå™¨
                selected_sensors = self.select_sensors_by_partition(sensitivity_data, threshold)
                if not selected_sensors:
                    continue

                # è¯„ä¼°éŸ§æ€§
                resilience_results = self.evaluate_resilience(selected_sensors, sensitivity_data, threshold)
                if not resilience_results:
                    continue

                # è®¡ç®—ç»¼åˆè¯„åˆ†
                total_score = self._calculate_total_score(selected_sensors, resilience_results)

                self.log_info(f"é˜ˆå€¼{threshold}çš„ç»¼åˆè¯„åˆ†: {total_score:.4f}")

                if total_score > best_score:
                    best_score = total_score
                    best_solution = {
                        'sensors': selected_sensors,
                        'resilience': resilience_results,
                        'threshold': threshold,
                        'score': total_score
                    }

            if best_solution:
                self.log_info(f"æœ€ä¼˜è§£: é˜ˆå€¼={best_solution['threshold']}, è¯„åˆ†={best_solution['score']:.4f}")

            return best_solution

        except Exception as e:
            error_msg = f"ä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None

    def _calculate_total_score(self, selected_sensors, resilience_results):
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        # ä¼ æ„Ÿå™¨æ•°é‡æƒ©ç½šï¼ˆé™ä½æƒ©ç½šåŠ›åº¦ï¼‰
        total_sensors = sum(len(sensors) for sensors in selected_sensors.values())
        sensor_penalty = total_sensors * 0.001  # æ¯ä¸ªä¼ æ„Ÿå™¨æ‰£0.001åˆ†

        # éŸ§æ€§åˆ†æ•°
        resilience_scores = [r['resilience_score'] for r in resilience_results.values()]
        avg_resilience = np.mean(resilience_scores) if resilience_scores else 0.0

        # è¦†ç›–ç‡åˆ†æ•°ï¼ˆåŸºäºä¼ æ„Ÿå™¨æ•°é‡å’Œåˆ†åŒºè¦†ç›–ï¼‰
        partition_count = len(selected_sensors)
        coverage_score = min(1.0, total_sensors / (partition_count * 2))  # ç†æƒ³æƒ…å†µæ¯åˆ†åŒº2ä¸ªä¼ æ„Ÿå™¨

        # ç»¼åˆè¯„åˆ†ï¼šéŸ§æ€§æƒé‡40%ï¼Œè¦†ç›–ç‡æƒé‡60%
        resilience_weight = self.default_params['resilience_weight']
        coverage_weight = self.default_params['coverage_weight']

        total_score = (avg_resilience * resilience_weight +
                      coverage_score * coverage_weight -
                      sensor_penalty)

        return max(0.001, total_score)  # ç¡®ä¿æœ€å°åˆ†æ•°ä¸º0.001

    def _save_detailed_resilience_analysis(self, solution, conversation_id):
        """ä¿å­˜è¯¦ç»†çš„éŸ§æ€§åˆ†æç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"resilience_analysis_{conversation_id[:8]}_{timestamp}.csv"
            filepath = os.path.join(self.downloads_folder, filename)

            analysis_data = []

            for partition_id, resilience_info in solution['resilience'].items():
                detailed_scenarios = resilience_info.get('detailed_scenarios', [])

                for scenario in detailed_scenarios:
                    analysis_data.append({
                        'åˆ†åŒºç¼–å·': partition_id,
                        'åœºæ™¯ç±»å‹': scenario['scenario_type'],
                        'å¤±æ•ˆä¼ æ„Ÿå™¨': ', '.join(scenario['failed_sensors']) if scenario['failed_sensors'] else 'æ— ',
                        'å‰©ä½™ä¼ æ„Ÿå™¨': ', '.join(scenario['remaining_sensors']),
                        'æ£€æµ‹åˆ°èŠ‚ç‚¹æ•°': scenario['detected_nodes'],
                        'æ€»èŠ‚ç‚¹æ•°': scenario['total_nodes'],
                        'è¦†ç›–ç‡': f"{scenario['coverage_rate']:.4f}",
                        'è¦†ç›–ç™¾åˆ†æ¯”': scenario['coverage_percentage'],
                        'æ•æ„Ÿåº¦é˜ˆå€¼': scenario['threshold_used'],
                        'åˆ†åŒºä¼ æ„Ÿå™¨æ€»æ•°': resilience_info['sensor_count'],
                        'åˆ†åŒºå¹³å‡éŸ§æ€§': f"{resilience_info['resilience_score']:.4f}"
                    })

            # åˆ›å»ºDataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(analysis_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')

            self.log_info(f"è¯¦ç»†éŸ§æ€§åˆ†æå·²ä¿å­˜åˆ°: {filepath}")
            return filepath

        except Exception as e:
            self.log_error(f"ä¿å­˜éŸ§æ€§åˆ†æå¤±è´¥: {str(e)}")
            return None

    def save_sensor_results(self, solution, inp_file_path, conversation_id):
        """ä¿å­˜ä¼ æ„Ÿå™¨å¸ƒç½®ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sensor_placement_{conversation_id[:8]}_{timestamp}.csv"
            filepath = os.path.join(self.downloads_folder, filename)

            # åŠ è½½ç½‘ç»œæ¨¡å‹è·å–åæ ‡
            wn = wntr.network.WaterNetworkModel(inp_file_path)

            # å‡†å¤‡æ•°æ®
            results_data = []
            sensor_id = 1

            for partition_id, sensors in solution['sensors'].items():
                for sensor in sensors:
                    node_name = sensor['node']

                    # è·å–èŠ‚ç‚¹åæ ‡
                    try:
                        coord = wn.get_node(node_name).coordinates
                        if coord is None:
                            coord = (0, 0)
                    except:
                        coord = (0, 0)

                    # è·å–éŸ§æ€§ä¿¡æ¯
                    resilience_info = solution['resilience'].get(partition_id, {})
                    resilience_score = resilience_info.get('resilience_score', 0.0)

                    results_data.append({
                        'ä¼ æ„Ÿå™¨ID': f'S{sensor_id:03d}',
                        'èŠ‚ç‚¹åç§°': node_name,
                        'åˆ†åŒºç¼–å·': partition_id,
                        'Xåæ ‡': coord[0],
                        'Yåæ ‡': coord[1],
                        'å½±å“åŠ›åˆ†æ•°': sensor['influence_score'],
                        'å¹³å‡æ•æ„Ÿåº¦': f"{sensor['avg_sensitivity']:.4f}",
                        'è¦†ç›–èŠ‚ç‚¹æ•°': sensor['coverage'],
                        'åˆ†åŒºéŸ§æ€§åˆ†æ•°': f"{resilience_score:.4f}",
                        'æ•æ„Ÿåº¦é˜ˆå€¼': solution['threshold']
                    })
                    sensor_id += 1

            # åˆ›å»ºDataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(results_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')

            # ä¿å­˜è¯¦ç»†éŸ§æ€§åˆ†æ
            self._save_detailed_resilience_analysis(solution, conversation_id)

            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            stats = self._generate_statistics(solution)

            file_size = os.path.getsize(filepath)

            self.log_info(f"ä¼ æ„Ÿå™¨å¸ƒç½®ç»“æœå·²ä¿å­˜åˆ°: {filepath}")

            return {
                'success': True,
                'filename': filename,
                'filepath': filepath,
                'file_size': file_size,
                'sensor_count': len(results_data),
                'statistics': stats,
                'download_url': f'/download/{filename}'
            }

        except Exception as e:
            error_msg = f"ä¿å­˜ä¼ æ„Ÿå™¨ç»“æœå¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {
                'success': False,
                'error': error_msg
            }

    def _generate_statistics(self, solution):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_sensors': 0,
            'partitions': len(solution['sensors']),
            'avg_resilience': 0.0,
            'threshold': solution['threshold'],
            'total_score': solution['score']
        }

        # è®¡ç®—æ€»ä¼ æ„Ÿå™¨æ•°
        for sensors in solution['sensors'].values():
            stats['total_sensors'] += len(sensors)

        # è®¡ç®—å¹³å‡éŸ§æ€§
        if solution['resilience']:
            resilience_scores = [r['resilience_score'] for r in solution['resilience'].values()]
            stats['avg_resilience'] = np.mean(resilience_scores)

        # åˆ†åŒºè¯¦æƒ…
        stats['partition_details'] = {}
        for partition_id, sensors in solution['sensors'].items():
            resilience_info = solution['resilience'].get(partition_id, {})
            stats['partition_details'][partition_id] = {
                'sensor_count': len(sensors),
                'resilience_score': resilience_info.get('resilience_score', 0.0)
            }

        return stats

    def generate_visualization(self, solution, inp_file_path, conversation_id):
        """ç”Ÿæˆä¼ æ„Ÿå™¨å¸ƒç½®å¯è§†åŒ–å›¾"""
        try:
            # è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡å­—ä½“
            plt.rcParams['font.family'] = 'DejaVu Sans'
            plt.rcParams['axes.unicode_minus'] = False

            # åŠ è½½ç½‘ç»œæ¨¡å‹
            wn = wntr.network.WaterNetworkModel(inp_file_path)
            G = wn.to_graph().to_undirected()

            # å‡†å¤‡èŠ‚ç‚¹ä½ç½®
            pos = {}
            layout = None

            for node in G.nodes():
                try:
                    coord = wn.get_node(node).coordinates
                    if coord is None or coord == (0, 0):
                        if layout is None:
                            import networkx as nx
                            layout = nx.spring_layout(G, seed=42)
                        coord = layout.get(node, (0, 0))
                except:
                    if layout is None:
                        import networkx as nx
                        layout = nx.spring_layout(G, seed=42)
                    coord = layout.get(node, (0, 0))
                pos[node] = coord

            # åˆ›å»ºå›¾å½¢
            plt.figure(figsize=(15, 12))

            # ç»˜åˆ¶ç½‘ç»œè¾¹
            import networkx as nx
            nx.draw_networkx_edges(G, pos=pos, alpha=0.3, width=0.5, edge_color='gray')

            # ç»˜åˆ¶æ™®é€šèŠ‚ç‚¹
            all_nodes = list(G.nodes())
            nx.draw_networkx_nodes(G, pos=pos, nodelist=all_nodes,
                                 node_color='lightblue', node_size=20, alpha=0.6)

            # ç»˜åˆ¶ä¼ æ„Ÿå™¨èŠ‚ç‚¹
            colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray']
            sensor_nodes_by_partition = {}

            for partition_id, sensors in solution['sensors'].items():
                sensor_nodes = [s['node'] for s in sensors]
                sensor_nodes_by_partition[partition_id] = sensor_nodes

                color = colors[partition_id % len(colors)]
                nx.draw_networkx_nodes(G, pos=pos, nodelist=sensor_nodes,
                                     node_color=color, node_size=100, alpha=0.8,
                                     label=f'Partition {partition_id} Sensors')

            # æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.title(f'Sensor Placement Results\n'
                     f'Total Sensors: {sum(len(s) for s in solution["sensors"].values())}, '
                     f'Threshold: {solution["threshold"]}, '
                     f'Score: {solution["score"]:.4f}')
            plt.axis('off')

            # ä¿å­˜å›¾åƒ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            viz_filename = f"sensor_placement_viz_{conversation_id[:8]}_{timestamp}.png"
            viz_path = os.path.join(self.downloads_folder, viz_filename)

            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.log_info(f"ä¼ æ„Ÿå™¨å¸ƒç½®å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {viz_path}")

            return viz_path

        except Exception as e:
            error_msg = f"ç”Ÿæˆå¯è§†åŒ–å›¾å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return None

    def process(self, inp_file_path, partition_csv_path, user_message, conversation_id):
        """ä¸»å¤„ç†å‡½æ•°"""
        try:
            self.log_info(f"å¼€å§‹ä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®: {user_message}")
            self.log_info(f"è¾“å…¥æ–‡ä»¶: {inp_file_path}")
            self.log_info(f"åˆ†åŒºæ–‡ä»¶: {partition_csv_path}")

            # Step 1: åŠ è½½åˆ†åŒºç»“æœ
            self.log_info("Step 1: åŠ è½½åˆ†åŒºç»“æœ...")
            partitions = self.load_partition_results(partition_csv_path)
            if not partitions:
                self.log_error("åˆ†åŒºç»“æœåŠ è½½å¤±è´¥")
                return {
                    'success': False,
                    'response': "åˆ†åŒºç»“æœåŠ è½½å¤±è´¥",
                    'error': "æ— æ³•åŠ è½½åˆ†åŒºCSVæ–‡ä»¶"
                }

            self.log_info(f"åˆ†åŒºåŠ è½½æˆåŠŸï¼Œå…±{len(partitions)}ä¸ªåˆ†åŒº")

            # Step 2: è®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µ
            self.log_info("Step 2: è®¡ç®—å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µ...")
            sensitivity_data = self.compute_pressure_sensitivity_matrix(
                inp_file_path, partitions, self.default_params['demand_ratios']
            )

            if not sensitivity_data:
                self.log_error("å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µè®¡ç®—å¤±è´¥")
                return {
                    'success': False,
                    'response': "å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µè®¡ç®—å¤±è´¥",
                    'error': "æ•æ„Ÿåº¦è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
                }

            self.log_info("æ•æ„Ÿåº¦çŸ©é˜µè®¡ç®—æˆåŠŸ")

            # Step 3: ä¼˜åŒ–ä¼ æ„Ÿå™¨å¸ƒç½®
            self.log_info("Step 3: ä¼˜åŒ–ä¼ æ„Ÿå™¨å¸ƒç½®...")
            solution = self.optimize_sensor_placement(sensitivity_data)

            if not solution:
                self.log_error("ä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–å¤±è´¥")
                return {
                    'success': False,
                    'response': "ä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–å¤±è´¥",
                    'error': "ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
                }

            self.log_info("ä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–æˆåŠŸ")

            # Step 4: ä¿å­˜ç»“æœ
            save_result = self.save_sensor_results(solution, inp_file_path, conversation_id)

            # Step 5: ç”Ÿæˆå¯è§†åŒ–
            viz_path = self.generate_visualization(solution, inp_file_path, conversation_id)

            # Step 6: ç”Ÿæˆåˆ†ææŠ¥å‘Š
            stats = save_result.get('statistics', {})

            response_text = f"""
ä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®å®Œæˆï¼

ğŸ“Š **å¸ƒç½®æ¦‚å†µ**
- æ€»ä¼ æ„Ÿå™¨æ•°: {stats.get('total_sensors', 0)}
- åˆ†åŒºæ•°é‡: {stats.get('partitions', 0)}
- æ•æ„Ÿåº¦é˜ˆå€¼: {stats.get('threshold', 0.5)}
- ç»¼åˆè¯„åˆ†: {stats.get('total_score', 0.0):.4f}

ğŸ“ˆ **éŸ§æ€§åˆ†æ**
- å¹³å‡éŸ§æ€§åˆ†æ•°: {stats.get('avg_resilience', 0.0):.4f}
- æ‰°åŠ¨æ¯”ä¾‹: {self.default_params['demand_ratios']}

ğŸ¯ **åˆ†åŒºè¯¦æƒ…**
"""

            for partition_id, details in stats.get('partition_details', {}).items():
                response_text += f"- åˆ†åŒº{partition_id}: {details['sensor_count']}ä¸ªä¼ æ„Ÿå™¨ (éŸ§æ€§: {details['resilience_score']:.4f})\n"

            response_text += f"""
âœ… ä¼ æ„Ÿå™¨å¸ƒç½®ä¼˜åŒ–ç­–ç•¥ï¼š
1. åŸºäºå‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µè¿›è¡Œä¼ æ„Ÿå™¨é€‰æ‹©
2. è€ƒè™‘å¤šç§æ•…éšœåœºæ™¯çš„éŸ§æ€§è¯„ä¼°
3. ç¡®ä¿æ¯ä¸ªåˆ†åŒºè‡³å°‘2ä¸ªä¼ æ„Ÿå™¨
4. ä¼˜åŒ–æ£€æµ‹è¦†ç›–ç‡å’Œä¼ æ„Ÿå™¨æ•°é‡çš„å¹³è¡¡

ğŸ“ ç»“æœæ–‡ä»¶å·²ä¿å­˜ï¼ŒåŒ…å«è¯¦ç»†çš„ä¼ æ„Ÿå™¨ä½ç½®å’Œæ€§èƒ½æŒ‡æ ‡
"""

            # ç”Ÿæˆä¸“ä¸špromptç”¨äºGPTåˆ†æ
            prompt = self._build_sensor_placement_prompt(solution, stats, user_message, save_result)

            result = {
                'success': True,
                'response': response_text,
                'prompt': prompt,  # æ·»åŠ ä¸“ä¸špromptç”¨äºGPTåˆ†æ
                'solution': solution,
                'statistics': stats
            }

            # æ·»åŠ æ–‡ä»¶ä¸‹è½½ä¿¡æ¯
            if save_result['success']:
                result['csv_info'] = save_result

            # æ·»åŠ éŸ§æ€§åˆ†ææ–‡ä»¶ä¿¡æ¯
            resilience_csv_path = self._save_detailed_resilience_analysis(solution, conversation_id)
            if resilience_csv_path:
                result['resilience_csv_info'] = resilience_csv_path

            if viz_path:
                result['visualization'] = {
                    'filename': os.path.basename(viz_path),
                    'path': viz_path
                }

            return result

        except Exception as e:
            error_msg = f"ä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®å¤±è´¥: {str(e)}"
            self.log_error(error_msg)
            return {
                'success': False,
                'response': error_msg,
                'error': str(e)
            }

    def _build_sensor_placement_prompt(self, solution, stats, user_message, save_result):
        """æ„å»ºä¼ æ„Ÿå™¨å¸ƒç½®åˆ†æçš„ä¸“ä¸šprompt"""

        # è·å–ä¼ æ„Ÿå™¨è¯¦ç»†ä¿¡æ¯
        sensors_info = []
        for partition_id, sensors in solution['sensors'].items():
            for sensor in sensors:
                sensors_info.append(f"åˆ†åŒº{partition_id}: èŠ‚ç‚¹{sensor['node']} (æ•æ„Ÿåº¦: {sensor.get('avg_sensitivity', 0):.4f})")

        # è·å–éŸ§æ€§åˆ†æè¯¦æƒ…
        resilience_details = []
        for partition_id, resilience_data in solution['resilience'].items():
            resilience_details.append(f"åˆ†åŒº{partition_id}: éŸ§æ€§åˆ†æ•°{resilience_data['resilience_score']:.4f}, {resilience_data['sensor_count']}ä¸ªä¼ æ„Ÿå™¨")

        prompt = f"""
ç”¨æˆ·è¯·æ±‚: {user_message}

## ä¼ æ„Ÿå™¨ä¼˜åŒ–å¸ƒç½®åˆ†ææŠ¥å‘Š

### ğŸ“Š å¸ƒç½®æ¦‚å†µ
- **æ€»ä¼ æ„Ÿå™¨æ•°**: {stats.get('total_sensors', 0)}ä¸ª
- **åˆ†åŒºæ•°é‡**: {stats.get('partitions', 0)}ä¸ª
- **æœ€ä¼˜æ•æ„Ÿåº¦é˜ˆå€¼**: {stats.get('threshold', 0.5)}
- **ç»¼åˆè¯„åˆ†**: {stats.get('total_score', 0.0):.4f}
- **å¹³å‡éŸ§æ€§åˆ†æ•°**: {stats.get('avg_resilience', 0.0):.4f}

### ğŸ¯ ä¼ æ„Ÿå™¨å¸ƒç½®è¯¦æƒ…
{chr(10).join(sensors_info)}

### ğŸ“ˆ éŸ§æ€§åˆ†æç»“æœ
{chr(10).join(resilience_details)}

### ğŸ”§ æŠ€æœ¯å‚æ•°
- **æ‰°åŠ¨æ¯”ä¾‹**: {self.default_params['demand_ratios']}
- **éŸ§æ€§æƒé‡**: {self.default_params['resilience_weight']}
- **è¦†ç›–ç‡æƒé‡**: {self.default_params['coverage_weight']}

### ğŸ“ ç”Ÿæˆæ–‡ä»¶
- **ä¼ æ„Ÿå™¨å¸ƒç½®ç»“æœ**: {save_result.get('filename', 'N/A')}
- **æ–‡ä»¶å¤§å°**: {save_result.get('file_size', 0)} bytes
- **è®°å½•æ•°**: {save_result.get('sensor_count', 0)}æ¡

### ğŸ¯ ä¼˜åŒ–ç­–ç•¥è¯´æ˜
1. **å‹åŠ›æ•æ„Ÿåº¦åˆ†æ**: åŸºäºéœ€æ°´é‡æ‰°åŠ¨è®¡ç®—å„èŠ‚ç‚¹é—´çš„å‹åŠ›æ•æ„Ÿåº¦çŸ©é˜µ
2. **åˆ†åŒºå†…ä¼˜åŒ–**: åœ¨æ¯ä¸ªåˆ†åŒºå†…é€‰æ‹©å½±å“åŠ›æœ€å¤§çš„èŠ‚ç‚¹ä½œä¸ºä¼ æ„Ÿå™¨ä½ç½®
3. **éŸ§æ€§è¯„ä¼°**: è€ƒè™‘ä¼ æ„Ÿå™¨æ•…éšœåœºæ™¯ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨éƒ¨åˆ†ä¼ æ„Ÿå™¨å¤±æ•ˆæ—¶ä»èƒ½æ­£å¸¸å·¥ä½œ
4. **å¤šç›®æ ‡å¹³è¡¡**: åœ¨æ£€æµ‹è¦†ç›–ç‡ã€éŸ§æ€§å’Œä¼ æ„Ÿå™¨æ•°é‡ä¹‹é—´æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡

è¯·åŸºäºä»¥ä¸ŠæŠ€æœ¯åˆ†æï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„ä¼ æ„Ÿå™¨å¸ƒç½®æ–¹æ¡ˆè§£è¯»å’Œå»ºè®®ã€‚é‡ç‚¹è¯´æ˜ï¼š
1. ä¼ æ„Ÿå™¨å¸ƒç½®çš„ç§‘å­¦æ€§å’Œåˆç†æ€§
2. éŸ§æ€§è®¾è®¡çš„é‡è¦æ€§å’Œæ•ˆæœ
3. å„åˆ†åŒºä¼ æ„Ÿå™¨é…ç½®çš„ç‰¹ç‚¹
4. å®é™…åº”ç”¨ä¸­çš„æ³¨æ„äº‹é¡¹å’Œå»ºè®®

è¯·åœ¨å›å¤çš„æœ€åä½¿ç”¨ä»¥ä¸‹ç­¾åæ ¼å¼ï¼š

ç¥å¥½ï¼Œ

Tianwei Mu
Guangzhou Institute of Industrial Intelligence
"""

        return prompt
